{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.exceptions import Timeout\n",
    "import pandas as pd\n",
    "from helium import *\n",
    "import cv2\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver import ChromeOptions\n",
    "\n",
    "from pprint import pprint\n",
    "import regex as re\n",
    "from multiprocessing import Process\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagesize\n",
    "import numpy as np\n",
    "\n",
    "import download_imgs # used in getAsyncImgFunctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. normal',\n",
       " '1. fmemes',\n",
       " '2. ememes',\n",
       " '3. eSocialMedia',\n",
       " '4. fFbPosts',\n",
       " '5. fTwtrPosts',\n",
       " '6. fTxtMssgs',\n",
       " '7. eGreetingAndMisc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = 'dataset/'\n",
    "imgTypes = list(filter(lambda v: re.match('\\d{1,3}\\. ', v), os.listdir(dataDir)))\n",
    "imgTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_fb_email = 'your_email'\n",
    "secret_fb_password = 'your_password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pklSave(contentToBeSaved, fullPath):\n",
    "    with open(fullPath, 'wb') as f:\n",
    "        pickle.dump(contentToBeSaved, f)\n",
    "\n",
    "def pklLoad(fullPath):\n",
    "    with open(fullPath, 'rb') as f:\n",
    "        content = pickle.load(f)\n",
    "    return content\n",
    "\n",
    "def pklForceLoad(path, dtype = 'dict'):\n",
    "    try:\n",
    "        content = pklLoad(path)\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        if dtype == 'list':\n",
    "            pklSave([], path)\n",
    "            return []\n",
    "        else:\n",
    "            pklSave({}, path)\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits (source): https://medium.com/cubemail88/automatically-download-chromedriver-for-selenium-aaf2e3fd9d81\n",
    "def updateChromeDriver():\n",
    "    \"\"\"\n",
    "    Installs newest version of ChromeDriver.exe and adds it to path that Helium will read from.\n",
    "    Therefore, your chrome browser should be updated to the latest version\n",
    "    \"\"\"\n",
    "    os.environ['WDM_LOG_LEVEL'] = '0'\n",
    "    os.environ['WDM_LOCAL'] = '1'\n",
    "    oldPath = ChromeDriverManager().install()\n",
    "    helium_chromedriver_path = r'.env\\Lib\\site-packages\\helium\\_impl\\webdrivers\\windows'\n",
    "    newPath = oldPath[:oldPath.find('.wdm')] + helium_chromedriver_path + '\\\\' + oldPath.split('\\\\')[-1]\n",
    "    os.replace(oldPath, newPath) # moves installed chromedriver.exe from old path to new path\n",
    "    shutil.rmtree('.wdm/') # removes old path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_facebook():\n",
    "    write(secret_fb_email, into='Email')\n",
    "    write(secret_fb_password, into='password')\n",
    "    click('log in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this function if the chromedriver version in helium_chromedriver_path is diffeent than your chrome version to update it\n",
    "# updateChromeDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ChromeOptions()\n",
    "options.add_argument('--disable-notifications')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up images and Labels' dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0. normal': 0,\n",
       " '1. fmemes': 1,\n",
       " '2. ememes': 2,\n",
       " '3. eSocialMedia': 3,\n",
       " '4. fFbPosts': 4,\n",
       " '5. fTwtrPosts': 5,\n",
       " '6. fTxtMssgs': 6,\n",
       " '7. eGreetingAndMisc': 7}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgTypeToNum = {}\n",
    "for  i, type in enumerate(imgTypes):\n",
    "    imgTypeToNum[type] = i\n",
    "imgTypeToNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pklSave(imgTypeToNum, dataDir+\"imgTypeToNum.pickle\") # to be later used in another .ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Foreign Memes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From [Kaggle's Meme Generator Dataset](https://www.kaggle.com/datasets/electron0zero/memegenerator-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting memes' URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Meme ID</th>\n",
       "      <th>Base Meme Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10509464</td>\n",
       "      <td>Spiderman Approves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12285257</td>\n",
       "      <td>Alright Then Business Kid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20612245</td>\n",
       "      <td>Archer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20614628</td>\n",
       "      <td>Futurama Fry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24194267</td>\n",
       "      <td>One Does Not Simply</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Meme ID             Base Meme Name\n",
       "0  10509464         Spiderman Approves\n",
       "1  12285257  Alright Then Business Kid\n",
       "2  20612245                     Archer\n",
       "3  20614628               Futurama Fry\n",
       "4  24194267        One Does Not Simply"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmemesCsvLinks = pd.read_csv(\"dataset/memegenerator.csv\", usecols=[\"Meme ID\", \"Base Meme Name\"]).squeeze() # convert to Series\n",
    "fmemesCsvLinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection, we want to get the original template of memes, not its varientswe'll find that to fetch a page of only the meme image, the URL has to be in this format <br>\n",
    "`https://memegenerator.net/img/instances/XX.jpg` <br>\n",
    "Where `XX` is the unique id of the image on [Meme Generator](https://memegenerator.net/) <br>\n",
    "Thus, we will use the `Meme ID` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10509464\n",
       "1    12285257\n",
       "2    20612245\n",
       "3    20614628\n",
       "4    24194267\n",
       "Name: Meme ID, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmemesIds = fmemesCsvLinks['Meme ID']\n",
    "fmemesIds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://memegenerator.net/img/instances/10509464.jpg\n",
       "1    https://memegenerator.net/img/instances/12285257.jpg\n",
       "2    https://memegenerator.net/img/instances/20612245.jpg\n",
       "3    https://memegenerator.net/img/instances/20614628.jpg\n",
       "4    https://memegenerator.net/img/instances/24194267.jpg\n",
       "Name: Meme ID, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 70)\n",
    "fmemesLinks = 'https://memegenerator.net/img/instances/' + fmemesIds + '.jpg'\n",
    "fmemesLinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the foreign images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAsyncImgFunctions(imgsLinks, imgType, interval, offset=0):\n",
    "    fns = []\n",
    "    i = 0\n",
    "    check = True\n",
    "    while i < len(imgsLinks):\n",
    "        fns.append((download_imgs.downloadImgs, (imgType, imgsLinks, i, i+interval, offset)))\n",
    "        i += interval\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = getAsyncImgFunctions(fmemesLinks, imgTypes[1], 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# Using runInParallel() below, we will run xx functions parallely to fetch data\n",
    "print(len(fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runInParallel(fns):\n",
    "  proc = []\n",
    "  for fn in fns:\n",
    "    p = Process(target=fn[0], args=(fn[1]))\n",
    "    p.start()\n",
    "    proc.append(p)\n",
    "  for p in proc:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "    #runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Egyptian Memes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Imgur using this [Facebook post]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HxfPF', 'ilzyz', 'lhDIg', 'mMJkA', 'nqdmP', 'XsucN', 'fUQNf', 'NPUrK', 'UeRnH', 'wMQTL', 'xxLOL', 'GdBOz', 'AIgjT', 'OKOME', 'VFUyL', 'mXJkz', 'dxfdi', 'wAhJp', 'hYvug', 'bbTvu', 'PMlpR', 'qjcBf', 'vIzBa', 'ysymZ', 'rBvFC', 'hwjFu', 'pxapm', 'QdWKQ']\n"
     ]
    }
   ],
   "source": [
    "fbPostContent = \"Templates HQ - المنتدى ( ألبومات تمبلتس _ مستلزمات الكوميك _ شروحات _ افضل المواقع _ مستلزمات التصميم _ ايموشنز _ وشوش ) : plus.google.com/u/0/communities/108675057016512986137 ____ ألبومات تمبلتس :(تمبلتس | مقصوصه ، افلام ، مسلسلات ، مشاهد ، برامج ) ___________ .......... محمد هنيدى .......... رمضان مبروك ابو العالمين حمودة | imgur.com/a/HxfPF اسماعيلية رايح جى | imgur.com/a/i5Msi فول الصين العظيم | imgur.com/a/C8OLI جائنا البيان التالى | imgur.com/a/r4X66 صاحب صاحبه | imgur.com/a/ilzyz يا انا يا خالتى | imgur.com/a/sTx1q امير البحار | imgur.com/a/mRYs0 وش اجرام | imgur.com/a/lhDIg .......... محمد سعد .......... اللي بالي بالك | imgur.com/a/mMJkA اللمبى‏ | imgur.com/a/kNwD8 بوحه | imgur.com/a/nqdmP .......... عادل امام .......... عريس من جهة امنيه | imgur.com/a/h4pA7 السفارة فى العمارة | imgur.com/a/Y519O سلام يا صاحبى | imgur.com/a/XsucN زهايمر | imgur.com/a/iLUE0 .......... احمد مكى .......... لا تراجع ولا استسلام | imgur.com/a/Ri6QP الكبير اوى | imgur.com/a/fUQNf طير انت | imgur.com/a/NPUrK .......... شيكو و هشام ماجد و أحمد فهمي .......... سمير وشهير وبهير | imgur.com/a/UeRnH حملة فريزر | imgur.com/a/bKB5D بنات العم | imgur.com/a/wMQTL .......... كريم عبد العزيز .......... حرامية في تايلاند‏ | imgur.com/a/MxxS6 فى محطة مصر | imgur.com/a/xxLOL الباشا تلميذ | imgur.com/a/VS6Ui ابو على | imgur.com/a/GdBOz .......... احمد رزق و احمد عيد .......... اوعى وشك | imgur.com/a/tw0Zk فيلم ثقافى‏ | imgur.com/a/AIgjT .......... احمد ادم.......... الرجل الابيض المتوسط | imgur.com/a/OKOME معلش إحنا بنتبهدل | imgur.com/a/VFUyL .......... احمد حلمى .......... عسل اسود | imgur.com/a/mXJkz على جثتى | imgur.com/a/1xzZT كده رضا | imgur.com/a/dxfdi .......... حماده هلال .......... الهرم الرابع | imgur.com/a/wAhJp عيال حبيبة |imgur.com/a/QpP7R غبي منه فيه | imgur.com/a/hYvug مقلب حرامية | imgur.com/a/bbTvu هى فوضى | imgur.com/a/PMlpR تيمور وشفيقه | imgur.com/a/qjcBf طباخ الريس | imgur.com/a/vIzBa كابتن هيما | imgur.com/a/X3abp حلم العمر | imgur.com/a/h1w4F الناظر | imgur.com/a/UQWH7 ايظن | imgur.com/a/G4q1vA2 Euc | imgur.com/a/wM1rrTE ..... تمبلتس باسم يوسف | imgur.com/a/QoM3E تمبلتس افلام اجنبى | imgur.com/a/m2anE ـــــــــــــــــــــــــــــــــــــــ البوم الكيف (مشهد العزاء) | imgur.com/a/ysymZ البوم عايز حقى (مشهد هواء هواء) | imgur.com/a/rBvFC البوم مرتضى (انا معايا دورى وصوير و 4 كاس) | imgur.com/a/Mf9OK البوم لخمة راس (هو انت ياد مش بتتكيف غير لم تضرب)| imgur.com/a/5v0GK البوم ابو العربى ( مشهد الكباريه ) | imgur.com/a/Ga7kGAY البوم الناظر (اوعى يغرك جسمك) | imgur.com/a/hwjFu ألبوم لن اعيش فى جلباب ابى | imgur.com/a/K2bosbc ـــــــــــــــــــــــــــــــــــــــ تمبلتس اعلانات ........ اتصالات محمد رمضان 2017 | imgur.com/a/Pxuv9 فودافون 2017 | imgur.com/a/BaCF8 حديد الجارحى | imgur.com/a/4uLwR كارير | imgur.com/a/pxapm بوادى | imgur.com/a/QdWKQ ـــــــــــــــــــــــــــــــــــــــ تمبلتس مقصوصة | imgur.com/a/y1bDn ــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ ⛔ كل الألبومات بنعدلها كل يوم وبنضيف عليها كل التمبلتس الجديده | Facebook\"\n",
    "ememesHashes = re.findall(\"imgur.com/a/([A-Za-z]{5})\", fbPostContent)\n",
    "print(ememesHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [Imgur's API](https://api.imgur.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'your-data-here'\n",
    "client_secret = 'your-data-here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"text\",\n",
    "    'Authorization' : \"Client-ID \" + client_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'error': 'Invalid client_id',\n",
      "          'method': 'GET',\n",
      "          'request': '/3/album/i5Msi/images'},\n",
      " 'status': 403,\n",
      " 'success': False}\n"
     ]
    }
   ],
   "source": [
    "jsn = json.loads(requests.get('https://api.imgur.com/3/album/i5Msi/images', headers=headers).text)\n",
    "pprint(jsn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that initially (before I removed cliend_id string) the JSON response was like this: <br><br>\n",
    "<img src='project_media/0. imgur_api_response.png' width=350>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ememesLinks = []\n",
    "def getImgurLinks():\n",
    "    for hash in ememesHashes:\n",
    "        imgsDetailsJson = json.loads(requests.get(f'https://api.imgur.com/3/album/{hash}/images', headers=headers).text)\n",
    "        for img in imgsDetailsJson['data']:\n",
    "            ememesLinks.append(img['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getImgurLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pklSave(ememesLinks, dataDir+\"ememesImgurLinks.pickle\") # store the links, in order not to keep accessing imgur's API each time we run the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247\n"
     ]
    }
   ],
   "source": [
    "ememesLinks = pklLoad(dataDir+\"ememesImgurLinks.pickle\")\n",
    "print(len(ememesLinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://i.imgur.com/EqbCNv1.png'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ememesLinks[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fns = getAsyncImgFunctions(ememesLinks, imgTypes[2], len(ememesLinks)//50) #last argument means this: if #links to download are 10,000 then there will be 50 functions, each will take from link i to link i + 10,000/50 (which is 200)  \n",
    "#if __name__ == '__main__':\n",
    "    #runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Facebook's public meme pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important update: <br>\n",
    "It was discorvered that when not logged-in to Facebook, the scraped images become cropped: <br>\n",
    "<img src='project_media/fbNormalVsLoggedInImages.png' width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the link fetched from a non-logged in browser is different than that from a logged-in browser: <br>\n",
    "<img src='project_media/fbNormalVsLoggedInLinks.png' width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why `isLoggedIn` parameter is provided in `fbPageImgScraper()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbLinks = [ # These are just some Egyptian pages that were gathered\n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%84%D8%A7-%D9%81%D8%A7%D8%A6%D8%AF%D8%A9-%D9%85%D9%86%D9%87%D8%A7-%D9%85%D8%AB%D9%84-%D8%AD%D9%8A%D8%A7%D8%AA%D9%83-290489111460684/photos\",\n",
    "    \"https://www.facebook.com/MemesYard/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/memes.Stolen.1.0/photos/?ref=page_internal\",\n",
    "\n",
    "    \"https://www.facebook.com/%D8%A8%D9%86%D8%B3%D8%B1%D9%82-%D9%85%D9%8A%D9%85%D8%B2-%D9%88%D9%83%D9%88%D9%85%D9%8A%D9%83-%D8%B9%D8%B4%D8%A7%D9%86-%D9%85%D8%B4-%D8%A8%D9%86%D8%B9%D8%B1%D9%81-%D9%86%D8%B9%D9%85%D9%84-1735677470065180/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/profile.php?id=100064850389099&sk=photos\",\n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%84%D9%88%D8%B1%D8%AF-%D9%82%D9%85%D8%AF-100182408618014/photos/?ref=page_internal\",\n",
    "    \n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%85%D8%B5%D8%B1%D9%8A%D9%87-101966151287716/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%85%D8%B4-%D9%87%D9%8A%D9%81%D9%87%D9%85%D9%87%D8%A7-%D8%A7%D9%84%D9%86%D9%88%D8%B1%D9%85%D9%8A%D8%B2-%D8%B9%D8%B4%D8%A7%D9%86-%D9%86%D9%88%D8%B1%D9%85%D9%8A%D8%B2-833501407023909/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/arabicclassicalartmemes/photos/?ref=page_internal\",\n",
    "    \n",
    "    \"https://web.facebook.com/True.Memes.Comics/photos/?ref=page_internal\",\n",
    "    \"https://web.facebook.com/societyforsarcasm/photos/?ref=page_internal\",\n",
    "    \"https://web.facebook.com/memes.officil/photos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitPageBottom(browser):\n",
    "    \"\"\"\n",
    "    Traverses the browser to the end of the page,\n",
    "    and returns the current length of the page.\n",
    "    If an error occurs (ex: no internet), return -1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight); window.scrollBy(0,-200);\") # \"-200\" scrolls up a little, so that websites like FB think that the user is approaching the end, thus load more content\")\n",
    "        lenOfPage = browser.execute_script(\"var lenOfPage=document.body.scrollHeight; return lenOfPage;\")\n",
    "        return lenOfPage\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrollTillEnd(browser, timeTillNextScroll = 3):\n",
    "    \"\"\" \n",
    "    This function uses Helium (or Selenium) browser to scroll \n",
    "    Through a page that dynamically loads content in order to \n",
    "    fully load the html in browser.page_source, example:\n",
    "    https://www.facebook.com/progressofyear/photos\n",
    "    \"\"\"\n",
    "    pageLen = hitPageBottom(browser)\n",
    "    if (pageLen == -1):\n",
    "        return\n",
    "    while(True): # runs until website crashes (pageLen == -1) or we reach the bottom of the page\n",
    "        prevLen = pageLen\n",
    "        time.sleep(timeTillNextScroll)\n",
    "        pageLen = hitPageBottom(browser)\n",
    "        if (pageLen == -1 or pageLen == prevLen):\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbPageName(htmlText):\n",
    "    \"\"\"\n",
    "    Fetches name of a FB page, which upon inspection is usally in \n",
    "    the first <h1>, <h2>, <span> of <h1> or <span> of <h2> \n",
    "    \"\"\"\n",
    "    headerOfPageTitle = re.search('<h[12] .+?(?=/h[12]>)', htmlText)\n",
    "    if (headerOfPageTitle is None):\n",
    "        return \"\"\n",
    "    headerOfPageTitle = headerOfPageTitle.group() #group() to get the entired matched string out of the regex \"match\" object\n",
    "    spanText = re.search('<span>(.*)</span>', headerOfPageTitle)\n",
    "    if (spanText is not None):\n",
    "        pageName = spanText.group(1).strip() # group(1) to return what is between parentheses\n",
    "    else:\n",
    "        pageName = re.search('>(.*)<', headerOfPageTitle).group(1).strip()\n",
    "\n",
    "    pageName = re.sub(r'<.*>', '', pageName) # to make sure no tags remain\n",
    "\n",
    "    return pageName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbExtractImgsLinks(htmlText):\n",
    "    \"\"\"\n",
    "    Extracts links to images in the \"photos\" section of a FB page,\n",
    "    which usually starts from the label \"Photos\" or \"All photos\"\n",
    "    Note, there should be at least #images > 30\n",
    "    \"\"\"\n",
    "    skipPages = 30\n",
    "    try:\n",
    "        htmlStartingFromAllPhotos = re.findall(r'>(?:All )?[pP]hotos.*', htmlText)[0] # Regex to get pages starting from \"All photos\" or \"photos\", but it has to be preceded by \">\", in order not to match \"photos\" in random URLs\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    imgLinks = re.findall(r'https://scontent[^\"]+', htmlStartingFromAllPhotos)\n",
    "    imgLinks = [html.unescape(link) for link in imgLinks][skipPages:]  #avoiding first couple of images, as they might be related to the page itself (logo, etc)\n",
    "    return imgLinks\n",
    "\n",
    "# UNUSED\n",
    "def fbExtractImgsLinksOld(heliumBrowser): # didn't use this, as regex is faster than selenium methods \n",
    "    \"\"\"\n",
    "    Extracts links to images in the \"photos\" section of a FB page,\n",
    "    which usually starts from the label \"Photos\" or \"All photos\"\n",
    "    Note, there should be at least #images > 30\n",
    "    \"\"\"\n",
    "    skipPages = 30\n",
    "    imgTags = heliumBrowser.find_elements_by_tag_name('img')\n",
    "    imgLinks = []\n",
    "    for link in imgTags:\n",
    "        if link.get_attribute('src') is not None:\n",
    "            imgLinks.append(link.get_attribute('src'))\n",
    "    imgLinks = [html.unescape(link) for link in imgLinks][skipPages:] #avoiding first couple of images, as they might be related to the page itself (logo, etc)\n",
    "    return imgLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbPageImgScraper(pagesLinks, heliumBrowser, singlePagePickleName, allPagesPickleName, pagesNamesPickle, prevObtainedPages={}, isLoggedIn=False):\n",
    "    \"\"\"\n",
    "    Uses fbPageName() to get page name from each FB page in the \"pagesLinks\" list,\n",
    "    and  Uses fbExtractImgsLinks() to get all images' links from these pages.\n",
    "    Note that \"prevObtainedPages\" is a dictionary of the already fetched pages, so that \n",
    "    if this function is executed again, it won't scrape previously scraped pages.\n",
    "    At the end, it saves all the links and page names in pickle files.\n",
    "    \"\"\"\n",
    "    if (not isLoggedIn):\n",
    "        heliumBrowser.get('https://www.facebook.com/')\n",
    "        login_to_facebook()\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            click('Block') # to close prompt: \"facebook wants to show notifications\"\n",
    "        except:\n",
    "            pass\n",
    "    allLinks = []\n",
    "    prevObtainedPages = {}\n",
    "    for i in range(len(pagesLinks)):\n",
    "        heliumBrowser.get(pagesLinks[i])\n",
    "        pageName = fbPageName(heliumBrowser.page_source)\n",
    "        if pageName == \"\":\n",
    "            print(\"page not found...\")\n",
    "            continue\n",
    "        if (pageName in prevObtainedPages):\n",
    "            print(\"This page has already been scraped\")\n",
    "            continue\n",
    "        time.sleep(1) # code from here till the try/except is to close the popup that sometimes appear when scrolling down a fb page (while not signed in)\n",
    "        press(PAGE_DOWN) \n",
    "        press(PAGE_DOWN)\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            click('Close')\n",
    "        except Exception as e:\n",
    "            print(\"pop-up didn't  show, moving on...\")\n",
    "        print(\"Scrolling page:\", pageName)\n",
    "        scrollTillEnd(heliumBrowser, 4)\n",
    "        time.sleep(3)\n",
    "        scrollTillEnd(heliumBrowser, 8) # means that FB now starts loading content more slowly, so try again with a bigger time interval between scrolls\n",
    "        imgLinks = fbExtractImgsLinks(heliumBrowser.page_source)\n",
    "        allLinks.extend(imgLinks) #extend() expands the elements of an iterable\n",
    "        pklSave(imgLinks, f\"{dataDir}{singlePagePickleName}{i}.pickle\")\n",
    "        prevObtainedPages[pageName] = (i, pagesLinks[i])\n",
    "        if i == len(pagesLinks)-1:\n",
    "            print(\"Finished scraping pages!\")\n",
    "        else:\n",
    "            print(\"Finished, scraping next link...\", end='\\n\\n')\n",
    "    pklSave(allLinks, f\"{dataDir}{allPagesPickleName}.pickle\")\n",
    "    pklSave(prevObtainedPages, f\"{dataDir}{pagesNamesPickle}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side note: as you see, the function will sequentially check the fb pages, that's because my attempt of checking the pages asynchronously in \"fb_page_img_scraper.py\" have failed\n",
    "# browser = start_chrome(options=options, headless=False)\n",
    "#fbPageImgScraper(fbLinks, browser, \"ememesLinksfbPage\", \"allFbPagesEmemes\", \"fbPagesUsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when `fbPageImgScraper()` was initially called, it saved the links in .pickle files <br>and printed the name of the pages, such as these last 4: <br><br>\n",
    "<img src='project_media/1. fbEMemes.png' width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the memes into \"`2. ememes`\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16996\n"
     ]
    }
   ],
   "source": [
    "# Note 1: I've previously saved 1247 memes (from imgur) in that folder\n",
    "# I want to save files starting from the index after the last saved photo \"ememes0001246.jpg\"\n",
    "# So it will start from \"ememes0001247.jpg\" \n",
    "# Note 2: The current output of this cell is obviously not 1247, because by now I've already added the rest of the images\n",
    "prevFiles = os.listdir(f\"{dataDir}{imgTypes[2]}\")\n",
    "if len(prevFiles) > 0:\n",
    "    lastFileName = prevFiles[-1]\n",
    "    dotLoc = lastFileName.find('.')\n",
    "    offset = int(lastFileName[dotLoc-7:dotLoc])+1\n",
    "    print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Egyptian memes scraped from FB: 15749\n",
      "Total FB pages scraped: 12\n"
     ]
    }
   ],
   "source": [
    "allEmemesLinks = pklLoad(f\"{dataDir}allFbPagesEmemes.pickle\")\n",
    "prevObtainedPages = pklLoad(f\"{dataDir}fbPagesUsed.pickle\")\n",
    "print(\"Total Egyptian memes scraped from FB:\", len(allEmemesLinks))\n",
    "print(\"Total FB pages scraped:\", len(prevObtainedPages))\n",
    "# print(\"Saving images to '2. ememes'\")\n",
    "# fns = getAsyncImgFunctions(allEmemesLinks, imgTypes[2], len(allEmemesLinks)//50, offset)\n",
    "# if __name__ == '__main__':\n",
    "#     runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important update: <br>\n",
    "CDN URLs (such as the ones obtained from Facebook (FB) images) expire after a certain time. This is why any FB pickle files are now useless. <br>\n",
    "[source](\"https://stackoverflow.com/questions/30477877/facebook-image-url-gets-expired#:~:text=What%20i%20came%20to%20know%20from%20other%20community%20about%20this%20issue%20is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://scontent.fcai2-2.fna.fbcdn.net/v/t39.30808-6/270036956_920548311788091_2784946419902323867_n.jpg?_nc_cat=110&ccb=1-7&_nc_sid=8bfeb9&_nc_ohc=dpKopgomzsYAX9N9jeG&_nc_ht=scontent.fcai2-2.fna&oh=00_AT_FRJyzuHGtl-NuogZfhxKwVURhYniqWJ_m0aH8Kqtzog&oe=632350F9'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the \"oe=xxxx\" part at the end specifies the timestamp of expiry (approximately 1 week after the data is obtained)\n",
    "allEmemesLinks[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Egyptian Social Media Images\n",
    "For example, screenshots from FB's posts, twitter's tweets, whatsapp's/messenger's chats <br>\n",
    "Examples of each of these: <br><br>\n",
    "<img src='project_media/2. eSocialMedia.png' width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbLinks = [ # These are just some Egyptian pages that were gathered\n",
    "    'https://www.facebook.com/inbox.ertu/photos/?ref=page_internal',\n",
    "    'https://www.facebook.com/Inbox.Status/photos',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop-up didn't  show, moving on...\n",
      "Scrolling page: Inbox .\n",
      "Finished, scraping next link...\n",
      "\n",
      "pop-up didn't  show, moving on...\n",
      "Scrolling page: Inbox ツ\n",
      "Finished scraping pages!\n"
     ]
    }
   ],
   "source": [
    "#browser = start_chrome(options=options, headless=False)\n",
    "#fbPageImgScraper(fbLinks, browser, \"eSocialMediaLinksfbPage\", \"allFbPagesESocialMedia\", \"fbESocialMediaPagesUsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ESocialMedia links obtained: 6778\n",
      "all FB pages used to get links from: 2\n"
     ]
    }
   ],
   "source": [
    "allESocialMediaLinks = pklLoad(f\"{dataDir}allFbPagesESocialMedia.pickle\")\n",
    "prevObtainedPages = pklLoad(f\"{dataDir}fbESocialMediaPagesUsed.pickle\")\n",
    "print('Total ESocialMedia links obtained:', len(allESocialMediaLinks))\n",
    "print('all FB pages used to get links from:', len(prevObtainedPages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = getAsyncImgFunctions(allESocialMediaLinks, imgTypes[3], len(allESocialMediaLinks)//50)\n",
    "if __name__ == '__main__':\n",
    "   runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Foreign Social Media Images\n",
    "For example, screenshots from FB's posts, twitter's tweets, whatsapp's/messenger's chats <br>\n",
    "Examples: <br><br>\n",
    "<img src='project_media/3. fFb.png' width=300>\n",
    "<img src='project_media/4. fTwtr.png' width=300>\n",
    "<img src='project_media/5. fWhats.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: since all of the cells below are commented <br> \n",
    "(because they've already saved the links to pickle files, so no need to re-run them) <br>\n",
    "here is the output of the `getSubredditImgsLinks()` function used on `bestoftwitter` subreddit: <br><br>\n",
    "<img src='project_media/6. getSubredditImgsLinks() on twtr.png' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditLinks = [ # some links that were gathered\n",
    "    'https://www.reddit.com/r/FacebookCringe/',\n",
    "    'https://www.reddit.com/r/bestoftwitter/',\n",
    "    'https://www.reddit.com/r/texts/',\n",
    "    'https://www.reddit.com/r/GoodFakeTexts/',\n",
    "    'https://www.reddit.com/r/Badfaketexts/'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?size=1000&is_video=false&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    try:\n",
    "        r = requests.get(url, timeout=(10, 20))\n",
    "    except Timeout as t:\n",
    "        print('no internet connection, or api.pushift.io is down')\n",
    "        print(t)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubredditCreationDate(browser):\n",
    "    dateStr = re.search('Created .+?>([^<]*)', browser.page_source)\n",
    "    while dateStr is None:\n",
    "        time.sleep(2)\n",
    "        dateStr = re.search('Created .+?>([^<]*)', browser.page_source)\n",
    "    try:\n",
    "        return dateStr.group(1)\n",
    "    except Exception as e:\n",
    "        return \"Jan 1, 2000\" # a random old date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUnixCode(date):\n",
    "    return int(time.mktime(date.timetuple()))\n",
    "def getDate(dateStr):\n",
    "    return datetime.strptime(dateStr, '%b %d, %Y').date()\n",
    "def imgLinkParser(link):\n",
    "    if re.match('.*(\\.jpg|\\.png|\\.jpeg)', link):\n",
    "        return link\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubredditImgsLinks(subLink, browser, pickleName):\n",
    "    redditImgLinks = []\n",
    "    visitedSubreddits = {}\n",
    "    browser.get(subLink)\n",
    "    subName = re.search('r/([^/]*)', subLink).group(1)\n",
    "    try:\n",
    "        visitedSubreddits = pklLoad(f'{dataDir}visitedSubreddits.pickle')\n",
    "        if subName in visitedSubreddits:\n",
    "            print('This subreddit has already been scraped before!')\n",
    "            return\n",
    "    except:\n",
    "        pass\n",
    "    startDate = getUnixCode(getDate(getSubredditCreationDate(browser)))\n",
    "    endDate = getUnixCode(datetime.now())\n",
    "    data = getPushshiftData(\n",
    "        after = startDate, \n",
    "        before = endDate, \n",
    "        sub = subName\n",
    "        )\n",
    "    # Will run until all posts have been gathered \n",
    "    # from the 'after' date up until before date\n",
    "    while len(data) > 0:\n",
    "        for submission in data:\n",
    "            link = imgLinkParser(submission['url'])\n",
    "            if (len(link) > 0):\n",
    "                redditImgLinks.append(link)\n",
    "        print(\"Current #links:\", len(redditImgLinks))\n",
    "        print('Current last img link:', redditImgLinks[-1])\n",
    "        startDate = data[-1]['created_utc']\n",
    "        print('It\\'s creation date:')\n",
    "        print(datetime.utcfromtimestamp(int(startDate)).strftime('%Y-%m-%d %H:%M:%S')) # convert from unix timestamp to readable date format\n",
    "        data = getPushshiftData(startDate, endDate, subName) # Calls getPushshiftData() with the created date of the last submission\n",
    "    pklSave(redditImgLinks, f'{dataDir}{pickleName}.pickle')\n",
    "    visitedSubreddits[subName] = True\n",
    "    pklSave(visitedSubreddits, f'{dataDir}visitedSubreddits.pickle')\n",
    "    return redditImgLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping screenshots of FB posts from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capabilities = DesiredCapabilities.CHROME\n",
    "# capabilities[\"pageLoadStrategy\"] = \"none\"\n",
    "# browser = start_chrome(options=options, headless=True, capabilities=capabilities)\n",
    "# redditImgLinks = getSubredditImgsLinks(redditLinks[0], browser, \"fFbPostsFromReddit\")\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3128\n"
     ]
    }
   ],
   "source": [
    "redditImgLinks = pklLoad(f'{dataDir}fFbPostsFromReddit.pickle')\n",
    "print(len(redditImgLinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fns = getAsyncImgFunctions(redditImgLinks, imgTypes[4], len(redditImgLinks)//50)\n",
    "# if __name__ == '__main__':\n",
    "#   runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping screenshots of Twitter's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capabilities = DesiredCapabilities.CHROME\n",
    "# capabilities[\"pageLoadStrategy\"] = \"none\"\n",
    "# browser = start_chrome(options=options, headless=True, capabilities=capabilities)\n",
    "# redditImgLinks = getSubredditImgsLinks(redditLinks[1], browser, \"fTwtrTweetsFromReddit\")\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1338\n"
     ]
    }
   ],
   "source": [
    "redditImgLinks = pklLoad(f'{dataDir}fTwtrTweetsFromReddit.pickle')\n",
    "print(len(redditImgLinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fns = getAsyncImgFunctions(redditImgLinks, imgTypes[5], len(redditImgLinks)//50)\n",
    "# if __name__ == '__main__':\n",
    "#   runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping screenshots of chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capabilities = DesiredCapabilities.CHROME\n",
    "# capabilities[\"pageLoadStrategy\"] = \"none\"\n",
    "# browser = start_chrome(options=options, headless=True, capabilities=capabilities)\n",
    "# redditImgLinks = getSubredditImgsLinks(redditLinks[2], browser, \"fChats0FromReddit\")\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16547\n"
     ]
    }
   ],
   "source": [
    "redditImgLinks0 = pklLoad(f'{dataDir}fChats0FromReddit.pickle')\n",
    "print(len(redditImgLinks0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capabilities = DesiredCapabilities.CHROME\n",
    "# capabilities[\"pageLoadStrategy\"] = \"none\"\n",
    "# browser = start_chrome(options=options, headless=True, capabilities=capabilities)\n",
    "# redditImgLinks = getSubredditImgsLinks(redditLinks[3], browser, \"fChats1FromReddit\")\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8435\n"
     ]
    }
   ],
   "source": [
    "redditImgLinks1 = pklLoad(f'{dataDir}fChats1FromReddit.pickle')\n",
    "print(len(redditImgLinks1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditImgLinks = redditImgLinks0 + redditImgLinks1\n",
    "#pklSave(redditImgLinks, f'{dataDir}fChatsAllFromReddit.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24982\n"
     ]
    }
   ],
   "source": [
    "redditImgLinks = pklLoad(f'{dataDir}fChatsAllFromReddit.pickle')\n",
    "print(len(redditImgLinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fns = getAsyncImgFunctions(redditImgLinks, imgTypes[6], len(redditImgLinks)//50)\n",
    "# if __name__ == '__main__':\n",
    "#   runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Greeting & Miscellaneous Images\n",
    "These are images that you see from your family and relatives which you don't need to store for a long period of time :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: the image link provided by google is very long (around 14k characters), <br>\n",
    "I was initially afraid this would slow down the `prevUsedImgLinks` dictionary when finding a key that long, <br>\n",
    "but it turned out [they still preform efficiently :D](https://stackoverflow.com/questions/28150047/efficiency-of-long-str-keys-in-python-dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleImg = 'data:image/jpeg;base64,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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14175"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampleImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `getGooglePagesImgsLinks()` is commented out, I'll display here the output <br>\n",
    "I've obtained when I originally ran the function: <br><br>\n",
    "<img src = 'project_media/7. getGooglePagesImgsLinks() on google images.png' width = 400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevUsedTerms = pklForceLoad(f'{dataDir}prevGoogleSearchTermsDict.pickle')\n",
    "prevUsedImgLinks = pklForceLoad(f'{dataDir}prevGoogleImgsDict.pickle')\n",
    "allGoogleImgsLinks = pklForceLoad(f'{dataDir}allGoogleGAndMImgLinks.pickle', 'list') # same as prevUsedImgLinks, but a list instead of a dict\n",
    "\n",
    "baseLink = 'https://www.google.com/search?q='\n",
    "imgParameters = '&sxsrf=ALiCzsZLmSQIUVB3D-f2Eqdfn85JCe2tDQ:1663483629074&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjkuLO23536AhUegf0HHWbuAEUQ_AUoAXoECAEQAw&biw=1536&bih=722&dpr=1.25'\n",
    "extraChip = '&chips=g_1:'  # new link structure should be: baseLink + searchTerm + extraChip + imgParameters\n",
    "\n",
    "extraChipsDivClassName = 'tzVsfd PKhmud sc-it' # If these don't work, inspect chrome to see new class names and replace these with the new ones!\n",
    "googleImgsImgClassName = 'rg_i Q4LuWd'\n",
    "\n",
    "searchTerms = [\n",
    "    'صباح الخير',\n",
    "    'جمعة مباركة',\n",
    "    'عيد مبارك',\n",
    "    'عيد سعيد',\n",
    "    'رمضان كريم',\n",
    "    'عيد ميلاد سعيد',\n",
    "    'نكت'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarTerms(html, numOfSimilarTerms = 1000): # 1000 is an arbitrary high value\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    soup.find_all('div', extraChipsDivClassName) \n",
    "    cardsText = []\n",
    "    i = 0\n",
    "    for div in soup.find_all('div', extraChipsDivClassName): \n",
    "        if i == numOfSimilarTerms:\n",
    "            break\n",
    "        if div.text not in prevUsedTerms:\n",
    "            cardsText.append(div.text) # don't add this similar term to prevUsedTerms, as it will be added in the getGoogleSinglePageImgsLinks()\n",
    "    return cardsText[:numOfSimilarTerms] # note that [:num_bigger_than_len] will return the full list, not an error, and [:0] will return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def googleScrollTillEnd(browser):\n",
    "    scrollTillEnd(browser)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        click(\"See more anyway\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    scrollTillEnd(browser)\n",
    "\n",
    "    try:\n",
    "        click('Show more results')\n",
    "    except:\n",
    "        pass\n",
    "    scrollTillEnd(browser)\n",
    "    time.sleep(3)\n",
    "    scrollTillEnd(browser) # here, the browser will see \"Looks like you've reached the end\" above the page's footer, or infinite loading svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGoogleSinglePageImgsLinks(googleLink, searchTerm, browser):\n",
    "    similarLinks = 0\n",
    "    if searchTerm in prevUsedTerms:\n",
    "        print(\"Already scraped images related to\", searchTerm, end='\\n\\n')\n",
    "        return []\n",
    "    prevUsedTerms[searchTerm] = True\n",
    "    print(\"Scraping imgs for the term:\", searchTerm)\n",
    "    browser.get(googleLink)\n",
    "    time.sleep(2)\n",
    "    googleScrollTillEnd(browser)\n",
    "    soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "    links = []\n",
    "    for img in soup.find_all('img', googleImgsImgClassName): # If this doesn't work, inspect chrome to see new class name and replace it with this one!\n",
    "        link = img.get_attribute_list('src')[0]\n",
    "        if (link is None):\n",
    "            continue\n",
    "        if (link in prevUsedImgLinks):\n",
    "            similarLinks += 1\n",
    "        else:\n",
    "            prevUsedImgLinks[link] = True\n",
    "            links.append(link)\n",
    "    print(f\"Finished! Found {len(links)} images!\")\n",
    "    if similarLinks > 1:\n",
    "        print(f'However, {similarLinks} of them are duplicates! (will be ignored)')\n",
    "    elif similarLinks == 1:\n",
    "        print(f'However, only 1 of them is a duplicate! (will be ignored)')\n",
    "    print()\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGooglePagesImgsLinks(searchTerms, browser, numOfSimilarTerms = 1000): # 1000 is an arbitrary high value\n",
    "    for searchTerm in searchTerms:\n",
    "        googleLink = f'{baseLink}{searchTerm}{imgParameters}'\n",
    "        links = getGoogleSinglePageImgsLinks(googleLink, searchTerm, browser)\n",
    "        allGoogleImgsLinks.extend(links)\n",
    "\n",
    "        similarTerms = getSimilarTerms(browser.page_source, numOfSimilarTerms)\n",
    "        for term in similarTerms:\n",
    "            googleLink = f'{baseLink}{searchTerm}{extraChip}{term}{imgParameters}'\n",
    "            links = getGoogleSinglePageImgsLinks(googleLink, term, browser)\n",
    "            allGoogleImgsLinks.extend(links)\n",
    "    pklSave(allGoogleImgsLinks, f'{dataDir}allGoogleGAndMImgLinks.pickle')\n",
    "    pklSave(prevUsedImgLinks, f'{dataDir}prevGoogleImgLinksDict.pickle')\n",
    "    pklSave(prevUsedTerms, f'{dataDir}prevGoogleSearchTermsDict.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browser = start_chrome(options=options, headless=False)\n",
    "# getGooglePagesImgsLinks(searchTerms, browser, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17434"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allGoogleImgsLinks = pklLoad(f'{dataDir}allGoogleGAndMImgLinks.pickle')\n",
    "len(allGoogleImgsLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading images from saved links\n",
    "Note: the logic of changing the image format from `base64` to `binary` is by the author \"Adakole Emmanuel Audu\" <br>\n",
    "More about the topic [here](https://medium.com/geekculture/scraping-google-image-search-result-dfe01bcbc610)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = getAsyncImgFunctions(allGoogleImgsLinks, imgTypes[7], len(allGoogleImgsLinks)//50)\n",
    "if __name__ == '__main__':\n",
    "  runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Reddit's \"deleted\" & corrupted images\n",
    "As you can see: <br> <br>\n",
    "<img src = 'project_media/8. reddit deleted.png' width = 500>\n",
    "<br>\n",
    "Some of the downloaded images were deleted from Reddit's servers [for various possible reasons](https://www.reddit.com/r/NoStupidQuestions/comments/j2jkel/when_reddit_says_if_you_were_looking_for_an_image/) <br>\n",
    "Note: all these images have the same dimensions (130x60) for width and height respectively, <br>\n",
    "so the images will be deleted upon this criteria <br>\n",
    "Note 2: Even though some real images could be lost, it is highly improbable that an image will be exactly 130x60, <br>\n",
    "so I'm willing to remove the very few images that are so.\n",
    "<br><br>\n",
    "Another problem is that some of the images are corrupted (i.e cv2 can't open them)\n",
    "<br><br>\n",
    "So `cleanImgs()` will remove these images <br>\n",
    "and update the images' suffix (e.g: img0000003 => img0000001) and pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanImgs(directoryPaths, picklesPaths = None, isReddit = False): \n",
    "    \"\"\"\n",
    "    removes bad images like corrupted or \"deleted\" images (in case of reddit) from specified directory paths, and updates their \n",
    "    corresponding pickle files (IMP note: \"picklesPath\" is a list of paths to pickle files\n",
    "    that should NOT include the \".pickle\" extension at the end of the string)\n",
    "    \"\"\"\n",
    "    for i in range(len(directoryPaths)):\n",
    "        try:\n",
    "            deletedIds = []\n",
    "            corruptedImgs = 0\n",
    "            imgsPaths = [os.path.join(directoryPaths[i], imgFile) for imgFile in os.listdir(directoryPaths[i])]\n",
    "            print(f'Before cleaning \"{directoryPaths[i]}\": {len(imgsPaths)} images..')\n",
    "            for imgPath in imgsPaths:\n",
    "                img = cv2.imread(imgPath)\n",
    "                id = int(re.search('(\\d{7})(?=\\.jpg|\\.png|\\.jpeg)', imgPath).group(1))\n",
    "                if img is None:\n",
    "                    os.remove(imgPath)\n",
    "                    deletedIds.append(id)\n",
    "                    corruptedImgs += 1\n",
    "                    continue\n",
    "                if isReddit and img.shape[0] == 60 and img.shape[1] == 130:\n",
    "                    os.remove(imgPath)\n",
    "                    deletedIds.append(id)\n",
    "\n",
    "            if len(deletedIds) == 0:\n",
    "                print('no bad images found!!', end = '\\n\\n')\n",
    "                continue\n",
    "            if isReddit:\n",
    "                imgsUrls = pklForceLoad(f'{picklesPaths[i]}.pickle', 'list')\n",
    "                for idx in reversed(deletedIds):\n",
    "                    imgsUrls.pop(idx)\n",
    "                pklSave(imgsUrls, f'{picklesPaths[i]}Updated.pickle')\n",
    "            if corruptedImgs > 0:\n",
    "                    print(f'{corruptedImgs} corrupted images were found!')\n",
    "\n",
    "            newImgsPaths = [os.path.join(directoryPaths[i], imgFile) for imgFile in os.listdir(directoryPaths[i])]\n",
    "            print(f'After cleaning: {len(newImgsPaths)} images..', end = '\\n\\n')\n",
    "            for j in range(len(newImgsPaths)):\n",
    "                newName = re.sub('(\\d{7})(?=\\.jpg|\\.png|\\.jpeg)', format(j, '07d'), newImgsPaths[j])\n",
    "                os.rename(newImgsPaths[j], newName)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: 3022 images..\n",
      "After cleaning: 2113 images..\n",
      "\n",
      "Before cleaning: 1259 images..\n",
      "After cleaning: 1007 images..\n",
      "\n",
      "Before cleaning: 23887 images..\n",
      "22 corrupted images were found!\n",
      "After cleaning: 17280 images..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# redditDirectoriesToClean = [\n",
    "#     'dataset/4. fFbPosts/',\n",
    "#     'dataset/5. fTwtrPosts/',\n",
    "#     'dataset/6. fTxtMssgs/',\n",
    "# ]\n",
    "# oldPickleFiles = [\n",
    "#     'dataset/fFbPostsFromReddit',\n",
    "#     'dataset/fTwtrTweetsFromReddit',\n",
    "#     'dataset/fChatsAllFromReddit'\n",
    "# ]\n",
    "# cleanImgs(redditDirectoriesToClean, oldPickleFiles, isReddit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these results were displayed when `cleanImgs()` was called: <br><br>\n",
    "<img src = 'project_media/9. cleanImgs(reddit).png' width = 230>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning corrupted images that didn't come from Reddit\n",
    "For example, in `ememes` folder, there are some corrupted images: <br>\n",
    "<img src=\"project_media/10. corrupted images.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning \"dataset/1. fmemes/\": 32513 images..\n",
      "1497 corrupted images were found!\n",
      "After cleaning: 31016 images..\n",
      "\n",
      "Before cleaning \"dataset/2. ememes/\": 16018 images..\n",
      "1900 corrupted images were found!\n",
      "After cleaning: 14118 images..\n",
      "\n",
      "Before cleaning \"dataset/3. eSocialMedia/\": 6713 images..\n",
      "no bad images found!!\n",
      "\n",
      "Before cleaning \"dataset/7. eGreetingAndMisc/\": 17044 images..\n",
      "no bad images found!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# otherDirectoriesToClean = [\n",
    "#     'dataset/1. fmemes/',\n",
    "#     'dataset/2. ememes/',\n",
    "# ]\n",
    "# cleanImgs(otherDirectoriesToClean)\n",
    "# # unxpected error occured, so will clean 2. ememes again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning \"dataset/2. ememes/\": 14118 images..\n",
      "no bad images found!!\n",
      "\n",
      "Before cleaning \"dataset/3. eSocialMedia/\": 6713 images..\n",
      "no bad images found!!\n",
      "\n",
      "Before cleaning \"dataset/7. eGreetingAndMisc/\": 17044 images..\n",
      "no bad images found!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# otherDirectoriesToClean = [\n",
    "#     'dataset/2. ememes/',\n",
    "#     'dataset/3. eSocialMedia/',\n",
    "#     'dataset/7. eGreetingAndMisc/'\n",
    "# ]\n",
    "# cleanImgs(otherDirectoriesToClean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Image Properties\n",
    "Now, we want to decide how much downscaling/upscaling we want to do to our images, so we could do so understanding the general sizes of the images in our dataset <br>\n",
    "most of the logic below is by [Aravind Ramalingam](https://medium.com/analytics-vidhya/how-to-pick-the-optimal-image-size-for-training-convolution-neural-network-65702b880f05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('d:/CS/projects/graduation_project/dataset/0. normal'),\n",
       " WindowsPath('d:/CS/projects/graduation_project/dataset/1. fmemes'),\n",
       " WindowsPath('d:/CS/projects/graduation_project/dataset/2. ememes'),\n",
       " WindowsPath('d:/CS/projects/graduation_project/dataset/3. eSocialMedia'),\n",
       " WindowsPath('d:/CS/projects/graduation_project/dataset/4. fFbPosts'),\n",
       " WindowsPath('d:/CS/projects/graduation_project/dataset/5. fTwtrPosts'),\n",
       " WindowsPath('d:/CS/projects/graduation_project/dataset/6. fTxtMssgs'),\n",
       " WindowsPath('d:/CS/projects/graduation_project/dataset/7. eGreetingAndMisc')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetPaths = [dp.absolute() for dp in Path(dataDir).iterdir() if re.match(\"^\\d+\\. \", dp.name)]\n",
    "datasetPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Image Resolutions\n",
    "\n",
    "def getImgsMetaInDf(datasetPaths):\n",
    "    dfs = []\n",
    "    for dp in datasetPaths:\n",
    "        # Get the Image Resolutions\n",
    "        imgs = [img.name for img in dp.iterdir()]\n",
    "        img_meta = {}\n",
    "        for imgName in imgs: \n",
    "            img_meta[str(imgName)] = imagesize.get(dp.as_posix() + imgName) # as_posix() converts WindowsPath object to just the string of the absolute path\n",
    "\n",
    "        # Convert it to Dataframe and compute aspect ratio\n",
    "        img_meta_df = pd.DataFrame.from_dict([img_meta]).T.reset_index().set_axis(['FileName', 'Size'], axis='columns', inplace=False)\n",
    "        img_meta_df[[\"Width\", \"Height\"]] = pd.DataFrame(img_meta_df[\"Size\"].tolist(), index=img_meta_df.index)\n",
    "        img_meta_df[\"Aspect Ratio\"] = round(img_meta_df[\"Width\"] / img_meta_df[\"Height\"], 2)\n",
    "        \n",
    "\n",
    "        print(f'Total Nr of Images in the dataset {dp.name}: {len(img_meta_df)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5fdb5dc06c166b55a77b4f45331a96b4e71d62bbf9d2ad2353a8ee537ba97fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
