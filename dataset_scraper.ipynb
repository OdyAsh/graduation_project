{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import regex as re\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from helium import *\n",
    "import praw\n",
    "\n",
    "import red_downloader # used to scrape reddit images. Credit: JackhammerYT (https://github.com/JackhammerYT/RedDownloader)\n",
    "import download_imgs # used in getAsyncImgFunctions()\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0. normal',\n",
       " '1. fmemes',\n",
       " '2. ememes',\n",
       " '3. eSocialMedia',\n",
       " '4. fFbPosts',\n",
       " '5. fTwtrPosts',\n",
       " '6. fTxtMssgs',\n",
       " '7. eGreetingAndMisc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDir = 'dataset/'\n",
    "imgTypes = list(filter(lambda v: re.match('\\d{1,3}\\. ', v), os.listdir(dataDir)))\n",
    "imgTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pklSave(contentToBeSaved, fullPath):\n",
    "    with open(fullPath, 'wb') as f:\n",
    "        pickle.dump(contentToBeSaved, f)\n",
    "\n",
    "def pklLoad(fullPath):\n",
    "    with open(fullPath, 'rb') as f:\n",
    "        contentToBeLoaded = pickle.load(f)\n",
    "    return contentToBeLoaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up images and Labels' dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0. normal': 0,\n",
       " '1. fmemes': 1,\n",
       " '2. ememes': 2,\n",
       " '3. eSocialMedia': 3,\n",
       " '4. fFbPosts': 4,\n",
       " '5. fTwtrPosts': 5,\n",
       " '6. fTxtMssgs': 6,\n",
       " '7. eGreetingAndMisc': 7}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgTypeToNum = {}\n",
    "for  i, type in enumerate(imgTypes):\n",
    "    imgTypeToNum[type] = i\n",
    "imgTypeToNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pklSave(imgTypeToNum, dataDir+\"imgTypeToNum.pickle\") # to be later used in another .ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Foreign Memes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From [Kaggle's Meme Generator Dataset](https://www.kaggle.com/datasets/electron0zero/memegenerator-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting memes' URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Meme ID</th>\n",
       "      <th>Base Meme Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10509464</td>\n",
       "      <td>Spiderman Approves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12285257</td>\n",
       "      <td>Alright Then Business Kid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20612245</td>\n",
       "      <td>Archer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20614628</td>\n",
       "      <td>Futurama Fry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24194267</td>\n",
       "      <td>One Does Not Simply</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Meme ID             Base Meme Name\n",
       "0  10509464         Spiderman Approves\n",
       "1  12285257  Alright Then Business Kid\n",
       "2  20612245                     Archer\n",
       "3  20614628               Futurama Fry\n",
       "4  24194267        One Does Not Simply"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmemesCsvLinks = pd.read_csv(\"dataset/memegenerator.csv\", usecols=[\"Meme ID\", \"Base Meme Name\"]).squeeze() # convert to Series\n",
    "fmemesCsvLinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspection, we want to get the original template of memes, not its varientswe'll find that to fetch a page of only the meme image, the URL has to be in this format <br>\n",
    "`https://memegenerator.net/img/instances/XX.jpg` <br>\n",
    "Where `XX` is the unique id of the image on [Meme Generator](https://memegenerator.net/) <br>\n",
    "Thus, we will use the `Meme ID` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10509464\n",
       "1    12285257\n",
       "2    20612245\n",
       "3    20614628\n",
       "4    24194267\n",
       "Name: Meme ID, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmemesIds = fmemesCsvLinks['Meme ID']\n",
    "fmemesIds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://memegenerator.net/img/instances/10509464.jpg\n",
       "1    https://memegenerator.net/img/instances/12285257.jpg\n",
       "2    https://memegenerator.net/img/instances/20612245.jpg\n",
       "3    https://memegenerator.net/img/instances/20614628.jpg\n",
       "4    https://memegenerator.net/img/instances/24194267.jpg\n",
       "Name: Meme ID, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 70)\n",
    "fmemesLinks = 'https://memegenerator.net/img/instances/' + fmemesIds + '.jpg'\n",
    "fmemesLinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the foreign images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getAsyncImgFunctions(imgsLinks, imgType, interval, offset=0):\n",
    "    fns = []\n",
    "    i = 0\n",
    "    check = True\n",
    "    while i < len(imgsLinks):\n",
    "        fns.append((download_imgs.downloadImgs, (imgType, imgsLinks, i, i+interval, offset)))\n",
    "        i += interval\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = getAsyncImgFunctions(fmemesLinks, imgTypes[1], 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "# Using runInParallel() below, we will run xx functions parallely to fetch data\n",
    "print(len(fns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runInParallel(fns):\n",
    "  proc = []\n",
    "  for fn in fns:\n",
    "    p = Process(target=fn[0], args=(fn[1]))\n",
    "    p.start()\n",
    "    proc.append(p)\n",
    "  for p in proc:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "    #runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Egyptian Memes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Imgur using this [Facebook post]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HxfPF', 'ilzyz', 'lhDIg', 'mMJkA', 'nqdmP', 'XsucN', 'fUQNf', 'NPUrK', 'UeRnH', 'wMQTL', 'xxLOL', 'GdBOz', 'AIgjT', 'OKOME', 'VFUyL', 'mXJkz', 'dxfdi', 'wAhJp', 'hYvug', 'bbTvu', 'PMlpR', 'qjcBf', 'vIzBa', 'ysymZ', 'rBvFC', 'hwjFu', 'pxapm', 'QdWKQ']\n"
     ]
    }
   ],
   "source": [
    "fbPostContent = \"Templates HQ - المنتدى ( ألبومات تمبلتس _ مستلزمات الكوميك _ شروحات _ افضل المواقع _ مستلزمات التصميم _ ايموشنز _ وشوش ) : plus.google.com/u/0/communities/108675057016512986137 ____ ألبومات تمبلتس :(تمبلتس | مقصوصه ، افلام ، مسلسلات ، مشاهد ، برامج ) ___________ .......... محمد هنيدى .......... رمضان مبروك ابو العالمين حمودة | imgur.com/a/HxfPF اسماعيلية رايح جى | imgur.com/a/i5Msi فول الصين العظيم | imgur.com/a/C8OLI جائنا البيان التالى | imgur.com/a/r4X66 صاحب صاحبه | imgur.com/a/ilzyz يا انا يا خالتى | imgur.com/a/sTx1q امير البحار | imgur.com/a/mRYs0 وش اجرام | imgur.com/a/lhDIg .......... محمد سعد .......... اللي بالي بالك | imgur.com/a/mMJkA اللمبى‏ | imgur.com/a/kNwD8 بوحه | imgur.com/a/nqdmP .......... عادل امام .......... عريس من جهة امنيه | imgur.com/a/h4pA7 السفارة فى العمارة | imgur.com/a/Y519O سلام يا صاحبى | imgur.com/a/XsucN زهايمر | imgur.com/a/iLUE0 .......... احمد مكى .......... لا تراجع ولا استسلام | imgur.com/a/Ri6QP الكبير اوى | imgur.com/a/fUQNf طير انت | imgur.com/a/NPUrK .......... شيكو و هشام ماجد و أحمد فهمي .......... سمير وشهير وبهير | imgur.com/a/UeRnH حملة فريزر | imgur.com/a/bKB5D بنات العم | imgur.com/a/wMQTL .......... كريم عبد العزيز .......... حرامية في تايلاند‏ | imgur.com/a/MxxS6 فى محطة مصر | imgur.com/a/xxLOL الباشا تلميذ | imgur.com/a/VS6Ui ابو على | imgur.com/a/GdBOz .......... احمد رزق و احمد عيد .......... اوعى وشك | imgur.com/a/tw0Zk فيلم ثقافى‏ | imgur.com/a/AIgjT .......... احمد ادم.......... الرجل الابيض المتوسط | imgur.com/a/OKOME معلش إحنا بنتبهدل | imgur.com/a/VFUyL .......... احمد حلمى .......... عسل اسود | imgur.com/a/mXJkz على جثتى | imgur.com/a/1xzZT كده رضا | imgur.com/a/dxfdi .......... حماده هلال .......... الهرم الرابع | imgur.com/a/wAhJp عيال حبيبة |imgur.com/a/QpP7R غبي منه فيه | imgur.com/a/hYvug مقلب حرامية | imgur.com/a/bbTvu هى فوضى | imgur.com/a/PMlpR تيمور وشفيقه | imgur.com/a/qjcBf طباخ الريس | imgur.com/a/vIzBa كابتن هيما | imgur.com/a/X3abp حلم العمر | imgur.com/a/h1w4F الناظر | imgur.com/a/UQWH7 ايظن | imgur.com/a/G4q1vA2 Euc | imgur.com/a/wM1rrTE ..... تمبلتس باسم يوسف | imgur.com/a/QoM3E تمبلتس افلام اجنبى | imgur.com/a/m2anE ـــــــــــــــــــــــــــــــــــــــ البوم الكيف (مشهد العزاء) | imgur.com/a/ysymZ البوم عايز حقى (مشهد هواء هواء) | imgur.com/a/rBvFC البوم مرتضى (انا معايا دورى وصوير و 4 كاس) | imgur.com/a/Mf9OK البوم لخمة راس (هو انت ياد مش بتتكيف غير لم تضرب)| imgur.com/a/5v0GK البوم ابو العربى ( مشهد الكباريه ) | imgur.com/a/Ga7kGAY البوم الناظر (اوعى يغرك جسمك) | imgur.com/a/hwjFu ألبوم لن اعيش فى جلباب ابى | imgur.com/a/K2bosbc ـــــــــــــــــــــــــــــــــــــــ تمبلتس اعلانات ........ اتصالات محمد رمضان 2017 | imgur.com/a/Pxuv9 فودافون 2017 | imgur.com/a/BaCF8 حديد الجارحى | imgur.com/a/4uLwR كارير | imgur.com/a/pxapm بوادى | imgur.com/a/QdWKQ ـــــــــــــــــــــــــــــــــــــــ تمبلتس مقصوصة | imgur.com/a/y1bDn ــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ ⛔ كل الألبومات بنعدلها كل يوم وبنضيف عليها كل التمبلتس الجديده | Facebook\"\n",
    "ememesHashes = re.findall(\"imgur.com/a/([A-Za-z]{5})\", fbPostContent)\n",
    "print(ememesHashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [Imgur's API](https://api.imgur.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'your-data-here'\n",
    "client_secret = 'your-data-here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"text\",\n",
    "    'Authorization' : \"Client-ID \" + client_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'error': 'Invalid client_id',\n",
      "          'method': 'GET',\n",
      "          'request': '/3/album/i5Msi/images'},\n",
      " 'status': 403,\n",
      " 'success': False}\n"
     ]
    }
   ],
   "source": [
    "jsn = json.loads(requests.get('https://api.imgur.com/3/album/i5Msi/images', headers=headers).text)\n",
    "pprint(jsn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that initially (before I removed cliend_id string) the JSON response was like this: <br><br>\n",
    "<img src='project_media/0. imgur_api_response.png' width=350>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ememesLinks = []\n",
    "def getImgurLinks():\n",
    "    for hash in ememesHashes:\n",
    "        imgsDetailsJson = json.loads(requests.get(f'https://api.imgur.com/3/album/{hash}/images', headers=headers).text)\n",
    "        for img in imgsDetailsJson['data']:\n",
    "            ememesLinks.append(img['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getImgurLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pklSave(ememesLinks, dataDir+\"ememesImgurLinks.pickle\") # store the links, in order not to keep accessing imgur's API each time we run the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ememesLinks = pklLoad(dataDir+\"ememesImgurLinks.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fns = getAsyncImgFunctions(ememesLinks, imgTypes[2], len(ememesLinks)//50) #last argument means this: if #links to download are 10,000 then there will be 50 functions, each will take from link i to link i + 10,000/50 (which is 200)  \n",
    "#if __name__ == '__main__':\n",
    "    #runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Facebook's public meme pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbLinks = [ # These are just some Egyptian pages that were gathered\n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%84%D8%A7-%D9%81%D8%A7%D8%A6%D8%AF%D8%A9-%D9%85%D9%86%D9%87%D8%A7-%D9%85%D8%AB%D9%84-%D8%AD%D9%8A%D8%A7%D8%AA%D9%83-290489111460684/photos\",\n",
    "    \"https://www.facebook.com/MemesYard/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/memes.Stolen.1.0/photos/?ref=page_internal\",\n",
    "\n",
    "    \"https://www.facebook.com/%D8%A8%D9%86%D8%B3%D8%B1%D9%82-%D9%85%D9%8A%D9%85%D8%B2-%D9%88%D9%83%D9%88%D9%85%D9%8A%D9%83-%D8%B9%D8%B4%D8%A7%D9%86-%D9%85%D8%B4-%D8%A8%D9%86%D8%B9%D8%B1%D9%81-%D9%86%D8%B9%D9%85%D9%84-1735677470065180/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/profile.php?id=100064850389099&sk=photos\",\n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%84%D9%88%D8%B1%D8%AF-%D9%82%D9%85%D8%AF-100182408618014/photos/?ref=page_internal\",\n",
    "    \n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%85%D8%B5%D8%B1%D9%8A%D9%87-101966151287716/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/%D9%85%D9%8A%D9%85%D8%B2-%D9%85%D8%B4-%D9%87%D9%8A%D9%81%D9%87%D9%85%D9%87%D8%A7-%D8%A7%D9%84%D9%86%D9%88%D8%B1%D9%85%D9%8A%D8%B2-%D8%B9%D8%B4%D8%A7%D9%86-%D9%86%D9%88%D8%B1%D9%85%D9%8A%D8%B2-833501407023909/photos/?ref=page_internal\",\n",
    "    \"https://www.facebook.com/arabicclassicalartmemes/photos/?ref=page_internal\",\n",
    "    \n",
    "    \"https://web.facebook.com/True.Memes.Comics/photos/?ref=page_internal\",\n",
    "    \"https://web.facebook.com/societyforsarcasm/photos/?ref=page_internal\",\n",
    "    \"https://web.facebook.com/memes.officil/photos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrollTillEnd(browser):\n",
    "    \"\"\" \n",
    "    This function uses Helium (or Selenium) browser to scroll \n",
    "    Through a page that dynamically loads content in order to \n",
    "    fully load the html in browser.page_source, example:\n",
    "    https://www.facebook.com/progressofyear/photos\n",
    "    \"\"\"\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    lenOfPage = browser.execute_script(\"var lenOfPage=document.body.scrollHeight; return lenOfPage;\")\n",
    "    match=False\n",
    "    while(match==False):\n",
    "        lastCount = lenOfPage\n",
    "        time.sleep(3)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight); window.scrollBy(0,-200);\") # \"-200\" scrolls up a little, so that FB realizes that the user is approaching the end, thus load more content\n",
    "        lenOfPage = browser.execute_script(\"var lenOfPage=document.body.scrollHeight; return lenOfPage;\")\n",
    "        if lastCount==lenOfPage:\n",
    "            match=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbPageName(htmlText):\n",
    "    \"\"\"\n",
    "    Fetches name of a FB page, which upon inspection is usally in \n",
    "    the first <h1>, <h2>, <span> of <h1> or <span> of <h2> \n",
    "    \"\"\"\n",
    "    headerOfPageTitle = re.search('<h[12] .+?(?=/h[12]>)', htmlText)\n",
    "    if (headerOfPageTitle is None):\n",
    "        return \"\"\n",
    "    headerOfPageTitle = headerOfPageTitle.group() #group() to get the entired matched string out of the regex \"match\" object\n",
    "    spanText = re.search('<span>(.*)</span>', headerOfPageTitle)\n",
    "    if (spanText is not None):\n",
    "        pageName = spanText.group(1).strip() # group(1) to return what is between parentheses\n",
    "    else:\n",
    "        pageName = re.search('>(.*)<', headerOfPageTitle).group(1).strip()\n",
    "    return pageName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbExtractImgsLinks(htmlText):\n",
    "    \"\"\"\n",
    "    Extracts links to images in the \"photos\" section of a FB page,\n",
    "    which usually starts from the label \"Photos\" or \"All photos\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        htmlStartingFromAllPhotos = re.findall(r'>(?:All )?[pP]hotos.*', htmlText)[0] # Regex to get pages starting from \"All photos\" or \"photos\", but it has to be preceded by \">\", in order not to match \"photos\" in random URLs\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    imgLinks = re.findall(r'https://scontent[^\"]+', htmlStartingFromAllPhotos)\n",
    "    imgLinks = [html.unescape(link) for link in imgLinks][1:] #avoiding first image, as it might be the page's logo\n",
    "    return imgLinks\n",
    "\n",
    "def fbExtractImgsLinksOld(heliumBrowser): # didn't use this, as regex is faster than selenium methods \n",
    "    imgTags = heliumBrowser.find_elements_by_tag_name('img')\n",
    "    imgLinks = []\n",
    "    for link in imgTags:\n",
    "        if link.get_attribute('src') is not None:\n",
    "            imgLinks.append(link.get_attribute('src'))\n",
    "    imgLinks = [html.unescape(link) for link in imgLinks][1:] #avoiding first image, as it might be the page's logo\n",
    "    return imgLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbPageImgScraper(pagesLinks, heliumBrowser, singlePagePickleName, allPagesPickleName, pagesNamesPickle, prevObtainedPages={}):\n",
    "    \"\"\"\n",
    "    Uses fbPageName() to get page name from each FB page in the \"pagesLinks\" list,\n",
    "    and  Uses fbExtractImgsLinks() to get all images' links from these pages.\n",
    "    Note that \"prevObtainedPages\" is a dictionary of the already fetched pages, so that \n",
    "    if this function is executed again, it won't scrape previously scraped pages.\n",
    "    At the end, it saves all the links and page names in pickle files.\n",
    "    \"\"\"\n",
    "    allLinks = []\n",
    "    prevObtainedPages = {}\n",
    "    for i in range(len(pagesLinks)):\n",
    "        heliumBrowser.get(pagesLinks[i])\n",
    "        pageName = fbPageName(heliumBrowser.page_source)\n",
    "        if pageName == \"\":\n",
    "            print(\"page not found...\")\n",
    "            continue\n",
    "        if (pageName in prevObtainedPages):\n",
    "            print(\"This page has already been scraped\")\n",
    "            continue\n",
    "        time.sleep(1) # code from here till the try/except is to close the popup that sometimes appear when scrolling down a fb page (while not signed in)\n",
    "        press(PAGE_DOWN) \n",
    "        press(PAGE_DOWN)\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            click('Close')\n",
    "        except Exception as e:\n",
    "            print(\"pop-up didn't  show, moving on...\")\n",
    "        print(\"Scrolling page:\", pageName)\n",
    "        scrollTillEnd(heliumBrowser)\n",
    "        imgLinks = fbExtractImgsLinks(heliumBrowser.page_source)\n",
    "        allEmemesLinks.extend(imgLinks) #extend() expands the elements of an iterable\n",
    "        pklSave(imgLinks, f\"{dataDir}{singlePagePickleName}{i}.pickle\")\n",
    "        prevObtainedPages[pageName] = (i, pagesLinks[i])\n",
    "        if i == len(pagesLinks)-1:\n",
    "            print(\"Finished scraping pages!\")\n",
    "        else:\n",
    "            print(\"Finished, scraping next link...\", end='\\n\\n')\n",
    "    pklSave(allEmemesLinks, f\"{dataDir}{allPagesPickleName}.pickle\")\n",
    "    pklSave(prevObtainedPages, f\"{dataDir}{pagesNamesPickle}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side note: as you see, the function will sequentially check the fb pages, that's because my attempt of checking the pages asynchronously in \"fb_page_img_scraper.py\" have failed\n",
    "#browser = start_chrome(headless=False)\n",
    "#fbPageImgScraper(fbLinks, browser, \"ememesLinksfbPage\", \"allFbPagesEmemes\", \"fbEMemesPagesUsed\")\n",
    "#browser.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when `fbPageImgScraper()` was initially called, it saved the links in .pickle files <br>and printed the name of the pages, such as these last 4: <br><br>\n",
    "<img src='project_media/1. fbEMemes.png' width=350>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "allEmemesLinks = pklLoad(f\"{dataDir}allFbPagesEmemes.pickle\")\n",
    "prevObtainedPages = pklLoad(f\"{dataDir}fbEMemesPagesUsed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Egyptian memes scraped from FB: 15749\n",
      "Total FB pages scraped: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Egyptian memes scraped from FB:\", len(allEmemesLinks))\n",
    "print(\"Total FB pages scraped:\", len(prevObtainedPages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the memes into \"`2. ememes`\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1247\n"
     ]
    }
   ],
   "source": [
    "# Note 1: I've previously saved 1247 memes (from imgur) in that folder\n",
    "# I want to save files starting from the index after the last saved photo \"ememes0001246.jpg\"\n",
    "# So it will start from \"ememes0001247.jpg\" \n",
    "# Note 2: The output of this cell is obviously not 1247, because by now I've already added the rest of the images\n",
    "prevFiles = os.listdir(f\"{dataDir}{imgTypes[2]}\")\n",
    "if len(prevFiles) > 0:\n",
    "    lastFileName = prevFiles[-1]\n",
    "    dotLoc = lastFileName.find('.')\n",
    "    offset = int(lastFileName[dotLoc-7:dotLoc])+1\n",
    "    print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fns = getAsyncFunctions(allEmemesLinks, imgTypes[2], len(allEmemesLinks)//50, offset)\n",
    "#if __name__ == '__main__':\n",
    "#    runInParallel(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to-do: \n",
    "# Uncomment and execute the cell above this one\n",
    "# scrape links of these new folders; one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Egyptian Social Media Images\n",
    "For example, screenshots from FB's posts, twitter's tweets, whatsapp's/messenger's chats <br>\n",
    "Examples of each of these: <br><br>\n",
    "<img src='project_media/2. eSocialMedia.png' width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbLinks = [ # These are just some Egyptian pages that were gathered\n",
    "    'https://www.facebook.com/inbox.ertu/photos/?ref=page_internal',\n",
    "    'https://www.facebook.com/Inbox.Status/photos',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browser = start_chrome(headless=False)\n",
    "#fbPageImgScraper(fbLinks, browser, \"eSocialMediaLinksfbPage\", \"allFbPagesESocialMedia\", \"fbESocialMediaPagesUsed\")\n",
    "#browser.quit()\n",
    "# to-do: run the above lines ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allESocialMediaLinks = pklLoad(f\"{dataDir}allFbPagesESocialMedia.pickle\")\n",
    "prevObtainedPages = pklLoad(f\"{dataDir}fbESocialMediaPagesUsed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fns = getAsyncFunctions(allEmemesLinks, imgTypes[3], len(allESocialMediaLinks)//50)\n",
    "#if __name__ == '__main__':\n",
    "#    runInParallel(fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Foreign Social Media Images\n",
    "For example, screenshots from FB's posts, twitter's tweets, whatsapp's/messenger's chats <br>\n",
    "Examples: <br><br>\n",
    "<img src='project_media/3. fFb.png' width=300>\n",
    "<img src='project_media/4. fTwtr.png' width=300>\n",
    "<img src='project_media/5. fWhats.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditLinks = [\n",
    "    'https://www.reddit.com/r/FacebookCringe/',\n",
    "    'https://www.reddit.com/r/bestoftwitter/',\n",
    "    'https://www.reddit.com/r/texts/',\n",
    "    'https://www.reddit.com/r/GoodFakeTexts/',\n",
    "    'https://www.reddit.com/r/Badfaketexts/'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping screenshots of FB posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Posts...\n",
      "Unable to fetch posts\n",
      "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<red_downloader.DownloadImagesBySubreddit at 0x1cc409775b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_downloader.DownloadImagesBySubreddit('FacebookCringe', 3000, SortBy='new', quality=360, output='fFbPost', destination=f'{dataDir}{imgTypes[4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping screenshots of Twitter Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping screenshots of Chat Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5fdb5dc06c166b55a77b4f45331a96b4e71d62bbf9d2ad2353a8ee537ba97fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
