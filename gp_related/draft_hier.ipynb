{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft code used to be in class MyLogger(Logger)'s __init()__ method\n",
    "# used to set up initial history keys\n",
    "num_output_layers = 6\n",
    "\n",
    "self.num_output_layers = num_output_layers\n",
    "        history_per_output_layer = {\n",
    "            # per epoch\n",
    "            'loss':[],\n",
    "            'acc':[],\n",
    "            'f1_score':[],\n",
    "            # these 2 will be replaced with step_y_pred and step_y_true respectively\n",
    "            'y_pred':[],\n",
    "            'y_true':[],\n",
    "        } \n",
    "\n",
    "def pre_suf_fix_dict(dict, fix_val='output_layer', prefix=True):\n",
    "    return {f\"{fix_val}_{key}\" if prefix else f\"{key}_{fix_val}\" : value for key, value in dict.items()}\n",
    "\n",
    "# to create val/test prefixed keys\n",
    "history_per_output_layer.update(\n",
    "    **pre_suf_fix_dict(history_per_output_layer, fix_val='val', prefix=True),\n",
    "    **pre_suf_fix_dict(history_per_output_layer, fix_val='test', prefix=True),\n",
    ")\n",
    "# to create step_ prefixed keys\n",
    "history_per_output_layer.update(\n",
    "    **pre_suf_fix_dict(history_per_output_layer, fix_val='step', prefix=True),\n",
    ")\n",
    "# removing y_pred, y_true and keeping step_y_pred and step_y_true\n",
    "history_per_output_layer.pop('y_pred')\n",
    "history_per_output_layer.pop('y_true')\n",
    "\n",
    "# fix_val == suffix without \"_\"; to provide flexibility in using fix_val as prefix as well (if we wanted to!)\n",
    "self.out_layers_fix_val = 'output_layer' \n",
    "\n",
    "# pre_suf_fix_dict function is used to create unique key names to merge the `num_output_layers` histories into one history dictionary\n",
    "history_ol_lists = [pre_suf_fix_dict(history_per_output_layer, \n",
    "                                    fix_val=f'{self.out_layers_fix_val}_{i}', \n",
    "                                    prefix=False) \n",
    "                    for i in range(self.num_output_layers)] # ol == output layers\n",
    "\n",
    "# 'loss' here is for the final loss calculated by the loss aggregator function which uses all `num_output_layers` losses\n",
    "self.history = {'loss':[], 'val_loss':[], 'test_loss':[]}\n",
    "self.history.update(**pre_suf_fix_dict(self.history, 'step', prefix=True))\n",
    "# merging the `num_output_layers` histories into one history dictionary\n",
    "for hist_dict in history_ol_lists:\n",
    "    self.history.update(**hist_dict)\n",
    "\n",
    "self.hist_keys = list(self.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed manual logging\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class HierarchalModelPL(pl.LightningModule):\n",
    "   def __init__(self, hierarchical_model, num_classes_per_layer:list, loss_weights=None, dummy_input_size=(32, 3, 306, 306), the_device='cpu'):\n",
    "      super(HierarchalModelPL, self).__init__()\n",
    "      self.hierarchical_model = hierarchical_model # .to(the_device)\n",
    "      self.num_output_layers = len(num_classes_per_layer)\n",
    "      self.loss_weights = [1]*self.num_output_layers if loss_weights is None else loss_weights\n",
    "      self.num_output_layers = self.num_output_layers\n",
    "      \n",
    "      # useful for writing computational graph in tensorboard, summary(), etc\n",
    "      self.example_input_array = torch.randn(*dummy_input_size) # , device=the_device\n",
    "\n",
    "      self.the_device = the_device if 'cpu' in the_device else 'cuda'\n",
    "      self.metrics = ['acc', 'f1_score']\n",
    "      def _create_metric_func(metric_type:str, num_classes):\n",
    "         if 'acc' in metric_type.lower():\n",
    "               return torchmetrics.Accuracy(task='multiclass', \n",
    "                                          num_classes=num_classes, \n",
    "                                          average=\"micro\").to(the_device)\n",
    "         elif 'f1' in metric_type.lower():\n",
    "               return torchmetrics.F1Score(task=\"multiclass\", \n",
    "                                          num_classes=num_classes, \n",
    "                                          average=None).to(the_device)\n",
    "      \n",
    "      #                                         step/epoch vvv\n",
    "      # max indexing possible: self.metric_funcs['f1_score'][1][num_output_layers-1][2]\n",
    "      #                                       softmax output layers ^^^            ^^^  train/val/test \n",
    "      # output example of metric_funcs['acc']\n",
    "      # (note: MCA == MulticlassAccuracy()):\n",
    "      # {'acc': [ \n",
    "      #   [ #step\n",
    "      #    [MCA, MCA, MCA], # softmax_output_layer_0 ; train/val/test\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA], # ...\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA] # # softmax_output_layer_5 ; train/val/test\n",
    "      #   ],\n",
    "      #   [ #epoch\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA]\n",
    "      #   ]\n",
    "      # ]\n",
    "      \n",
    "      self.metric_funcs = {}\n",
    "      for metric in self.metrics:\n",
    "         # i --> step/epoch, j --> num_output_layers-1, k --> train/val/test\n",
    "         self.metric_funcs[metric] = [[[_create_metric_func(metric, num_classes_per_layer[j]) for k in range(3)] \n",
    "                                       for j in range(self.num_output_layers)] \n",
    "                                       for i in range(2)]\n",
    "      \n",
    "      # fix_val == suffix without \"_\"; to provide flexibility in using fix_val as prefix as well (if we wanted to!)\n",
    "      self.history_layers_fix_val = 'output_layer' \n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "      # len(hier_y_probs) == num_hierarchy_output_layers, \n",
    "      # while each element is 2D tensor of shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "      hier_y_pred = []\n",
    "      for tensor_output in self.hierarchical_model(x):\n",
    "         hier_y_pred.append(F.softmax(tensor_output, dim=1))\n",
    "      hier_y_pred\n",
    "      return hier_y_pred\n",
    "   \n",
    "   def training_step(self, batch, batch_idx):\n",
    "      ds_prefix = ''\n",
    "      metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "      # add other key:value pairs here or pass the entire metrics_dict if you want. just make sure 'loss' key is present\n",
    "      return metrics_dict\n",
    "    \n",
    "   def training_epoch_end(self, outputs) -> None:\n",
    "      # 'outputs' argument here contains values from what was returned from training_step()\n",
    "      ds_prefix = ''\n",
    "      self._epoch_end_logic(outputs, ds_prefix)\n",
    "\n",
    "   def validation_step(self, batch, batch_idx):\n",
    "      ds_prefix = 'val_'\n",
    "      metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "      return metrics_dict\n",
    "   \n",
    "   def validation_epoch_end(self, outputs) -> None:\n",
    "      ds_prefix = 'val_'\n",
    "      self._epoch_end_logic(outputs, ds_prefix)\n",
    "   \n",
    "   def test_step(self, batch, batch_idx):\n",
    "      ds_prefix = 'test_'\n",
    "      metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "      return metrics_dict\n",
    "   \n",
    "   def test_epoch_end(self, outputs) -> None:\n",
    "      ds_prefix = 'test_'\n",
    "      self._epoch_end_logic(outputs, ds_prefix=ds_prefix)\n",
    "   \n",
    "\n",
    "   def _store_metric(self, metric_name, metric_val):\n",
    "      self.logger.log(metric_name, metric_val)\n",
    "      # try:\n",
    "      #    self.log(metric_name, metric_val)\n",
    "      # except Exception:\n",
    "      #    # then metric_val is not scalar, then this means:\n",
    "      #    # metric_name is an f1 score metric \n",
    "      #    # so, we just take the mean value\n",
    "      #    self.log(metric_name, metric_val.mean())\n",
    "      # # putting this here ensures we don't include the mean of f1 score, \n",
    "      # # but rather the tensor of shape (1, num_classes)\n",
    "\n",
    "   def _step_logic(self, batch, ds_prefix=''):\n",
    "      # ds_prefix == dataset_prefix\n",
    "      if 'val' in ds_prefix:\n",
    "         ds_type_idx = 1\n",
    "      elif 'test' in ds_prefix:\n",
    "         ds_type_idx = 2\n",
    "      else:\n",
    "         ds_type_idx = 0\n",
    "      ds_prefix = 'step_' + ds_prefix\n",
    "\n",
    "      x, y = batch\n",
    "      hier_y_pred = self(x)\n",
    "\n",
    "      # y now has shape (batch_size, len(hier_y_pred))\n",
    "      # in other words, each row now consists of `len(hier_y_pred)` output layers' labels of 1 sample\n",
    "      # side note: len(hier_y_pred) == self.num_output_layers\n",
    "      y = torch.tensor([labelToHierarchy[int(y[i])] for i in range(len(y))], dtype=int, device=self.the_device) # alternatively, range(y.size()[0])\n",
    "      # transpose y to take each row (batch of labels) with its corresponding hier_y_pred row (batch of predicted labels)\n",
    "      # in other words, each row now consists of `batch_size` labels of the i_th output layer\n",
    "      y = y.T\n",
    "      # softmax_output_layer elements each has shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "      # side note: the wording of \"i_th output layers\" refers to the strings mentioned in \n",
    "      # output_order argument of HierarchalModel() used in create_hnn_model_arch()\n",
    "\n",
    "      losses_dict = {}\n",
    "      other_metrics_dict = {}\n",
    "      for out_layer_idx, softmax_output_layer in enumerate(hier_y_pred): \n",
    "         # 'cur' refers to current output layer\n",
    "         y_cur = y[out_layer_idx]\n",
    "         y_pred_cur = softmax_output_layer.to(self.the_device)\n",
    "         ol_suffix = f'_{self.history_layers_fix_val}_{out_layer_idx}'\n",
    "\n",
    "         # log step metrics\n",
    "\n",
    "         loss = F.cross_entropy(y_pred_cur, y_cur)\n",
    "         loss_full_metric_name = f'{ds_prefix}loss{ol_suffix}'\n",
    "         self._store_metric(loss_full_metric_name, loss)\n",
    "         losses_dict[loss_full_metric_name] = loss\n",
    "\n",
    "         for metric_name in self.metrics:\n",
    "            # recall the indexing of self.metric_funcs:\n",
    "            # metric_name, step/epoch, num_output_layers-1, train/val/test\n",
    "            metric_val = self.metric_funcs[metric_name][0][out_layer_idx][ds_type_idx](y_pred_cur, y_cur)\n",
    "            full_metric_name = f'{ds_prefix}{metric_name}{ol_suffix}'\n",
    "            self._store_metric(full_metric_name, metric_val)\n",
    "            self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].update(y_pred_cur, y_cur)\n",
    "            other_metrics_dict[full_metric_name] = metric_val\n",
    "      \n",
    "         f1_score = self.metric_funcs[metric_name][0][out_layer_idx][ds_type_idx](y_pred_cur, y_cur)\n",
    "         self._store_metric(f'{ds_prefix}{metric_name}{ol_suffix}', f1_score)\n",
    "         self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].update(y_pred_cur, y_cur)\n",
    "\n",
    "         # storing y_pred/y_true\n",
    "         # self.history[f'{ds_prefix}y_pred{ol_suffix}'].extend(y_pred_cur)\n",
    "         # self.history[f'{ds_prefix}y_true{ol_suffix}'].extend(y_cur)\n",
    "\n",
    "      final_loss = self.metric_reduce_fx(losses_dict.values(), 'weighted_sum')\n",
    "      self._store_metric(f'{ds_prefix}loss', final_loss)\n",
    "\n",
    "      # Important note: 'loss' key must be present, or else you'll get this error:\n",
    "      # MisconfigurationException: In automatic_optimization, \n",
    "      # when `training_step` returns a dict, the 'loss' key needs to be present\n",
    "      # side note: add `.update(losses_dict)` and `.update(other_metrics_dict)` \n",
    "      # if you want to directly use other metrics in \"..._epoch_end()\" methods\n",
    "      return {f'loss' : final_loss} \n",
    "\n",
    "   def _epoch_end_logic(self, outputs, ds_prefix=''):\n",
    "      if 'val' in ds_prefix:\n",
    "         ds_type_idx = 1\n",
    "      elif 'test' in ds_prefix:\n",
    "         ds_type_idx = 2\n",
    "      else:\n",
    "         ds_type_idx = 0\n",
    "\n",
    "      # log epoch metrics\n",
    "      final_loss_epoch = torch.tensor([x[f'loss'] for x in outputs], dtype=float).mean()\n",
    "      self._store_metric(f'{ds_prefix}loss', final_loss_epoch)\n",
    "\n",
    "      for out_layer_idx in range(self.num_output_layers):\n",
    "         ol_suffix = f'_{self.history_layers_fix_val}_{out_layer_idx}'\n",
    "\n",
    "         for metric_name in self.metrics:\n",
    "            # recall: '1' for accessing epoch func (not step func)\n",
    "            metric_val_epoch = self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].compute()\n",
    "            self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].reset()\n",
    "            full_metric_name = f'{ds_prefix}{metric_name}{ol_suffix}'\n",
    "            self._store_metric(full_metric_name, metric_val_epoch)\n",
    "            if out_layer_idx == 0:\n",
    "               print(metric_name)\n",
    "               print(metric_val_epoch)\n",
    "               print()\n",
    "   \n",
    "   def metric_reduce_fx(self, metric_list, agg_type='weighted_sum'):\n",
    "      '''\n",
    "      aggregates metric values from all `num_output_layers` into a single value\n",
    "      side note: called \"reduce_fx\" as a reference to PyTorch Lightning's reduce_fx parameter found in self.log()\n",
    "      '''\n",
    "      if 'weighted' in agg_type.lower() and 'sum' in agg_type.lower():\n",
    "         weighted_sum_val = 0\n",
    "         for i, layer_metric_val in enumerate(metric_list):\n",
    "            weighted_sum_val += self.loss_weights[i] * layer_metric_val\n",
    "         final_val = weighted_sum_val\n",
    "\n",
    "      return final_val\n",
    "\n",
    "   def configure_optimizers(self):\n",
    "      # make it self.hierarchical_model.parameters() if you defined other parameters in __init()__ which you don't want to optimize\n",
    "      # side note: under the hood, pl automatically gets gradients \n",
    "      # from final_loss returned from training_step() and adjusts the models' branches accordingly\n",
    "      # source: https://github.com/Lightning-AI/lightning/issues/2645#issuecomment-660681760\n",
    "      return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft function: useless, as \n",
    "def _get_num_classes(key, hierarchy_version):\n",
    "    '''\n",
    "    get the number of classes for the output layer which its order is specified in ol_num parameter,\n",
    "    and based on level of hierarchy chosen\n",
    "    Example: if hierarchy_version is 1, then a key with \"0\" suffix (as in, ..._output_layer_0 for example),\n",
    "    will be considered an output layer with 2 classes: high/low color diversity, and if \"1\", then one of the 9 flat classes.\n",
    "    However, if hierarchy_version is set to 2, then \"0\" will have same meaning, but \"1\" will mean `Category` classes (which were 4),\n",
    "    and \"2\" will mean the 9 flat classes\n",
    "\n",
    "    To summarize: you'll most likely change this function's logic based on the possible hierarchies that you want to expirement with\n",
    "    '''\n",
    "    if hierarchy_version == '1':\n",
    "        if '0' in key:\n",
    "            return ['00. selfies', ..., '82. academicDigital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useless draft code for log_metrics() function in MyLogger class:\n",
    "# ... code above\n",
    "for metric_name, metric_val in metrics.items():\n",
    "\n",
    "    if isinstance(metric_val, torch.Tensor):\n",
    "                    # I've added .clone() as I don't know if calling detach() on losses are advisable or not\n",
    "                    # so using clone() and detach() to avoid detaching original losses from computation graph\n",
    "                    # source for using clone():\n",
    "                    # https://www.educba.com/pytorch-detach/#:~:text=If%20we%20need%20to%20copy%20constructs%20from%20the%20tensor%2C%20we%20can%20use%20sourceTensor.clone().detach()\n",
    "                    metric_val = np.array(metric_val.clone().detach().cpu())\n",
    "                else:\n",
    "                    metric_val = np.array([np.array(tensor.clone().detach().cpu()) for tensor in metric_val])\n",
    "\n",
    "    # converting tensors to list\n",
    "    metric_val = metric_val.tolist() # tolist() will not however convert a single float value to a list (but will leave it as float instead)\n",
    "    # ... code below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debugging why stratification of imbalanced dataset doesn't really work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrrr = trainer_debug.train_dataloader.sampler.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "tensor(5)\n",
      "tensor(6)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(7)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "while ii < 10:\n",
    "    idx = next(itrrr)\n",
    "    print(trainer_debug.train_dataloader.sampler.labels_in_csv[idx])\n",
    "    ii += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(705,\n",
       " [117,\n",
       "  139,\n",
       "  145,\n",
       "  252,\n",
       "  290,\n",
       "  360,\n",
       "  388,\n",
       "  396,\n",
       "  573,\n",
       "  618,\n",
       "  639,\n",
       "  681,\n",
       "  688,\n",
       "  716,\n",
       "  738,\n",
       "  977,\n",
       "  1111,\n",
       "  1159,\n",
       "  1191,\n",
       "  1195,\n",
       "  1293,\n",
       "  1436,\n",
       "  1578,\n",
       "  1626,\n",
       "  1648,\n",
       "  1683,\n",
       "  1782,\n",
       "  1783,\n",
       "  1905,\n",
       "  2113,\n",
       "  2122,\n",
       "  2167,\n",
       "  2169,\n",
       "  2181,\n",
       "  2193,\n",
       "  2246,\n",
       "  2249,\n",
       "  2260,\n",
       "  2354,\n",
       "  2388,\n",
       "  2417,\n",
       "  2449,\n",
       "  2493,\n",
       "  2525,\n",
       "  2561,\n",
       "  2598,\n",
       "  2648,\n",
       "  2675,\n",
       "  2691,\n",
       "  2692,\n",
       "  2715,\n",
       "  2962,\n",
       "  3007,\n",
       "  3049,\n",
       "  3119,\n",
       "  3121,\n",
       "  3127,\n",
       "  3135,\n",
       "  3242,\n",
       "  3437,\n",
       "  3444,\n",
       "  3464,\n",
       "  3467,\n",
       "  3491,\n",
       "  3507,\n",
       "  3511,\n",
       "  3535,\n",
       "  3569,\n",
       "  3578,\n",
       "  3668,\n",
       "  3967,\n",
       "  4144,\n",
       "  4194,\n",
       "  4222,\n",
       "  4254,\n",
       "  4620,\n",
       "  4670,\n",
       "  4693,\n",
       "  4768,\n",
       "  4780,\n",
       "  4817,\n",
       "  4820,\n",
       "  4881,\n",
       "  4941,\n",
       "  5068,\n",
       "  5104,\n",
       "  5157,\n",
       "  5259,\n",
       "  5301,\n",
       "  5304,\n",
       "  5354,\n",
       "  5468,\n",
       "  5513,\n",
       "  5721,\n",
       "  5848,\n",
       "  5930,\n",
       "  5989,\n",
       "  6058,\n",
       "  6206,\n",
       "  6209,\n",
       "  6363,\n",
       "  6378,\n",
       "  6748,\n",
       "  6802,\n",
       "  6824,\n",
       "  7152,\n",
       "  7277,\n",
       "  7305,\n",
       "  7318,\n",
       "  7401,\n",
       "  7430,\n",
       "  7561,\n",
       "  7677,\n",
       "  7743,\n",
       "  7822,\n",
       "  7852,\n",
       "  7919,\n",
       "  7929,\n",
       "  8004,\n",
       "  8009,\n",
       "  8016,\n",
       "  8135,\n",
       "  8218,\n",
       "  8297,\n",
       "  8436,\n",
       "  8480,\n",
       "  8536,\n",
       "  8593,\n",
       "  8693,\n",
       "  8710,\n",
       "  8744,\n",
       "  8756,\n",
       "  8836,\n",
       "  9011,\n",
       "  9017,\n",
       "  9023,\n",
       "  9044,\n",
       "  9062,\n",
       "  9146,\n",
       "  9314,\n",
       "  9343,\n",
       "  9552,\n",
       "  9630,\n",
       "  9682,\n",
       "  9782,\n",
       "  9791,\n",
       "  9825,\n",
       "  9878,\n",
       "  9915,\n",
       "  9952,\n",
       "  10005,\n",
       "  10008,\n",
       "  10117,\n",
       "  10123,\n",
       "  10131,\n",
       "  10162,\n",
       "  10231,\n",
       "  10240,\n",
       "  10417,\n",
       "  10469,\n",
       "  10495,\n",
       "  10635,\n",
       "  10670,\n",
       "  10686,\n",
       "  10708,\n",
       "  10797,\n",
       "  10940,\n",
       "  10945,\n",
       "  11133,\n",
       "  11153,\n",
       "  11198,\n",
       "  11321,\n",
       "  11487,\n",
       "  11507,\n",
       "  11673,\n",
       "  11734,\n",
       "  11802,\n",
       "  11870,\n",
       "  11912,\n",
       "  11983,\n",
       "  11994,\n",
       "  12049,\n",
       "  12130,\n",
       "  12147,\n",
       "  12181,\n",
       "  12317,\n",
       "  12385,\n",
       "  12405,\n",
       "  12422,\n",
       "  12503,\n",
       "  12536,\n",
       "  12563,\n",
       "  12587,\n",
       "  12592,\n",
       "  12653,\n",
       "  12786,\n",
       "  12828,\n",
       "  12842,\n",
       "  13012,\n",
       "  13089,\n",
       "  13099,\n",
       "  13166,\n",
       "  13213,\n",
       "  13428,\n",
       "  13473,\n",
       "  13474,\n",
       "  13582,\n",
       "  13589,\n",
       "  13766,\n",
       "  13920,\n",
       "  14023,\n",
       "  14039,\n",
       "  14065,\n",
       "  14130,\n",
       "  14165,\n",
       "  14310,\n",
       "  14336,\n",
       "  14339,\n",
       "  14441,\n",
       "  14643,\n",
       "  14651,\n",
       "  14652,\n",
       "  14678,\n",
       "  15078,\n",
       "  15092,\n",
       "  15350,\n",
       "  15372,\n",
       "  15517,\n",
       "  15592,\n",
       "  15694,\n",
       "  15792,\n",
       "  15953,\n",
       "  16094,\n",
       "  16108,\n",
       "  16413,\n",
       "  16566,\n",
       "  16597,\n",
       "  16723,\n",
       "  16770,\n",
       "  16781,\n",
       "  16853,\n",
       "  16854,\n",
       "  16931,\n",
       "  16946,\n",
       "  16954,\n",
       "  17000,\n",
       "  17148,\n",
       "  17151,\n",
       "  17204,\n",
       "  17252,\n",
       "  17298,\n",
       "  17326,\n",
       "  17373,\n",
       "  17596,\n",
       "  17635,\n",
       "  17682,\n",
       "  17724,\n",
       "  17788,\n",
       "  17802,\n",
       "  17850,\n",
       "  17853,\n",
       "  17906,\n",
       "  18080,\n",
       "  18310,\n",
       "  18363,\n",
       "  18373,\n",
       "  18478,\n",
       "  18608,\n",
       "  18689,\n",
       "  18692,\n",
       "  18949,\n",
       "  19055,\n",
       "  19071,\n",
       "  19126,\n",
       "  19142,\n",
       "  19229,\n",
       "  19273,\n",
       "  19307,\n",
       "  19329,\n",
       "  19405,\n",
       "  19408,\n",
       "  19500,\n",
       "  19509,\n",
       "  19536,\n",
       "  19540,\n",
       "  19544,\n",
       "  19599,\n",
       "  19922,\n",
       "  19928,\n",
       "  19933,\n",
       "  19954,\n",
       "  20035,\n",
       "  20065,\n",
       "  20218,\n",
       "  20222,\n",
       "  20280,\n",
       "  20284,\n",
       "  20343,\n",
       "  20366,\n",
       "  20623,\n",
       "  20657,\n",
       "  20692,\n",
       "  20702,\n",
       "  20758,\n",
       "  20769,\n",
       "  20857,\n",
       "  20876,\n",
       "  20881,\n",
       "  21011,\n",
       "  21023,\n",
       "  21024,\n",
       "  21250,\n",
       "  21271,\n",
       "  21377,\n",
       "  21453,\n",
       "  21482,\n",
       "  21746,\n",
       "  21840,\n",
       "  21920,\n",
       "  22067,\n",
       "  22076,\n",
       "  22169,\n",
       "  22260,\n",
       "  22307,\n",
       "  22418,\n",
       "  22441,\n",
       "  22451,\n",
       "  22569,\n",
       "  22601,\n",
       "  22603,\n",
       "  22634,\n",
       "  22672,\n",
       "  22703,\n",
       "  22819,\n",
       "  22822,\n",
       "  22887,\n",
       "  22905,\n",
       "  23020,\n",
       "  23227,\n",
       "  23270,\n",
       "  23403,\n",
       "  23555,\n",
       "  23639,\n",
       "  23682,\n",
       "  23887,\n",
       "  23896,\n",
       "  23974,\n",
       "  24142,\n",
       "  24205,\n",
       "  24225,\n",
       "  24375,\n",
       "  24420,\n",
       "  24590,\n",
       "  24868,\n",
       "  24882,\n",
       "  24898,\n",
       "  24912,\n",
       "  24930,\n",
       "  24960,\n",
       "  24972,\n",
       "  24976,\n",
       "  25019,\n",
       "  25103,\n",
       "  25148,\n",
       "  25162,\n",
       "  25321,\n",
       "  25334,\n",
       "  25712,\n",
       "  25750,\n",
       "  25829,\n",
       "  26066,\n",
       "  26082,\n",
       "  26093,\n",
       "  26203,\n",
       "  26396,\n",
       "  26487,\n",
       "  26564,\n",
       "  26575,\n",
       "  26650,\n",
       "  26673,\n",
       "  26751,\n",
       "  26910,\n",
       "  26945,\n",
       "  27114,\n",
       "  27160,\n",
       "  27242,\n",
       "  27270,\n",
       "  27399,\n",
       "  27471,\n",
       "  27673,\n",
       "  27690,\n",
       "  27700,\n",
       "  27707,\n",
       "  27799,\n",
       "  27912,\n",
       "  27938,\n",
       "  28085,\n",
       "  28110,\n",
       "  28186,\n",
       "  28256,\n",
       "  28290,\n",
       "  28457,\n",
       "  28562,\n",
       "  28596,\n",
       "  28650,\n",
       "  28667,\n",
       "  28749,\n",
       "  28816,\n",
       "  28822,\n",
       "  29044,\n",
       "  29071,\n",
       "  29236,\n",
       "  29246,\n",
       "  29279,\n",
       "  29369,\n",
       "  29468,\n",
       "  29524,\n",
       "  29547,\n",
       "  29617,\n",
       "  29660,\n",
       "  29735,\n",
       "  29752,\n",
       "  29765,\n",
       "  29772,\n",
       "  29830,\n",
       "  29851,\n",
       "  29970,\n",
       "  30132,\n",
       "  30156,\n",
       "  30195,\n",
       "  30266,\n",
       "  30268,\n",
       "  30329,\n",
       "  30507,\n",
       "  30770,\n",
       "  30780,\n",
       "  30809,\n",
       "  30817,\n",
       "  30838,\n",
       "  30871,\n",
       "  30948,\n",
       "  31231,\n",
       "  31317,\n",
       "  31372,\n",
       "  31426,\n",
       "  31443,\n",
       "  31498,\n",
       "  31527,\n",
       "  31657,\n",
       "  31717,\n",
       "  31758,\n",
       "  31785,\n",
       "  31793,\n",
       "  31860,\n",
       "  31943,\n",
       "  31955,\n",
       "  32131,\n",
       "  32178,\n",
       "  32187,\n",
       "  32218,\n",
       "  32297,\n",
       "  32364,\n",
       "  32450,\n",
       "  32459,\n",
       "  32577,\n",
       "  32586,\n",
       "  32610,\n",
       "  32616,\n",
       "  32696,\n",
       "  32787,\n",
       "  32824,\n",
       "  32883,\n",
       "  32900,\n",
       "  32988,\n",
       "  33172,\n",
       "  33197,\n",
       "  33388,\n",
       "  33389,\n",
       "  33407,\n",
       "  33458,\n",
       "  33503,\n",
       "  33675,\n",
       "  33706,\n",
       "  33780,\n",
       "  33795,\n",
       "  33804,\n",
       "  33853,\n",
       "  33856,\n",
       "  33980,\n",
       "  34027,\n",
       "  34197,\n",
       "  34220,\n",
       "  34359,\n",
       "  34404,\n",
       "  34475,\n",
       "  34526,\n",
       "  34581,\n",
       "  34870,\n",
       "  34936,\n",
       "  34961,\n",
       "  34972,\n",
       "  35074,\n",
       "  35133,\n",
       "  35147,\n",
       "  35361,\n",
       "  35369,\n",
       "  35430,\n",
       "  35612,\n",
       "  35791,\n",
       "  35928,\n",
       "  35944,\n",
       "  36088,\n",
       "  36097,\n",
       "  36210,\n",
       "  36429,\n",
       "  36500,\n",
       "  36634,\n",
       "  36642,\n",
       "  36666,\n",
       "  36668,\n",
       "  36696,\n",
       "  36774,\n",
       "  36812,\n",
       "  36925,\n",
       "  36958,\n",
       "  36970,\n",
       "  36972,\n",
       "  37045,\n",
       "  37120,\n",
       "  37124,\n",
       "  37126,\n",
       "  37128,\n",
       "  37165,\n",
       "  37269,\n",
       "  37307,\n",
       "  37374,\n",
       "  37545,\n",
       "  37548,\n",
       "  37573,\n",
       "  37693,\n",
       "  37728,\n",
       "  37800,\n",
       "  37853,\n",
       "  37857,\n",
       "  37974,\n",
       "  38010,\n",
       "  38024,\n",
       "  38116,\n",
       "  38176,\n",
       "  38214,\n",
       "  38235,\n",
       "  38278,\n",
       "  38366,\n",
       "  38394,\n",
       "  38406,\n",
       "  38412,\n",
       "  38671,\n",
       "  38682,\n",
       "  38727,\n",
       "  38780,\n",
       "  38851,\n",
       "  38862,\n",
       "  38880,\n",
       "  39018,\n",
       "  39046,\n",
       "  39122,\n",
       "  39124,\n",
       "  39288,\n",
       "  39327,\n",
       "  39332,\n",
       "  39473,\n",
       "  39524,\n",
       "  39566,\n",
       "  39616,\n",
       "  39712,\n",
       "  39724,\n",
       "  39726,\n",
       "  39728,\n",
       "  39743,\n",
       "  39836,\n",
       "  39949,\n",
       "  39995,\n",
       "  40269,\n",
       "  40306,\n",
       "  40315,\n",
       "  40325,\n",
       "  40388,\n",
       "  40488,\n",
       "  40536,\n",
       "  40550,\n",
       "  40576,\n",
       "  40636,\n",
       "  40684,\n",
       "  40707,\n",
       "  40717,\n",
       "  40832,\n",
       "  41014,\n",
       "  41049,\n",
       "  41529,\n",
       "  41600,\n",
       "  41692,\n",
       "  41902,\n",
       "  41926,\n",
       "  41990,\n",
       "  42053,\n",
       "  42117,\n",
       "  42159,\n",
       "  42170,\n",
       "  42418,\n",
       "  42610,\n",
       "  42757,\n",
       "  42764,\n",
       "  42774,\n",
       "  42788,\n",
       "  42825,\n",
       "  42846,\n",
       "  42863,\n",
       "  42914,\n",
       "  43074,\n",
       "  43143,\n",
       "  43324,\n",
       "  43453,\n",
       "  43516,\n",
       "  43519,\n",
       "  43561,\n",
       "  43584,\n",
       "  43600,\n",
       "  43608,\n",
       "  43635,\n",
       "  43660,\n",
       "  43851,\n",
       "  43866,\n",
       "  44051,\n",
       "  44101,\n",
       "  44132,\n",
       "  44134,\n",
       "  44297,\n",
       "  44330,\n",
       "  44384,\n",
       "  44487,\n",
       "  44520,\n",
       "  44521,\n",
       "  44583,\n",
       "  44605,\n",
       "  44654,\n",
       "  44749,\n",
       "  44811,\n",
       "  44819,\n",
       "  44872,\n",
       "  45178,\n",
       "  45348,\n",
       "  45505,\n",
       "  45595,\n",
       "  45652,\n",
       "  45745,\n",
       "  45830,\n",
       "  45837,\n",
       "  45886,\n",
       "  45973,\n",
       "  45984,\n",
       "  46053,\n",
       "  46082,\n",
       "  46309,\n",
       "  46338,\n",
       "  46399,\n",
       "  46477,\n",
       "  46578,\n",
       "  46683,\n",
       "  46693,\n",
       "  46821,\n",
       "  46834,\n",
       "  46841,\n",
       "  46887,\n",
       "  46907,\n",
       "  46982,\n",
       "  46984,\n",
       "  47092,\n",
       "  47097,\n",
       "  47165,\n",
       "  47278,\n",
       "  47286,\n",
       "  47622,\n",
       "  47628,\n",
       "  47663,\n",
       "  47721,\n",
       "  47733,\n",
       "  47786,\n",
       "  47917,\n",
       "  47965,\n",
       "  48051,\n",
       "  48086,\n",
       "  48477,\n",
       "  48522,\n",
       "  48663,\n",
       "  48833,\n",
       "  48903,\n",
       "  49020,\n",
       "  49051,\n",
       "  49102,\n",
       "  49103,\n",
       "  49140,\n",
       "  49244,\n",
       "  49443,\n",
       "  49497,\n",
       "  49518,\n",
       "  49542,\n",
       "  49645,\n",
       "  49873,\n",
       "  49904,\n",
       "  50016,\n",
       "  50214,\n",
       "  50290,\n",
       "  50561,\n",
       "  50657,\n",
       "  50662,\n",
       "  50736,\n",
       "  50809,\n",
       "  50938,\n",
       "  50998,\n",
       "  51102,\n",
       "  51145,\n",
       "  51207,\n",
       "  51234,\n",
       "  51243,\n",
       "  51334,\n",
       "  51363,\n",
       "  51458,\n",
       "  51480,\n",
       "  51600,\n",
       "  52180,\n",
       "  52267,\n",
       "  52345,\n",
       "  52361,\n",
       "  52432,\n",
       "  52461,\n",
       "  52485,\n",
       "  52541,\n",
       "  52569,\n",
       "  52688,\n",
       "  52705,\n",
       "  52819,\n",
       "  52936,\n",
       "  53043,\n",
       "  53097,\n",
       "  53099,\n",
       "  53266,\n",
       "  53370,\n",
       "  53559,\n",
       "  53711,\n",
       "  54041,\n",
       "  54089,\n",
       "  54103,\n",
       "  54159,\n",
       "  54234,\n",
       "  54283,\n",
       "  54515,\n",
       "  54521,\n",
       "  54538,\n",
       "  54556,\n",
       "  54578,\n",
       "  54682,\n",
       "  54859,\n",
       "  54977,\n",
       "  55010,\n",
       "  55065,\n",
       "  55128,\n",
       "  55162,\n",
       "  55244,\n",
       "  55272,\n",
       "  55457,\n",
       "  55635,\n",
       "  55648,\n",
       "  55671,\n",
       "  55684,\n",
       "  55686,\n",
       "  55835,\n",
       "  55909,\n",
       "  55970,\n",
       "  56023,\n",
       "  56049,\n",
       "  56064,\n",
       "  56097,\n",
       "  56138,\n",
       "  56185,\n",
       "  56214,\n",
       "  56230,\n",
       "  56408,\n",
       "  56455,\n",
       "  56571,\n",
       "  56591,\n",
       "  56664,\n",
       "  56675,\n",
       "  56714,\n",
       "  56769,\n",
       "  56770,\n",
       "  56845,\n",
       "  56912,\n",
       "  56952,\n",
       "  57089,\n",
       "  57166,\n",
       "  57203,\n",
       "  57485,\n",
       "  57545,\n",
       "  57612,\n",
       "  57661,\n",
       "  57672,\n",
       "  57779,\n",
       "  57787,\n",
       "  57992,\n",
       "  58009,\n",
       "  58010,\n",
       "  58072,\n",
       "  58076,\n",
       "  58268,\n",
       "  58310,\n",
       "  58375,\n",
       "  58378,\n",
       "  58414,\n",
       "  58439,\n",
       "  58543,\n",
       "  58596,\n",
       "  58616,\n",
       "  58682,\n",
       "  58717,\n",
       "  58887,\n",
       "  58901,\n",
       "  59191,\n",
       "  59195,\n",
       "  59467,\n",
       "  59517,\n",
       "  59536,\n",
       "  59607,\n",
       "  59619,\n",
       "  59628,\n",
       "  59636,\n",
       "  59694,\n",
       "  59747,\n",
       "  59794,\n",
       "  59818,\n",
       "  60040,\n",
       "  60178,\n",
       "  60306,\n",
       "  60343,\n",
       "  60360,\n",
       "  60441,\n",
       "  60538,\n",
       "  60681,\n",
       "  60756,\n",
       "  60802,\n",
       "  60818,\n",
       "  60857,\n",
       "  60911,\n",
       "  60997,\n",
       "  61045,\n",
       "  61057,\n",
       "  61144,\n",
       "  61188,\n",
       "  61192,\n",
       "  61260,\n",
       "  61271,\n",
       "  61279,\n",
       "  61316,\n",
       "  61330,\n",
       "  61486,\n",
       "  61543,\n",
       "  61575,\n",
       "  61629,\n",
       "  61770,\n",
       "  61809,\n",
       "  61854,\n",
       "  61941,\n",
       "  62033,\n",
       "  62127,\n",
       "  62181,\n",
       "  62254,\n",
       "  62407,\n",
       "  62594,\n",
       "  62599,\n",
       "  62617,\n",
       "  62619,\n",
       "  62863,\n",
       "  62873,\n",
       "  62926,\n",
       "  62940,\n",
       "  62970,\n",
       "  63010,\n",
       "  63067,\n",
       "  63078,\n",
       "  63092,\n",
       "  63103,\n",
       "  63148,\n",
       "  63185,\n",
       "  63199,\n",
       "  63240,\n",
       "  63284,\n",
       "  63293,\n",
       "  63299,\n",
       "  63565,\n",
       "  63662,\n",
       "  63719,\n",
       "  63847,\n",
       "  63872,\n",
       "  63873,\n",
       "  64018,\n",
       "  64093,\n",
       "  64097,\n",
       "  64109,\n",
       "  64126,\n",
       "  64166,\n",
       "  64176,\n",
       "  64201,\n",
       "  64252,\n",
       "  64375,\n",
       "  64381,\n",
       "  64447,\n",
       "  64462,\n",
       "  64634,\n",
       "  64642,\n",
       "  64857,\n",
       "  65063,\n",
       "  65096,\n",
       "  65162,\n",
       "  65277,\n",
       "  65296,\n",
       "  65370,\n",
       "  65386,\n",
       "  65620,\n",
       "  65663,\n",
       "  65788,\n",
       "  65876,\n",
       "  65919,\n",
       "  65925,\n",
       "  65948,\n",
       "  65974,\n",
       "  66039,\n",
       "  66118,\n",
       "  66120,\n",
       "  66258,\n",
       "  66282,\n",
       "  66327,\n",
       "  66422,\n",
       "  66469,\n",
       "  66555,\n",
       "  66598,\n",
       "  66705,\n",
       "  66738,\n",
       "  66786,\n",
       "  66811,\n",
       "  66824,\n",
       "  66934,\n",
       "  66956,\n",
       "  66990,\n",
       "  67049,\n",
       "  67126,\n",
       "  67196,\n",
       "  67211,\n",
       "  67245,\n",
       "  67255,\n",
       "  67331,\n",
       "  67433,\n",
       "  67447,\n",
       "  67516,\n",
       "  67555,\n",
       "  67620,\n",
       "  67625,\n",
       "  67803,\n",
       "  67811,\n",
       "  68272,\n",
       "  68275,\n",
       "  68447,\n",
       "  68493,\n",
       "  68622,\n",
       "  68640,\n",
       "  68772,\n",
       "  68794,\n",
       "  68849,\n",
       "  68867,\n",
       "  69035,\n",
       "  69085,\n",
       "  69100,\n",
       "  69147,\n",
       "  69162,\n",
       "  69202,\n",
       "  69306,\n",
       "  69386,\n",
       "  69390,\n",
       "  69555,\n",
       "  69569,\n",
       "  69575,\n",
       "  69641,\n",
       "  69659,\n",
       "  69747,\n",
       "  69785,\n",
       "  69919,\n",
       "  69977,\n",
       "  69986,\n",
       "  70053,\n",
       "  70134,\n",
       "  70274,\n",
       "  70300,\n",
       "  ...])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# when stratifying, the academicDigital class (most minor) appears at the order printed below (difference between orders)\n",
    "# as you can see, there are steps (i.e., batches of 32) in which that class doesn't appear at all\n",
    "lbls_loc = []\n",
    "for i, a in enumerate(trainer_debug.train_dataloader.sampler.labels_after_strat):\n",
    "    if a == 8:\n",
    "        lbls_loc.append(i)\n",
    "len(lbls_loc), lbls_loc\n",
    "diff_between_locs = []\n",
    "prev_loc = 0\n",
    "for loc in lbls_loc:\n",
    "    if loc-prev_loc < 64:\n",
    "        continue\n",
    "    diff_between_locs.append(loc-prev_loc)\n",
    "    prev_loc = loc\n",
    "len(diff_between_locs), lbls_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(736,\n",
       " [23,\n",
       "  66,\n",
       "  181,\n",
       "  281,\n",
       "  326,\n",
       "  388,\n",
       "  396,\n",
       "  510,\n",
       "  564,\n",
       "  606,\n",
       "  624,\n",
       "  645,\n",
       "  695,\n",
       "  847,\n",
       "  1020,\n",
       "  1091,\n",
       "  1183,\n",
       "  1357,\n",
       "  1417,\n",
       "  1451,\n",
       "  1666,\n",
       "  1698,\n",
       "  1744,\n",
       "  1788,\n",
       "  1851,\n",
       "  1858,\n",
       "  1860,\n",
       "  1931,\n",
       "  1968,\n",
       "  2044,\n",
       "  2056,\n",
       "  2094,\n",
       "  2160,\n",
       "  2177,\n",
       "  2239,\n",
       "  2264,\n",
       "  2273,\n",
       "  2372,\n",
       "  2407,\n",
       "  2414,\n",
       "  2432,\n",
       "  2445,\n",
       "  2705,\n",
       "  2861,\n",
       "  2902,\n",
       "  2996,\n",
       "  3050,\n",
       "  3086,\n",
       "  3269,\n",
       "  3289,\n",
       "  3346,\n",
       "  3598,\n",
       "  3627,\n",
       "  3693,\n",
       "  3718,\n",
       "  3801,\n",
       "  3951,\n",
       "  4095,\n",
       "  4120,\n",
       "  4171,\n",
       "  4228,\n",
       "  4296,\n",
       "  4297,\n",
       "  4338,\n",
       "  4450,\n",
       "  4495,\n",
       "  4531,\n",
       "  4599,\n",
       "  4726,\n",
       "  4797,\n",
       "  4853,\n",
       "  4877,\n",
       "  4882,\n",
       "  4886,\n",
       "  4919,\n",
       "  4935,\n",
       "  4971,\n",
       "  5039,\n",
       "  5096,\n",
       "  5140,\n",
       "  5246,\n",
       "  5393,\n",
       "  5408,\n",
       "  5444,\n",
       "  5451,\n",
       "  5459,\n",
       "  5660,\n",
       "  5725,\n",
       "  5745,\n",
       "  5757,\n",
       "  5943,\n",
       "  5973,\n",
       "  6132,\n",
       "  6177,\n",
       "  6241,\n",
       "  6300,\n",
       "  6309,\n",
       "  6336,\n",
       "  6390,\n",
       "  6644,\n",
       "  6660,\n",
       "  6669,\n",
       "  6752,\n",
       "  6909,\n",
       "  6964,\n",
       "  6965,\n",
       "  7076,\n",
       "  7217,\n",
       "  7258,\n",
       "  7297,\n",
       "  7338,\n",
       "  7365,\n",
       "  7414,\n",
       "  7435,\n",
       "  7496,\n",
       "  7504,\n",
       "  7508,\n",
       "  7517,\n",
       "  7546,\n",
       "  7578,\n",
       "  7613,\n",
       "  7617,\n",
       "  7629,\n",
       "  7710,\n",
       "  7862,\n",
       "  7904,\n",
       "  8028,\n",
       "  8047,\n",
       "  8106,\n",
       "  8110,\n",
       "  8234,\n",
       "  8260,\n",
       "  8397,\n",
       "  8406,\n",
       "  8449,\n",
       "  8466,\n",
       "  8538,\n",
       "  8773,\n",
       "  8963,\n",
       "  9128,\n",
       "  9261,\n",
       "  9321,\n",
       "  9520,\n",
       "  9564,\n",
       "  9565,\n",
       "  9689,\n",
       "  9801,\n",
       "  9805,\n",
       "  9887,\n",
       "  9916,\n",
       "  9925,\n",
       "  9955,\n",
       "  10018,\n",
       "  10051,\n",
       "  10463,\n",
       "  10505,\n",
       "  10534,\n",
       "  10561,\n",
       "  10746,\n",
       "  10860,\n",
       "  10991,\n",
       "  11040,\n",
       "  11210,\n",
       "  11436,\n",
       "  11505,\n",
       "  11528,\n",
       "  11569,\n",
       "  11624,\n",
       "  11632,\n",
       "  11751,\n",
       "  11785,\n",
       "  11792,\n",
       "  11820,\n",
       "  11865,\n",
       "  11905,\n",
       "  11947,\n",
       "  11959,\n",
       "  11976,\n",
       "  12175,\n",
       "  12281,\n",
       "  12383,\n",
       "  12415,\n",
       "  12549,\n",
       "  12692,\n",
       "  12736,\n",
       "  12742,\n",
       "  12842,\n",
       "  12928,\n",
       "  12941,\n",
       "  13052,\n",
       "  13058,\n",
       "  13073,\n",
       "  13127,\n",
       "  13181,\n",
       "  13248,\n",
       "  13351,\n",
       "  13394,\n",
       "  13491,\n",
       "  13532,\n",
       "  13547,\n",
       "  13714,\n",
       "  13867,\n",
       "  13878,\n",
       "  14035,\n",
       "  14051,\n",
       "  14102,\n",
       "  14255,\n",
       "  14256,\n",
       "  14260,\n",
       "  14320,\n",
       "  14352,\n",
       "  14439,\n",
       "  14451,\n",
       "  14559,\n",
       "  14575,\n",
       "  14601,\n",
       "  14661,\n",
       "  14770,\n",
       "  14818,\n",
       "  14915,\n",
       "  14935,\n",
       "  14996,\n",
       "  15028,\n",
       "  15048,\n",
       "  15079,\n",
       "  15094,\n",
       "  15136,\n",
       "  15235,\n",
       "  15296,\n",
       "  15357,\n",
       "  15468,\n",
       "  15642,\n",
       "  15661,\n",
       "  15671,\n",
       "  15690,\n",
       "  15724,\n",
       "  15919,\n",
       "  15932,\n",
       "  15933,\n",
       "  16011,\n",
       "  16071,\n",
       "  16085,\n",
       "  16124,\n",
       "  16200,\n",
       "  16265,\n",
       "  16296,\n",
       "  16339,\n",
       "  16352,\n",
       "  16416,\n",
       "  16467,\n",
       "  16518,\n",
       "  16522,\n",
       "  16538,\n",
       "  16573,\n",
       "  16770,\n",
       "  16829,\n",
       "  16886,\n",
       "  16903,\n",
       "  16949,\n",
       "  16963,\n",
       "  17011,\n",
       "  17135,\n",
       "  17250,\n",
       "  17265,\n",
       "  17334,\n",
       "  17395,\n",
       "  17432,\n",
       "  17437,\n",
       "  17450,\n",
       "  17475,\n",
       "  17488,\n",
       "  17497,\n",
       "  17548,\n",
       "  17713,\n",
       "  17797,\n",
       "  17914,\n",
       "  18097,\n",
       "  18157,\n",
       "  18204,\n",
       "  18270,\n",
       "  18384,\n",
       "  18521,\n",
       "  18530,\n",
       "  18612,\n",
       "  18715,\n",
       "  18784,\n",
       "  18809,\n",
       "  18892,\n",
       "  18941,\n",
       "  18998,\n",
       "  19009,\n",
       "  19022,\n",
       "  19036,\n",
       "  19079,\n",
       "  19114,\n",
       "  19154,\n",
       "  19442,\n",
       "  19496,\n",
       "  19601,\n",
       "  19753,\n",
       "  19770,\n",
       "  19771,\n",
       "  19811,\n",
       "  19818,\n",
       "  19821,\n",
       "  19961,\n",
       "  20050,\n",
       "  20079,\n",
       "  20121,\n",
       "  20145,\n",
       "  20229,\n",
       "  20310,\n",
       "  20316,\n",
       "  20402,\n",
       "  20454,\n",
       "  20459,\n",
       "  20597,\n",
       "  20649,\n",
       "  20698,\n",
       "  20769,\n",
       "  20772,\n",
       "  20987,\n",
       "  21021,\n",
       "  21025,\n",
       "  21070,\n",
       "  21285,\n",
       "  21374,\n",
       "  21413,\n",
       "  21463,\n",
       "  21483,\n",
       "  21536,\n",
       "  21711,\n",
       "  21730,\n",
       "  21857,\n",
       "  22186,\n",
       "  22215,\n",
       "  22230,\n",
       "  22347,\n",
       "  22438,\n",
       "  22535,\n",
       "  22601,\n",
       "  22620,\n",
       "  22709,\n",
       "  22747,\n",
       "  22761,\n",
       "  22806,\n",
       "  22984,\n",
       "  23149,\n",
       "  23310,\n",
       "  23433,\n",
       "  23492,\n",
       "  23551,\n",
       "  23598,\n",
       "  23807,\n",
       "  23918,\n",
       "  23999,\n",
       "  24045,\n",
       "  24093,\n",
       "  24111,\n",
       "  24156,\n",
       "  24166,\n",
       "  24371,\n",
       "  24377,\n",
       "  24388,\n",
       "  24416,\n",
       "  24428,\n",
       "  24576,\n",
       "  24722,\n",
       "  24803,\n",
       "  24863,\n",
       "  24914,\n",
       "  25050,\n",
       "  25064,\n",
       "  25116,\n",
       "  25249,\n",
       "  25267,\n",
       "  25358,\n",
       "  25429,\n",
       "  25444,\n",
       "  25520,\n",
       "  25565,\n",
       "  25628,\n",
       "  25718,\n",
       "  25854,\n",
       "  25909,\n",
       "  26081,\n",
       "  26151,\n",
       "  26177,\n",
       "  26180,\n",
       "  26283,\n",
       "  26386,\n",
       "  26568,\n",
       "  26589,\n",
       "  26644,\n",
       "  26752,\n",
       "  26816,\n",
       "  26870,\n",
       "  26874,\n",
       "  26909,\n",
       "  27061,\n",
       "  27084,\n",
       "  27215,\n",
       "  27304,\n",
       "  27340,\n",
       "  27365,\n",
       "  27412,\n",
       "  27454,\n",
       "  27467,\n",
       "  27636,\n",
       "  27639,\n",
       "  27722,\n",
       "  27725,\n",
       "  27736,\n",
       "  27842,\n",
       "  27867,\n",
       "  27869,\n",
       "  27891,\n",
       "  27912,\n",
       "  28094,\n",
       "  28112,\n",
       "  28113,\n",
       "  28277,\n",
       "  28350,\n",
       "  28432,\n",
       "  28457,\n",
       "  28572,\n",
       "  28649,\n",
       "  28768,\n",
       "  28828,\n",
       "  28881,\n",
       "  28882,\n",
       "  28908,\n",
       "  29042,\n",
       "  29104,\n",
       "  29106,\n",
       "  29169,\n",
       "  29210,\n",
       "  29250,\n",
       "  29279,\n",
       "  29363,\n",
       "  29480,\n",
       "  29592,\n",
       "  29669,\n",
       "  29694,\n",
       "  29921,\n",
       "  29927,\n",
       "  30025,\n",
       "  30074,\n",
       "  30080,\n",
       "  30125,\n",
       "  30138,\n",
       "  30166,\n",
       "  30187,\n",
       "  30200,\n",
       "  30307,\n",
       "  30308,\n",
       "  30328,\n",
       "  30386,\n",
       "  30400,\n",
       "  30486,\n",
       "  30517,\n",
       "  30546,\n",
       "  30572,\n",
       "  30598,\n",
       "  30606,\n",
       "  30665,\n",
       "  30753,\n",
       "  30765,\n",
       "  30781,\n",
       "  30809,\n",
       "  30870,\n",
       "  31007,\n",
       "  31074,\n",
       "  31107,\n",
       "  31109,\n",
       "  31166,\n",
       "  31225,\n",
       "  31263,\n",
       "  31305,\n",
       "  31379,\n",
       "  31504,\n",
       "  31505,\n",
       "  31612,\n",
       "  31626,\n",
       "  31632,\n",
       "  31683,\n",
       "  31752,\n",
       "  31788,\n",
       "  31847,\n",
       "  31977,\n",
       "  32105,\n",
       "  32176,\n",
       "  32197,\n",
       "  32237,\n",
       "  32258,\n",
       "  32396,\n",
       "  32695,\n",
       "  32722,\n",
       "  32876,\n",
       "  32908,\n",
       "  33010,\n",
       "  33031,\n",
       "  33076,\n",
       "  33326,\n",
       "  33327,\n",
       "  33393,\n",
       "  33447,\n",
       "  33525,\n",
       "  33708,\n",
       "  33717,\n",
       "  33796,\n",
       "  33838,\n",
       "  33948,\n",
       "  33984,\n",
       "  34031,\n",
       "  34165,\n",
       "  34233,\n",
       "  34266,\n",
       "  34287,\n",
       "  34312,\n",
       "  34526,\n",
       "  34581,\n",
       "  34592,\n",
       "  34779,\n",
       "  34835,\n",
       "  34907,\n",
       "  35107,\n",
       "  35288,\n",
       "  35289,\n",
       "  35414,\n",
       "  35452,\n",
       "  35615,\n",
       "  35721,\n",
       "  35749,\n",
       "  35819,\n",
       "  35829,\n",
       "  35893,\n",
       "  35968,\n",
       "  35998,\n",
       "  36081,\n",
       "  36187,\n",
       "  36278,\n",
       "  36304,\n",
       "  36393,\n",
       "  36439,\n",
       "  36440,\n",
       "  36514,\n",
       "  36518,\n",
       "  36618,\n",
       "  36648,\n",
       "  36683,\n",
       "  36743,\n",
       "  36756,\n",
       "  36801,\n",
       "  36805,\n",
       "  36821,\n",
       "  36910,\n",
       "  36965,\n",
       "  37255,\n",
       "  37385,\n",
       "  37399,\n",
       "  37473,\n",
       "  37556,\n",
       "  37630,\n",
       "  37651,\n",
       "  37746,\n",
       "  37816,\n",
       "  37864,\n",
       "  37881,\n",
       "  38058,\n",
       "  38072,\n",
       "  38360,\n",
       "  38396,\n",
       "  38459,\n",
       "  38476,\n",
       "  38513,\n",
       "  38550,\n",
       "  38746,\n",
       "  38828,\n",
       "  38843,\n",
       "  38858,\n",
       "  38884,\n",
       "  38909,\n",
       "  39048,\n",
       "  39280,\n",
       "  39451,\n",
       "  39563,\n",
       "  39785,\n",
       "  39808,\n",
       "  39951,\n",
       "  40006,\n",
       "  40132,\n",
       "  40150,\n",
       "  40264,\n",
       "  40290,\n",
       "  40309,\n",
       "  40324,\n",
       "  40535,\n",
       "  40606,\n",
       "  40611,\n",
       "  40807,\n",
       "  40853,\n",
       "  41002,\n",
       "  41029,\n",
       "  41078,\n",
       "  41094,\n",
       "  41115,\n",
       "  41199,\n",
       "  41224,\n",
       "  41282,\n",
       "  41287,\n",
       "  41304,\n",
       "  41392,\n",
       "  41577,\n",
       "  41716,\n",
       "  41743,\n",
       "  42064,\n",
       "  42198,\n",
       "  42377,\n",
       "  42472,\n",
       "  42484,\n",
       "  42499,\n",
       "  42513,\n",
       "  42560,\n",
       "  42610,\n",
       "  42612,\n",
       "  42827,\n",
       "  42857,\n",
       "  43031,\n",
       "  43096,\n",
       "  43117,\n",
       "  43177,\n",
       "  43183,\n",
       "  43195,\n",
       "  43510,\n",
       "  43515,\n",
       "  43526,\n",
       "  43585,\n",
       "  43647,\n",
       "  43700,\n",
       "  43703,\n",
       "  43745,\n",
       "  43782,\n",
       "  43800,\n",
       "  43827,\n",
       "  43839,\n",
       "  43844,\n",
       "  43859,\n",
       "  44172,\n",
       "  44192,\n",
       "  44309,\n",
       "  44484,\n",
       "  44505,\n",
       "  44547,\n",
       "  44648,\n",
       "  44695,\n",
       "  44704,\n",
       "  44769,\n",
       "  44817,\n",
       "  44880,\n",
       "  45035,\n",
       "  45076,\n",
       "  45156,\n",
       "  45163,\n",
       "  45205,\n",
       "  45232,\n",
       "  45386,\n",
       "  45431,\n",
       "  45527,\n",
       "  45557,\n",
       "  45612,\n",
       "  45827,\n",
       "  45835,\n",
       "  45945,\n",
       "  45981,\n",
       "  46064,\n",
       "  46076,\n",
       "  46089,\n",
       "  46094,\n",
       "  46375,\n",
       "  46506,\n",
       "  46516,\n",
       "  46526,\n",
       "  46549,\n",
       "  46560,\n",
       "  46587,\n",
       "  46688,\n",
       "  47004,\n",
       "  47006,\n",
       "  47089,\n",
       "  47091,\n",
       "  47113,\n",
       "  47148,\n",
       "  47236,\n",
       "  47346,\n",
       "  47496,\n",
       "  47754,\n",
       "  47781,\n",
       "  47784,\n",
       "  47826,\n",
       "  48099,\n",
       "  48254,\n",
       "  48280,\n",
       "  48345,\n",
       "  48371,\n",
       "  48378,\n",
       "  48394,\n",
       "  48395,\n",
       "  48408,\n",
       "  48455,\n",
       "  48550,\n",
       "  48658,\n",
       "  48751,\n",
       "  48780,\n",
       "  48797,\n",
       "  48910,\n",
       "  48942,\n",
       "  49029,\n",
       "  49158,\n",
       "  49168,\n",
       "  49189,\n",
       "  49398,\n",
       "  49413,\n",
       "  49424,\n",
       "  49440,\n",
       "  49454,\n",
       "  49541,\n",
       "  49579,\n",
       "  49707,\n",
       "  49740,\n",
       "  49800,\n",
       "  49808,\n",
       "  49812,\n",
       "  49871,\n",
       "  49929,\n",
       "  49942,\n",
       "  49986,\n",
       "  50025,\n",
       "  50338,\n",
       "  50439,\n",
       "  50555,\n",
       "  50564,\n",
       "  50570,\n",
       "  50628,\n",
       "  50734,\n",
       "  50768,\n",
       "  50803,\n",
       "  50822,\n",
       "  50889,\n",
       "  51000,\n",
       "  51018,\n",
       "  51238,\n",
       "  51329,\n",
       "  51409,\n",
       "  51565,\n",
       "  51686,\n",
       "  51754,\n",
       "  51798,\n",
       "  51911,\n",
       "  52003,\n",
       "  52098,\n",
       "  52201,\n",
       "  52202,\n",
       "  52296,\n",
       "  52486,\n",
       "  52657,\n",
       "  52759,\n",
       "  52967,\n",
       "  53113,\n",
       "  53145,\n",
       "  53178,\n",
       "  53346,\n",
       "  53393,\n",
       "  53444,\n",
       "  53547,\n",
       "  53557,\n",
       "  53667,\n",
       "  53679,\n",
       "  53902,\n",
       "  53918,\n",
       "  53927,\n",
       "  54090,\n",
       "  54125,\n",
       "  54195,\n",
       "  54209,\n",
       "  54235,\n",
       "  54237,\n",
       "  54275,\n",
       "  54280,\n",
       "  54291,\n",
       "  54309,\n",
       "  54604,\n",
       "  54625,\n",
       "  54642,\n",
       "  54685,\n",
       "  54719,\n",
       "  54838,\n",
       "  54899,\n",
       "  54957,\n",
       "  54969,\n",
       "  55229,\n",
       "  55364,\n",
       "  55407,\n",
       "  55488,\n",
       "  55583,\n",
       "  55589,\n",
       "  55623,\n",
       "  55640,\n",
       "  55714,\n",
       "  55803,\n",
       "  55894,\n",
       "  55962,\n",
       "  55979,\n",
       "  56072,\n",
       "  56094,\n",
       "  56117,\n",
       "  56205,\n",
       "  56262,\n",
       "  56431,\n",
       "  56592,\n",
       "  56671,\n",
       "  56673,\n",
       "  56696,\n",
       "  56712,\n",
       "  56736,\n",
       "  56780,\n",
       "  56870,\n",
       "  57139,\n",
       "  57146,\n",
       "  57164,\n",
       "  57193,\n",
       "  57448,\n",
       "  57677,\n",
       "  57717,\n",
       "  57896,\n",
       "  57901,\n",
       "  57946,\n",
       "  57962,\n",
       "  58038,\n",
       "  58080,\n",
       "  58155,\n",
       "  58162,\n",
       "  58179,\n",
       "  58220,\n",
       "  58299,\n",
       "  58545,\n",
       "  58568,\n",
       "  58581,\n",
       "  58713,\n",
       "  58721,\n",
       "  58730,\n",
       "  58802,\n",
       "  58819,\n",
       "  58870,\n",
       "  58880,\n",
       "  58940,\n",
       "  59054,\n",
       "  59168,\n",
       "  59192,\n",
       "  59219,\n",
       "  59373,\n",
       "  59495,\n",
       "  59536,\n",
       "  59758,\n",
       "  59759,\n",
       "  59818,\n",
       "  59825,\n",
       "  59918,\n",
       "  59973,\n",
       "  60015,\n",
       "  60077,\n",
       "  60128,\n",
       "  60148,\n",
       "  60170,\n",
       "  60244,\n",
       "  60336,\n",
       "  60530,\n",
       "  60576,\n",
       "  60621,\n",
       "  60728,\n",
       "  60859,\n",
       "  61079,\n",
       "  61256,\n",
       "  61346,\n",
       "  61513,\n",
       "  61650,\n",
       "  61704,\n",
       "  61710,\n",
       "  61715,\n",
       "  61727,\n",
       "  61855,\n",
       "  61972,\n",
       "  62092,\n",
       "  62111,\n",
       "  62206,\n",
       "  62209,\n",
       "  62276,\n",
       "  62426,\n",
       "  62493,\n",
       "  62519,\n",
       "  62668,\n",
       "  62914,\n",
       "  63109,\n",
       "  63178,\n",
       "  63292,\n",
       "  63451,\n",
       "  63517,\n",
       "  63571,\n",
       "  63639,\n",
       "  63674,\n",
       "  63760,\n",
       "  63826,\n",
       "  63916,\n",
       "  64019,\n",
       "  64040,\n",
       "  64045,\n",
       "  64080,\n",
       "  64184,\n",
       "  64320,\n",
       "  64466,\n",
       "  64706,\n",
       "  64748,\n",
       "  64821,\n",
       "  64848,\n",
       "  64862,\n",
       "  64901,\n",
       "  65010,\n",
       "  65119,\n",
       "  65189,\n",
       "  65210,\n",
       "  65261,\n",
       "  65280,\n",
       "  65610,\n",
       "  65891,\n",
       "  65922,\n",
       "  66045,\n",
       "  66096,\n",
       "  66264,\n",
       "  66291,\n",
       "  66301,\n",
       "  66377,\n",
       "  66380,\n",
       "  66498,\n",
       "  66530,\n",
       "  66544,\n",
       "  66562,\n",
       "  66712,\n",
       "  66786,\n",
       "  66810,\n",
       "  66875,\n",
       "  66976,\n",
       "  67086,\n",
       "  67177,\n",
       "  67276,\n",
       "  67289,\n",
       "  67302,\n",
       "  67318,\n",
       "  67320,\n",
       "  67429,\n",
       "  67457,\n",
       "  67522,\n",
       "  67553,\n",
       "  67746,\n",
       "  67755,\n",
       "  67784,\n",
       "  67786,\n",
       "  67801,\n",
       "  67899,\n",
       "  68018,\n",
       "  68035,\n",
       "  68083,\n",
       "  68084,\n",
       "  68321,\n",
       "  68590,\n",
       "  68627,\n",
       "  68672,\n",
       "  68698,\n",
       "  68708,\n",
       "  68728,\n",
       "  68744,\n",
       "  68819,\n",
       "  68847,\n",
       "  68916,\n",
       "  68951,\n",
       "  68966,\n",
       "  68990,\n",
       "  69300,\n",
       "  69343,\n",
       "  69395,\n",
       "  69468,\n",
       "  69562,\n",
       "  69605,\n",
       "  69644,\n",
       "  69665,\n",
       "  69708,\n",
       "  69731,\n",
       "  69743,\n",
       "  69795,\n",
       "  69834,\n",
       "  69955,\n",
       "  ...])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# without stratifying, the academicDigital class (most minor) appears at the order printed below\n",
    "# as you can see, there are steps (i.e., batches of 32) in which that class doesn't appear at all\n",
    "# no big difference between enabling/disabling stratification, as dataset is too imbalanced\n",
    "lbls_loc = []\n",
    "for i, a in enumerate(trainer_debug.train_dataloader.dataset.datasets.labels):\n",
    "    if a == 8:\n",
    "        lbls_loc.append(i)\n",
    "len(lbls_loc), lbls_loc\n",
    "diff_between_locs = []\n",
    "prev_loc = 0\n",
    "for loc in lbls_loc:\n",
    "    if loc-prev_loc < 64:\n",
    "        continue\n",
    "    diff_between_locs.append(loc-prev_loc)\n",
    "    prev_loc = loc\n",
    "len(diff_between_locs), lbls_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, [117, 139, 145, 277, 299, 305])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aa = []\n",
    "for i, a in enumerate(history['step_y_true_ol_1']):\n",
    "    if a == 8:\n",
    "        aa.append(i)\n",
    "len(aa), aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(history['step_y_true_ol_1'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
