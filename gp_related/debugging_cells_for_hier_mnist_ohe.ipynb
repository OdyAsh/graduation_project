{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "import torch\n",
    "ff = torchmetrics.Accuracy('multiclass', num_classes=10, average=None, top_k=1, threshold=0.5)\n",
    "ff.update(torch.tensor([2]), torch.tensor([2]))\n",
    "ff.update(torch.tensor([0]), torch.tensor([2]))\n",
    "ff.update(torch.tensor([1]), torch.tensor([1]))\n",
    "res = ff.compute()\n",
    "ff.reset()\n",
    "res\n",
    "# output: tensor([0.0000, 1.0000, 0.5000])\n",
    "# when using average='micro', the output: tensor(0.6667) (as 2 correct preds / 3 all preds = 0.6667)\n",
    "# when using average='macro', the output: tensor(0.5000) (as (0+1+0.5)/3 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | conv1    | Conv2d     | 320   \n",
      "1 | conv2    | Conv2d     | 18.5 K\n",
      "2 | dropout1 | Dropout2d  | 0     \n",
      "3 | dropout2 | Dropout2d  | 0     \n",
      "4 | fc1      | LazyLinear | 0     \n",
      "5 | fc2      | LazyLinear | 0     \n",
      "----------------------------------------\n",
      "18.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.8 K    Total params\n",
      "0.075     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa75e9fda9f4d3e83cb1068e4913db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\projects\\graduation_project\\.mamba\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "from pytorch_lightning.utilities.types import EPOCH_OUTPUT\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, the_device='cuda', num_classes=10, dummy_input_size=(32,3,306,306)):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.LazyLinear(128)\n",
    "        self.fc2 = nn.LazyLinear(10)\n",
    "        \n",
    "        # useful for writing computational graph in tensorboard, summary(), etc\n",
    "        # self.example_input_array = torch.randn(*dummy_input_size) \n",
    "\n",
    "        self.the_device = the_device\n",
    "\n",
    "        def _create_metric_func(metric_type:str):\n",
    "            if 'acc' in metric_type.lower():\n",
    "                return torchmetrics.Accuracy(task='multiclass', \n",
    "                                             num_classes=num_classes, \n",
    "                                             average=\"micro\").to(the_device)\n",
    "            elif 'f1' in metric_type.lower():\n",
    "                return torchmetrics.F1Score(task=\"multiclass\", \n",
    "                                            num_classes=num_classes, \n",
    "                                            average=None).to(the_device)\n",
    "        \n",
    "        # for calculating metric of a step\n",
    "        self.acc_step_funcs = [_create_metric_func('acc') for _ in range(3)] # for computing train, val, test results respectively\n",
    "        self.f1_score_step_funcs = [_create_metric_func('f1') for _ in range(3)]\n",
    "        # for calculating metric of an epoch\n",
    "        self.acc_epoch_funcs = [_create_metric_func('acc') for _ in range(3)]\n",
    "        self.f1_score_epoch_funcs = [_create_metric_func('f1') for _ in range(3)]\n",
    "        \n",
    "        self.history = {\n",
    "            # per epoch\n",
    "            'loss':[],\n",
    "            'acc':[],\n",
    "            'f1_score':[],\n",
    "            'val_loss':[],\n",
    "            'val_acc':[],\n",
    "            'val_f1_score':[],\n",
    "            'test_loss':[],\n",
    "            'test_acc':[],\n",
    "            'test_f1_score':[],\n",
    "            # per step\n",
    "            'step_loss':[],\n",
    "            'step_acc':[],\n",
    "            'step_f1_score':[],\n",
    "            'val_step_loss':[],\n",
    "            'val_step_acc':[],\n",
    "            'val_step_f1_score':[],\n",
    "            'test_step_loss':[],\n",
    "            'test_step_acc':[],\n",
    "            'test_step_f1_score':[],\n",
    "            # for tracking pred/true values for train/val/test sets\n",
    "            'step_y_pred': [],\n",
    "            'step_y_true': [],\n",
    "            'val_step_y_pred': [],\n",
    "            'val_step_y_true': [],\n",
    "            'test_step_y_pred': [],\n",
    "            'test_step_y_true': [],\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def _store_metric(self, metric_name, metric_val):\n",
    "        try:\n",
    "            self.log(metric_name, metric_val)\n",
    "        except Exception as e:\n",
    "            # then metric_val is not scalar, then this means:\n",
    "            # metric_name is an f1 score metric \n",
    "            # so, we just take the mean value\n",
    "            self.log(metric_name, metric_val.mean())\n",
    "        # putting this here ensures we don't include the mean of f1 score, \n",
    "        # but rather the tensor of shape (1, num_classes)\n",
    "        self.history[metric_name].append(metric_val)\n",
    "\n",
    "    def _step_logic(self, batch, prefix=''):\n",
    "        if 'val' in prefix:\n",
    "            func_idx = 1\n",
    "        elif 'test' in prefix:\n",
    "            func_idx = 2\n",
    "        else:\n",
    "            func_idx = 0\n",
    "        \n",
    "        x, y_true = batch\n",
    "        y_pred = self(x)\n",
    "        # log step metrics\n",
    "        loss = F.cross_entropy(y_pred, y_true)\n",
    "        self._store_metric(f'{prefix}step_loss', loss)\n",
    "\n",
    "        acc = self.acc_step_funcs[func_idx](y_pred, y_true)\n",
    "        self._store_metric(f'{prefix}step_acc', acc)\n",
    "        self.acc_epoch_funcs[func_idx].update(y_pred, y_true)\n",
    "    \n",
    "        f1_score = self.f1_score_step_funcs[func_idx](y_pred, y_true)\n",
    "        self._store_metric(f'{prefix}step_f1_score', f1_score)\n",
    "        self.f1_score_epoch_funcs[func_idx].update(y_pred, y_true)\n",
    "\n",
    "        # storing y_pred/y_true\n",
    "        self.history[f'{prefix}step_y_pred'].extend(y_pred)\n",
    "        self.history[f'{prefix}step_y_true'].extend(y_true)\n",
    "\n",
    "        return loss, acc, f1_score\n",
    "    \n",
    "    def _epoch_end_logic(self, outs, prefix=''):\n",
    "        if 'val' in prefix:\n",
    "            func_idx = 1\n",
    "        elif 'test' in prefix:\n",
    "            func_idx = 2\n",
    "        else:\n",
    "            func_idx = 0\n",
    "\n",
    "        # log epoch metrics\n",
    "        loss_epoch = torch.tensor([x[f'{prefix}loss'] for x in outs], dtype=float).mean()\n",
    "        self._store_metric(f'{prefix}loss', loss_epoch)\n",
    "\n",
    "        acc_epoch = self.acc_epoch_funcs[func_idx].compute()\n",
    "        self.acc_epoch_funcs[func_idx].reset()\n",
    "        self._store_metric(f'{prefix}acc', acc_epoch)\n",
    "\n",
    "        f1_score_epoch = self.f1_score_epoch_funcs[func_idx].compute()\n",
    "        self.f1_score_epoch_funcs[func_idx].reset()\n",
    "        self._store_metric(f'{prefix}f1_score', f1_score_epoch)\n",
    "\n",
    "        return loss_epoch, acc_epoch, f1_score_epoch\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        prefix = ''\n",
    "        loss, acc, f1_score = self._step_logic(batch, prefix=prefix)\n",
    "        return {f'{prefix}loss':loss, f'{prefix}acc':acc, f'{prefix}f1_score':f1_score}\n",
    "    \n",
    "    def training_epoch_end(self, outs) -> None:\n",
    "        # 'outs' argument here contains values from what was returned from training_step()\n",
    "        _,_,_ = self._epoch_end_logic(outs)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        prefix = 'val_'\n",
    "        loss, acc, f1_score = self._step_logic(batch, prefix=prefix)\n",
    "        return {f'{prefix}loss':loss, f'{prefix}acc':acc, f'{prefix}f1_score':f1_score}\n",
    "\n",
    "    def validation_epoch_end(self, outs) -> None:\n",
    "        _,_,_ = self._epoch_end_logic(outs, prefix='val_')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        prefix = 'test_'\n",
    "        loss, acc, f1_score = self._step_logic(batch, prefix=prefix)\n",
    "        return {f'{prefix}loss':loss, f'{prefix}acc':acc, f'{prefix}f1_score':f1_score}\n",
    "\n",
    "    def test_epoch_end(self, outs) -> None:\n",
    "        _,_,_ = self._epoch_end_logic(outs, prefix='test_')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "cnnLightning = MNISTClassifier(the_device='cuda')\n",
    "# setup data\n",
    "dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "train_loader = utils.data.DataLoader(train_dataset, batch_size=32, num_workers=4)\n",
    "val_loader = utils.data.DataLoader(val_dataset, batch_size=32, num_workers=4)\n",
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = pl.Trainer(log_every_n_steps=1, num_sanity_val_steps=0, max_epochs=3, accelerator='gpu', fast_dev_run=False)\n",
    "trainer.fit(model=cnnLightning, train_dataloaders=train_loader, val_dataloaders=val_loader) # , val_dataloaders=val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.model.history.copy()\n",
    "for metric, tensors_list in history.items():\n",
    "    if isinstance(tensors_list, list) and len(tensors_list) > 0 and isinstance(tensors_list[0], torch.Tensor):\n",
    "        history[metric] = np.array([np.array(tens.detach().cpu()) for tens in tensors_list]).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.5039, 0.4961],\n",
       "         [0.5040, 0.4960],\n",
       "         [0.5040, 0.4960],\n",
       "         [0.5040, 0.4960]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.2513, 0.2444, 0.2263, 0.2780],\n",
       "         [0.2514, 0.2444, 0.2262, 0.2780],\n",
       "         [0.2513, 0.2444, 0.2263, 0.2780],\n",
       "         [0.2513, 0.2444, 0.2262, 0.2780]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.2561, 0.2539, 0.2301, 0.2599],\n",
       "         [0.2561, 0.2538, 0.2302, 0.2599],\n",
       "         [0.2561, 0.2538, 0.2301, 0.2599],\n",
       "         [0.2561, 0.2539, 0.2301, 0.2599]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3410, 0.3629, 0.2960],\n",
       "         [0.3410, 0.3629, 0.2961],\n",
       "         [0.3410, 0.3629, 0.2960],\n",
       "         [0.3410, 0.3630, 0.2960]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3407, 0.3076, 0.3518],\n",
       "         [0.3407, 0.3075, 0.3518],\n",
       "         [0.3407, 0.3075, 0.3518],\n",
       "         [0.3407, 0.3075, 0.3517]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3223, 0.3448, 0.3329],\n",
       "         [0.3223, 0.3448, 0.3329],\n",
       "         [0.3223, 0.3449, 0.3329],\n",
       "         [0.3223, 0.3449, 0.3328]], grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# debugging cell: dummy example for demonstration purposes:\n",
    "# the code in this cell can be found in forward()\n",
    "\n",
    "# len(hier_y_probs) == num_hierarchy_output_layers, \n",
    "# while each element is 2D tensor of shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "hier_y_pred = []\n",
    "for tensor_output in demo_model(a):\n",
    "    hier_y_pred.append(F.softmax(tensor_output, dim=1))\n",
    "hier_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 3, 2, 2, 2],\n",
      "        [0, 1, 3, 0, 2, 2],\n",
      "        [0, 1, 3, 1, 2, 2],\n",
      "        [1, 3, 0, 2, 0, 2]])\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 1, 1, 3],\n",
      "        [3, 3, 3, 0],\n",
      "        [2, 0, 1, 2],\n",
      "        [2, 2, 2, 0],\n",
      "        [2, 2, 2, 2]])\n",
      "\n",
      "tensor([0, 0, 0, 1])\n",
      "tensor([[0.5039, 0.4961],\n",
      "        [0.5040, 0.4960],\n",
      "        [0.5040, 0.4960],\n",
      "        [0.5040, 0.4960]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# debugging cell: dummy example for demonstration purposes:\n",
    "# the code in this cell can be found in train_step() (or actually, in _step_logic())\n",
    "\n",
    "# x, y = batch\n",
    "# hier_y_pred = self(x)\n",
    "y = torch.tensor([1,2,5,8]) # remove this in train_step()\n",
    "# y now has shape (batch_size, len(hier_y_pred))\n",
    "# in other words, each row now consists of `len(hier_y_pred)` output layers' labels of 1 sample\n",
    "y = torch.tensor([labelToHierarchy[i] for i in range(len(y))], dtype=int) # alternatively, range(y.size()[0])\n",
    "print(y)\n",
    "# transpose y to take each row (batch of labels) with its corresponding hier_y_pred row (batch of predicted labels)\n",
    "# in other words, each row now consists of `batch_size` labels of the i_th output layer\n",
    "y = y.T\n",
    "print(y)\n",
    "print()\n",
    "# softmax_output_layer elements each has shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "# side note: the wording of \"i_th output layers\" refers to the strings mentioned in \n",
    "# output_order argument of HierarchalModel() used in create_hnn_model_arch()\n",
    "for i, softmax_output_layer in enumerate(hier_y_pred):\n",
    "    # 'cur' refers to current output layer\n",
    "    y_cur = y[i]\n",
    "    y_pred_cur = softmax_output_layer\n",
    "    print(y_cur)\n",
    "    print(y_pred_cur)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function would've been useful if in training_step(), we returned 'y' as ohe (e.g., [0,0,1]) instead of integer labels (e.g., 2)\n",
    "def ohe(value, size): # ohe == one hot encode\n",
    "    identity = torch.eye(size)\n",
    "    one_hot = identity[value]\n",
    "    if (value == -1):\n",
    "        # then we want the ohe list to be all zeros\n",
    "        one_hot[-1] = 0\n",
    "    # uncomment this line if you want to return 2D tensor of shape (1, size)\n",
    "    # return one_hot.view(-1, size)\n",
    "    return one_hot\n",
    "\n",
    "classToHierarchy = {\n",
    "    '00. selfies' : [ohe(0, 2), # diversity of colors: many or few\n",
    "                     ohe(0, 4), # many colors: selfies, memes (fmm or emm), or eGreetingsAndMisc (+1 if none of them)\n",
    "                     ohe(-1, 4), # few colors: socialMedia (fsm or esm), fTxtMssgs, academic (photos or digital) (+1 if none of them)\n",
    "                     ohe(-1, 3), # meme type: fmm or emm (+1 if none of them)\n",
    "                     ohe(-1, 3), # social media type: fsm or esm (+1 if none of them)\n",
    "                     ohe(-1, 3)], # academic type: photos or digital (+1 if none of them)\n",
    "    '10. fmemes' : [ohe(0, 2), ohe(1, 4), ohe(-1, 4), ohe(0, 3), ohe(-1, 3), ohe(-1, 3)],\n",
    "    '20. ememes' : [ohe(0, 2), ohe(1, 4), ohe(-1, 4), ohe(1, 3), ohe(-1, 3), ohe(-1, 3)],\n",
    "    '30. fSocialMedia' : [ohe(1, 2), ohe(-1, 4), ohe(0, 4), ohe(-1, 3), ohe(0, 3), ohe(-1, 3)],\n",
    "    '40. eSocialMedia' : [ohe(1, 2), ohe(-1, 4), ohe(0, 4), ohe(-1, 3), ohe(1, 3), ohe(-1, 3)],\n",
    "    '50. fTxtMssgs' : [ohe(1, 2), ohe(-1, 4), ohe(1, 4), ohe(-1, 3), ohe(-1, 3), ohe(-1, 3)],\n",
    "    '70. eGreetingAndMisc' : [ohe(0, 2), ohe(2, 4), ohe(-1, 4), ohe(-1, 3), ohe(-1, 3), ohe(-1, 3)],\n",
    "    '81. academicPhotos' : [ohe(1, 2), ohe(-1, 4), ohe(2, 4), ohe(-1, 3), ohe(-1, 3), ohe(0, 3)],\n",
    "    '82. academicDigital' : [ohe(1, 2), ohe(-1, 4), ohe(2, 4), ohe(-1, 3), ohe(-1, 3), ohe(1, 3)],\n",
    "}\n",
    "classToHierarchy['00. selfies']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hierarchical model with old (not working) manual logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "import torchmetrics\n",
    "\n",
    "class HierarchalModelPL(pl.LightningModule):\n",
    "   def __init__(self, hierarchical_model, num_classes_per_layer:list, loss_weights=None, dummy_input_size=(32, 3, 306, 306), the_device='cpu'):\n",
    "      super(HierarchalModelPL, self).__init__()\n",
    "      self.hierarchical_model = hierarchical_model # .to(the_device)\n",
    "      self.num_output_layers = len(num_classes_per_layer)\n",
    "      self.loss_weights = [1]*self.num_output_layers if loss_weights is None else loss_weights\n",
    "      self.num_output_layers = self.num_output_layers\n",
    "      \n",
    "      # useful for writing computational graph in tensorboard, summary(), etc\n",
    "      self.example_input_array = torch.randn(*dummy_input_size) # , device=the_device\n",
    "\n",
    "      self.the_device = the_device if 'cpu' in the_device else 'cuda'\n",
    "      self.metrics = ['acc', 'f1_score']\n",
    "      def _create_metric_func(metric_type:str, num_classes):\n",
    "         if 'acc' in metric_type.lower():\n",
    "               return torchmetrics.Accuracy(task='multiclass', \n",
    "                                          num_classes=num_classes, \n",
    "                                          average=\"micro\").to(the_device)\n",
    "         elif 'f1' in metric_type.lower():\n",
    "               return torchmetrics.F1Score(task=\"multiclass\", \n",
    "                                          num_classes=num_classes, \n",
    "                                          average=None).to(the_device)\n",
    "      \n",
    "      #                                         step/epoch vvv\n",
    "      # max indexing possible: self.metric_funcs['f1_score'][1][num_output_layers-1][2]\n",
    "      #                                       softmax output layers ^^^            ^^^  train/val/test \n",
    "      # output example of metric_funcs['acc']\n",
    "      # (note: MCA == MulticlassAccuracy()):\n",
    "      # {'acc': [ \n",
    "      #   [ #step\n",
    "      #    [MCA, MCA, MCA], # softmax_output_layer_0 ; train/val/test\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA], # ...\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA] # # softmax_output_layer_5 ; train/val/test\n",
    "      #   ],\n",
    "      #   [ #epoch\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA],\n",
    "      #    [MCA, MCA, MCA]\n",
    "      #   ]\n",
    "      # ]\n",
    "      \n",
    "      self.metric_funcs = {}\n",
    "      for metric in self.metrics:\n",
    "         # i --> step/epoch, j --> num_output_layers-1, k --> train/val/test\n",
    "         self.metric_funcs[metric] = [[[_create_metric_func(metric, num_classes_per_layer[j]) for k in range(3)] \n",
    "                                       for j in range(self.num_output_layers)] \n",
    "                                       for i in range(2)]\n",
    "      \n",
    "      # history_per_output_layer = {\n",
    "      #    # per epoch\n",
    "      #    'loss':[],\n",
    "      #    'acc':[],\n",
    "      #    'f1_score':[],\n",
    "      #    'val_loss':[],\n",
    "      #    'val_acc':[],\n",
    "      #    'val_f1_score':[],\n",
    "      #    'test_loss':[],\n",
    "      #    'test_acc':[],\n",
    "      #    'test_f1_score':[],\n",
    "      #    # per step\n",
    "      #    'step_loss':[],\n",
    "      #    'step_acc':[],\n",
    "      #    'step_f1_score':[],\n",
    "      #    'val_step_loss':[],\n",
    "      #    'val_step_acc':[],\n",
    "      #    'val_step_f1_score':[],\n",
    "      #    'test_step_loss':[],\n",
    "      #    'test_step_acc':[],\n",
    "      #    'test_step_f1_score':[],\n",
    "      #    # for tracking pred/true values for train/val/test sets\n",
    "      #    'step_y_pred': [],\n",
    "      #    'step_y_true': [],\n",
    "      #    'val_step_y_pred': [],\n",
    "      #    'val_step_y_true': [],\n",
    "      #    'test_step_y_pred': [],\n",
    "      #    'test_step_y_true': [],\n",
    "      # }\n",
    "\n",
    "      history_per_output_layer = {\n",
    "         # per epoch\n",
    "         'loss':[],\n",
    "         'acc':[],\n",
    "         'f1_score':[],\n",
    "         # these 2 will be replaced with step_y_pred and step_y_true respectively\n",
    "         'y_pred':[],\n",
    "         'y_true':[],\n",
    "      }\n",
    "\n",
    "      def pre_suf_fix_dict(dict, fix_val='output_layer', prefix=True):\n",
    "         return {f\"{fix_val}_{key}\" if prefix else f\"{key}_{fix_val}\" : value for key, value in dict.items()}\n",
    "\n",
    "      # to create val/test prefixed keys\n",
    "      history_per_output_layer.update(\n",
    "         **pre_suf_fix_dict(history_per_output_layer, fix_val='val', prefix=True),\n",
    "         **pre_suf_fix_dict(history_per_output_layer, fix_val='test', prefix=True),\n",
    "      )\n",
    "      # to create step_ prefixed keys\n",
    "      history_per_output_layer.update(\n",
    "         **pre_suf_fix_dict(history_per_output_layer, fix_val='step', prefix=True),\n",
    "      )\n",
    "      # removing y_pred, y_true and keeping step_y_pred and step_y_true\n",
    "      history_per_output_layer.pop('y_pred')\n",
    "      history_per_output_layer.pop('y_true')\n",
    "      \n",
    "      # fix_val == suffix without \"_\"; to provide flexibility in using fix_val as prefix as well (if we wanted to!)\n",
    "      self.history_layers_fix_val = 'output_layer' \n",
    "\n",
    "      # pre_suf_fix_dict function is used to create unique key names to merge the `num_output_layers` histories into one history dictionary\n",
    "      history_ol_lists = [pre_suf_fix_dict(history_per_output_layer, \n",
    "                                           fix_val=f'{self.history_layers_fix_val}_{i}', \n",
    "                                           prefix=False) \n",
    "                           for i in range(self.num_output_layers)] # ol == output layers\n",
    "      \n",
    "      # 'loss' here is for the final loss calculated by the loss aggregator function which uses all `num_output_layers` losses\n",
    "      self.history = {'loss':[], 'val_loss':[], 'test_loss':[]}\n",
    "      self.history.update(**pre_suf_fix_dict(self.history, 'step', prefix=True))\n",
    "      # merging the `num_output_layers` histories into one history dictionary\n",
    "      for hist_dict in history_ol_lists:\n",
    "         self.history.update(**hist_dict)\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "      # len(hier_y_probs) == num_hierarchy_output_layers, \n",
    "      # while each element is 2D tensor of shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "      hier_y_pred = []\n",
    "      for tensor_output in self.hierarchical_model(x):\n",
    "         hier_y_pred.append(F.softmax(tensor_output, dim=1))\n",
    "      hier_y_pred\n",
    "      return hier_y_pred\n",
    "   \n",
    "   def training_step(self, batch, batch_idx):\n",
    "      ds_prefix = ''\n",
    "      metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "      # add other key:value pairs here or pass the entire metrics_dict if you want. just make sure 'loss' key is present\n",
    "      return metrics_dict\n",
    "    \n",
    "   def training_epoch_end(self, outputs) -> None:\n",
    "      # 'outputs' argument here contains values from what was returned from training_step()\n",
    "      ds_prefix = ''\n",
    "      self._epoch_end_logic(outputs, ds_prefix)\n",
    "\n",
    "   def validation_step(self, batch, batch_idx):\n",
    "      ds_prefix = 'val_'\n",
    "      metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "      return metrics_dict\n",
    "   \n",
    "   def validation_epoch_end(self, outputs) -> None:\n",
    "      ds_prefix = 'val_'\n",
    "      self._epoch_end_logic(outputs, ds_prefix)\n",
    "   \n",
    "   def test_step(self, batch, batch_idx):\n",
    "      ds_prefix = 'test_'\n",
    "      metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "      return metrics_dict\n",
    "   \n",
    "   def test_epoch_end(self, outputs) -> None:\n",
    "      ds_prefix = 'test_'\n",
    "      self._epoch_end_logic(outputs, ds_prefix=ds_prefix)\n",
    "   \n",
    "\n",
    "   def _store_metric(self, metric_name, metric_val):\n",
    "      try:\n",
    "         self.log(metric_name, metric_val)\n",
    "      except Exception:\n",
    "         # then metric_val is not scalar, then this means:\n",
    "         # metric_name is an f1 score metric \n",
    "         # so, we just take the mean value\n",
    "         self.log(metric_name, metric_val.mean())\n",
    "      # putting this here ensures we don't include the mean of f1 score, \n",
    "      # but rather the tensor of shape (1, num_classes)\n",
    "      self.history[metric_name].append(metric_val)\n",
    "\n",
    "   def _step_logic(self, batch, ds_prefix=''):\n",
    "      # ds_prefix == dataset_prefix\n",
    "      if 'val' in ds_prefix:\n",
    "         ds_type_idx = 1\n",
    "      elif 'test' in ds_prefix:\n",
    "         ds_type_idx = 2\n",
    "      else:\n",
    "         ds_type_idx = 0\n",
    "      ds_prefix = 'step_' + ds_prefix\n",
    "\n",
    "      x, y = batch\n",
    "      hier_y_pred = self(x)\n",
    "\n",
    "      # y now has shape (batch_size, len(hier_y_pred))\n",
    "      # in other words, each row now consists of `len(hier_y_pred)` output layers' labels of 1 sample\n",
    "      # side note: len(hier_y_pred) == self.num_output_layers\n",
    "      y = torch.tensor([labelToHierarchy[int(y[i])] for i in range(len(y))], dtype=int, device=self.the_device) # alternatively, range(y.size()[0])\n",
    "      # transpose y to take each row (batch of labels) with its corresponding hier_y_pred row (batch of predicted labels)\n",
    "      # in other words, each row now consists of `batch_size` labels of the i_th output layer\n",
    "      y = y.T\n",
    "      # softmax_output_layer elements each has shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "      # side note: the wording of \"i_th output layers\" refers to the strings mentioned in \n",
    "      # output_order argument of HierarchalModel() used in create_hnn_model_arch()\n",
    "\n",
    "      losses_dict = {}\n",
    "      other_metrics_dict = {}\n",
    "      for out_layer_idx, softmax_output_layer in enumerate(hier_y_pred): \n",
    "         # 'cur' refers to current output layer\n",
    "         y_cur = y[out_layer_idx]\n",
    "         y_pred_cur = softmax_output_layer.to(self.the_device)\n",
    "         ol_suffix = f'_{self.history_layers_fix_val}_{out_layer_idx}'\n",
    "\n",
    "         # log step metrics\n",
    "\n",
    "         loss = F.cross_entropy(y_pred_cur, y_cur)\n",
    "         loss_full_metric_name = f'{ds_prefix}loss{ol_suffix}'\n",
    "         self._store_metric(loss_full_metric_name, loss)\n",
    "         losses_dict[loss_full_metric_name] = loss\n",
    "\n",
    "         for metric_name in self.metrics:\n",
    "            # recall the indexing of self.metric_funcs:\n",
    "            # metric_name, step/epoch, num_output_layers-1, train/val/test\n",
    "            metric_val = self.metric_funcs[metric_name][0][out_layer_idx][ds_type_idx](y_pred_cur, y_cur)\n",
    "            full_metric_name = f'{ds_prefix}{metric_name}{ol_suffix}'\n",
    "            self._store_metric(full_metric_name, metric_val)\n",
    "            self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].update(y_pred_cur, y_cur)\n",
    "            other_metrics_dict[full_metric_name] = metric_val\n",
    "      \n",
    "         f1_score = self.metric_funcs[metric_name][0][out_layer_idx][ds_type_idx](y_pred_cur, y_cur)\n",
    "         self._store_metric(f'{ds_prefix}{metric_name}{ol_suffix}', f1_score)\n",
    "         self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].update(y_pred_cur, y_cur)\n",
    "\n",
    "         # storing y_pred/y_true\n",
    "         self.history[f'{ds_prefix}y_pred{ol_suffix}'].extend(y_pred_cur)\n",
    "         self.history[f'{ds_prefix}y_true{ol_suffix}'].extend(y_cur)\n",
    "\n",
    "      final_loss = self.metric_reduce_fx(losses_dict.values(), 'weighted_sum')\n",
    "      self._store_metric(f'{ds_prefix}loss', final_loss)\n",
    "\n",
    "      # Important note: 'loss' key must be present, or else you'll get this error:\n",
    "      # MisconfigurationException: In automatic_optimization, \n",
    "      # when `training_step` returns a dict, the 'loss' key needs to be present\n",
    "      # side note: add `.update(losses_dict)` and `.update(other_metrics_dict)` \n",
    "      # if you want to directly use other metrics in \"..._epoch_end()\" methods\n",
    "      return {f'loss' : final_loss} \n",
    "\n",
    "   def _epoch_end_logic(self, outputs, ds_prefix=''):\n",
    "      if 'val' in ds_prefix:\n",
    "         ds_type_idx = 1\n",
    "      elif 'test' in ds_prefix:\n",
    "         ds_type_idx = 2\n",
    "      else:\n",
    "         ds_type_idx = 0\n",
    "\n",
    "      # log epoch metrics\n",
    "      final_loss_epoch = torch.tensor([x[f'loss'] for x in outputs], dtype=float).mean()\n",
    "      self._store_metric(f'{ds_prefix}loss', final_loss_epoch)\n",
    "\n",
    "      for out_layer_idx in range(self.num_output_layers):\n",
    "         ol_suffix = f'_{self.history_layers_fix_val}_{out_layer_idx}'\n",
    "\n",
    "         for metric_name in self.metrics:\n",
    "            # recall: '1' for accessing epoch func (not step func)\n",
    "            metric_val_epoch = self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].compute()\n",
    "            self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].reset()\n",
    "            full_metric_name = f'{ds_prefix}{metric_name}{ol_suffix}'\n",
    "            self._store_metric(full_metric_name, metric_val_epoch)\n",
    "            if out_layer_idx == 0:\n",
    "               print(metric_name)\n",
    "               print(metric_val_epoch)\n",
    "               print()\n",
    "   \n",
    "   def metric_reduce_fx(self, metric_list, agg_type='weighted_sum'):\n",
    "      '''\n",
    "      aggregates metric values from all `num_output_layers` into a single value\n",
    "      side note: called \"reduce_fx\" as a reference to PyTorch Lightning's reduce_fx parameter found in self.log()\n",
    "      '''\n",
    "      if 'weighted' in agg_type.lower() and 'sum' in agg_type.lower():\n",
    "         weighted_sum_val = 0\n",
    "         for i, layer_metric_val in enumerate(metric_list):\n",
    "            weighted_sum_val += self.loss_weights[i] * layer_metric_val\n",
    "         final_val = weighted_sum_val\n",
    "\n",
    "      return final_val\n",
    "\n",
    "   def configure_optimizers(self):\n",
    "      # make it self.hierarchical_model.parameters() if you defined other parameters in __init()__ which you don't want to optimize\n",
    "      # side note: under the hood, pl automatically gets gradients \n",
    "      # from final_loss returned from training_step() and adjusts the models' branches accordingly\n",
    "      # source: https://github.com/Lightning-AI/lightning/issues/2645#issuecomment-660681760\n",
    "      return torch.optim.Adam(self.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "side note: for some reason setting `num_workers` argument for dataloader in MNIST dataset code above works\n",
    "while doing so for this cell does not work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CS\\projects\\graduation_project\\.mamba\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_order:\n",
      "[('colorDiversity', 2), ('manyColors', 4), ('fewColors', 4), ('memes', 3), ('socialMedia', 3), ('academic', 3)]\n",
      "94787 train images\n",
      "20311 val images\n",
      "so, #steps per epoch (if sampler is not batch):\n",
      "115098\n",
      "\n",
      "2962 training steps\n",
      "634 validation steps\n",
      "so #steps per epoch (if sampler is batch):\n",
      "3596\n",
      "\n",
      "20312 test images\n",
      "634 testing steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type            | Params | In sizes          | Out sizes                                             \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | hierarchical_model | HierarchalModel | 21.2 M | [32, 3, 306, 306] | [[32, 2], [32, 4], [32, 4], [32, 3], [32, 3], [32, 3]]\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "21.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.2 M    Total params\n",
      "84.640    Total estimated model params size (MB)\n",
      "d:\\CS\\projects\\graduation_project\\.mamba\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "d:\\CS\\projects\\graduation_project\\.mamba\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a0d5a023b54b5cacee15e7d9be11c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pickle\n",
    "    def pklLoad(fullPath):\n",
    "        with open(fullPath, 'rb') as f:\n",
    "            content = pickle.load(f)\n",
    "        return content\n",
    "\n",
    "    dataDir = '../dataset/'\n",
    "    dataReDir = '../dataset_related/'\n",
    "    classNameToNum = pklLoad('../dataset_related/imgTypeToNum.pickle')\n",
    "\n",
    "    import numpy as np\n",
    "    import pytorch_lightning as pl\n",
    "    from torch.nn import functional as F\n",
    "    import torchmetrics\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import torchvision\n",
    "    from torchvision import transforms as T\n",
    "    from torch import nn\n",
    "    from simple_hierarchy.hierarchal_model import HierarchalModel\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    from PIL import Image\n",
    "\n",
    "\n",
    "    class CustomDatasetForV03(torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Implementation notes: \n",
    "        you must have a column consisting of image names (with extension) as index of the dataframe (example: ['img0001.jpg', 'img0002.png', ...])\n",
    "        each image name has to be unique across the entire dataset, not just its class\n",
    "\n",
    "        This class is for PyTorch, unlike CustomDataGeneratorForV02, which was for TF. \n",
    "        \"\"\"\n",
    "        def __init__(self, img_labels_df, class_indices, target_size, batch_size=16, transforms_function=None):\n",
    "            # attributes that were in flow_from_dataframe function used previously in CustomDataGeneratorForV02. Added here for compatibility with other functions like trainModel(), etc\n",
    "            self.img_labels_df = img_labels_df\n",
    "            self.filenames = list(img_labels_df['filepath'])\n",
    "            self.labels = list(class_indices[class_name] for class_name in img_labels_df['label']) # 'labels' is actually not found in flow_from_dataframe's __dict__, but is a synonym of \"classes\" attribute found in the __dict__\n",
    "            self.samples = len(img_labels_df) # number of rows of the df\n",
    "            self.class_indices = class_indices\n",
    "            self.target_size = target_size\n",
    "            self.batch_size = batch_size\n",
    "            self.transforms_function = transforms_function\n",
    "\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.samples // min(1, self.batch_size)\n",
    "        \n",
    "        def __getitem__(self, index):\n",
    "            # note: if you pass batch_size in DataLoader() instead of batch_sampler, then __getitem__() will run for a batch_size number of times consecutively,\n",
    "            #       same thing happens if you pass a batch sampler to `sampler` argument.\n",
    "            #       However, if you pass a non-batch (i.e., single) sampler to `batch_sampler` argument, \n",
    "            #       __getitem__() will run once, but \"index\" argument will be a list of indices (of length of the batch size set by batch_sampler)\n",
    "            #       (side note: don't pass a batch sampler to `batch_sampler`, as this will cause error like this:)\n",
    "            #       (TypeError: 'numpy.int64' object is not iterable)\n",
    "            #       (source: https://github.com/pytorch/pytorch/issues/71872#issue-1115383219)\n",
    "            # source 1: https://discuss.pytorch.org/t/how-to-use-batchsampler-with-getitem-dataset/78788/4#:~:text=The%20index%20inside%20__getitem__%20will%20contain%2010%20random%20indices%2C%20which%20are%20used%20to%20create%20the%20batch%20in%3A\n",
    "            # source 2: https://stackoverflow.com/questions/65231299/load-csv-and-image-dataset-in-pytorch\n",
    "            if type(index) in [int, np.int64]:\n",
    "                filepath = self.filenames[index]\n",
    "                img = Image.open(filepath)\n",
    "                if self.transforms_function is not None:\n",
    "                    img = self.transforms_function(img)\n",
    "                y = torch.tensor(self.labels[index], dtype=torch.int8)\n",
    "                return img, y\n",
    "            else:        \n",
    "                indices = index\n",
    "                imgs = [Image.open(self.filenames[idx]) for idx in range(len(indices))]\n",
    "                if self.transforms_function is not None:\n",
    "                    imgs = [self.transforms_function(img) for img in imgs]\n",
    "                ys = torch.tensor([self.labels[idx] for idx in range(len(indices))], dtype=torch.int8)\n",
    "                return imgs, ys\n",
    "\n",
    "    class CustomDataLoader(torch.utils.data.DataLoader):\n",
    "        def __init__(self, dataset, *args, **kwargs):\n",
    "            if 'sampler' in kwargs.keys():\n",
    "                kwargs.pop('batch_size', None)\n",
    "                kwargs.pop('drop_last', None)\n",
    "                kwargs.pop('shuffle', None)\n",
    "            super().__init__(dataset, *args, **kwargs)\n",
    "            self.__dict__.update(dataset.__dict__)\n",
    "            self.__dict__.pop('img_labels_df')\n",
    "\n",
    "    class CustomStratifiedSampler: # (torch.utils.data.BatchSampler)\n",
    "        \"\"\"Stratified Sampling\n",
    "\n",
    "        Provides equal representation of target classes in each batch\n",
    "        Source: https://github.com/ncullen93/torchsample/blob/master/torchsample/samplers.py#L22\n",
    "        Found from here: https://discuss.pytorch.org/t/how-to-enable-the-dataloader-to-sample-from-each-class-with-equal-probability/911/2\n",
    "        \"\"\"\n",
    "        def __init__(self, class_vector, batch_size, random_state=42):\n",
    "            \"\"\"\n",
    "            Arguments\n",
    "            ---------\n",
    "            class_vector : torch tensor\n",
    "                a vector of class labels\n",
    "            batch_size : integer\n",
    "                batch_size\n",
    "            \"\"\"\n",
    "            self.n_splits = int(class_vector.size(0) / batch_size)\n",
    "            self.class_vector = class_vector\n",
    "            self.batch_size = batch_size\n",
    "            self.steps_num = class_vector.size(0) // batch_size\n",
    "            self.random_state = random_state\n",
    "\n",
    "        def gen_sample_array(self):\n",
    "            try:\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit\n",
    "            except:\n",
    "                print('Need scikit-learn for this functionality')\n",
    "            import numpy as np\n",
    "            s = StratifiedShuffleSplit(n_splits=self.n_splits, test_size=0.5, random_state=self.random_state)\n",
    "            X = torch.randn(self.class_vector.size(0),2)\n",
    "            y = self.class_vector\n",
    "            s.get_n_splits(X, y) \n",
    "\n",
    "            train_index, test_index = next(s.split(X, y))\n",
    "            return np.hstack([train_index, test_index])\n",
    "\n",
    "        def __iter__(self):\n",
    "            return iter(self.gen_sample_array())\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.steps_num\n",
    "        \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data import SequentialSampler, BatchSampler\n",
    "    def get_dataloaders(classNameToNum, dataReDir, img_size=(306, 306), batch_size=16, \n",
    "                        aug_val=False, aug_test=False, aug_values=False, \n",
    "                        shuffle=True, stratify=True, drop_last=True, num_workers=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            classNameToNum: should map each class name to its index\n",
    "            dataReDir: should be a path string of directory which has train/val/test CSVs, where each csv consists of 2 columns: 'filepath', and 'label'\n",
    "            aug_val/aug_test/aug_values: should be a torchvision.transforms's Compose() function, or False\n",
    "\n",
    "        Note: setting shuffle will not affect training; as train_data.csv was already shuffled \n",
    "        by train_test_split() in dataset_preprocessing_part_2.ipynb\n",
    "        \"\"\"\n",
    "\n",
    "        # this cell is from dataset_preprocessing_part_2.ipynb and will be used in most model ipynb files to prepare images\n",
    "        train_data = pd.read_csv(dataReDir+'train_datav01.csv')\n",
    "        val_data = pd.read_csv(dataReDir+'val_datav01.csv')\n",
    "        test_data = pd.read_csv(dataReDir+'test_datav01.csv')\n",
    "\n",
    "        default_transformations = T.Compose([T.Resize(img_size, interpolation=torchvision.transforms.InterpolationMode.NEAREST), T.ToTensor()])\n",
    "        if aug_values == False:\n",
    "            aug_values = default_transformations\n",
    "        if aug_val == False:\n",
    "            aug_val = default_transformations\n",
    "        if aug_test == False:\n",
    "            aug_test = default_transformations\n",
    "\n",
    "\n",
    "        # batch_size here only affects what is returned by __len__()\n",
    "        train_dataset = CustomDatasetForV03(train_data, classNameToNum, img_size, batch_size, aug_values)\n",
    "        val_dataset = CustomDatasetForV03(val_data, classNameToNum, img_size, batch_size, aug_val)\n",
    "        test_dataset = CustomDatasetForV03(test_data, classNameToNum, img_size, batch_size, aug_test)\n",
    "\n",
    "        if stratify:\n",
    "            train_stratify = CustomStratifiedSampler(torch.tensor(train_dataset.labels), batch_size, random_state=42)\n",
    "            val_sampler = BatchSampler(SequentialSampler(val_dataset), batch_size=batch_size, drop_last=drop_last)\n",
    "            test_sampler = BatchSampler(SequentialSampler(test_dataset), batch_size=batch_size, drop_last=drop_last)\n",
    "        else:\n",
    "            train_stratify = None\n",
    "            val_sampler = None\n",
    "            test_sampler = None\n",
    "\n",
    "        train_dataloader = CustomDataLoader(train_dataset, batch_size=batch_size, drop_last=drop_last, \n",
    "                                            sampler=train_stratify, shuffle=shuffle, num_workers=num_workers)\n",
    "        # always set to False if you'll use val_gen/test_gen.filenames list later to analyze the classification results of the val/test set\n",
    "        val_dataloader = CustomDataLoader(val_dataset, batch_size=batch_size, drop_last=drop_last, \n",
    "                                            sampler=val_sampler, shuffle=False, num_workers=num_workers)\n",
    "        test_dataloader = CustomDataLoader(test_dataset, batch_size=batch_size, drop_last=drop_last,\n",
    "                                            sampler=test_sampler, shuffle=False, num_workers=num_workers) \n",
    "    \n",
    "        spe = train_dataset.samples // batch_size\n",
    "        vsteps = val_dataset.samples // batch_size\n",
    "        tsteps = test_dataset.samples // batch_size\n",
    "        print(train_dataset.samples, 'train images')\n",
    "        print(val_dataset.samples, 'val images')\n",
    "        print('so, #steps per epoch (if sampler is not batch):')\n",
    "        print(train_dataset.samples + val_dataset.samples)\n",
    "        print()\n",
    "        print(spe, 'training steps')\n",
    "        print(vsteps, 'validation steps')\n",
    "        print('so #steps per epoch (if sampler is batch):')\n",
    "        print(spe + vsteps)\n",
    "        print()\n",
    "        print(test_dataset.samples, 'test images')\n",
    "        print(tsteps, 'testing steps')\n",
    "\n",
    "        return train_dataloader, val_dataloader, test_dataloader, spe, vsteps, tsteps\n",
    "\n",
    "\n",
    "    cgs = { # cgs <==> class groups\n",
    "            \"colorDiversity\": (\"colorDiversity\", 2), # many or few\n",
    "            \"manyColors\": (\"manyColors\", 4), # selfies, memes (e or f), or eGreetingsAndMisc (+1 if none of them)\n",
    "            \"fewColors\": (\"fewColors\", 4), # socialMedia (e or f), fTxtMssgs, academic (photos or digital) (+1 if none of them)\n",
    "            \"memes\": (\"memes\", 3), # e or f (+1 if none of them)\n",
    "            \"socialMedia\": (\"socialMedia\", 3), # e or f (+1 if none of them)\n",
    "            \"academic\": (\"academic\", 3) # photos or academic (+1 if none of them)\n",
    "        }\n",
    "    cs = { # cs <==> classes\n",
    "        \"selfies\": (\"selfies\", 1),\n",
    "        \"fmemes\": (\"fmemes\", 1),\n",
    "        \"ememes\": (\"ememes\", 1),\n",
    "        \"fSocialMedia\": (\"fSocialMedia\", 1),\n",
    "        \"eSocialMedia\": (\"eSocialMedia\", 1),\n",
    "        \"fTxtMssgs\": (\"fTxtMssgs\", 1),\n",
    "        \"eGreetingsAndMisc\": (\"eGreetingsAndMisc\", 1),\n",
    "        \"academicPhotos\": (\"academicPhotos\", 1),\n",
    "        \"academicDigital\": (\"academicDigital\", 1),\n",
    "    }\n",
    "    # smaller hierarchy\n",
    "    hierarchy = {\n",
    "        cgs[\"colorDiversity\"] : [cgs[\"manyColors\"], cgs[\"fewColors\"]],\n",
    "        cgs[\"manyColors\"] : [cgs[\"memes\"]],\n",
    "        cgs[\"fewColors\"] : [cgs[\"socialMedia\"], cgs[\"academic\"]],\n",
    "    }\n",
    "    # mapping flat labels to hierarchical labels\n",
    "    labelToHierarchy = {\n",
    "        0 : [\n",
    "                0, # diversity of colors: many or few (so classes: 0,1)\n",
    "                0, # many colors: selfies, memes (fmm or emm), or eGreetingsAndMisc (+1 if none of them) (so classes: 0,1,2,3)\n",
    "                3, # few colors: socialMedia (fsm or esm), fTxtMssgs, academic (photos or digital) (+1 if none of them) (so classes: 0,1,2,3)\n",
    "                2, # meme type: fmm or emm (+1 if none of them) (so classes: 0,1,2)\n",
    "                2, # social media type: fsm or esm (+1 if none of them) (so classes: 0,1,2)\n",
    "                2, # academic type: photos or digital (+1 if none of them) (so classes: 0,1,2)\n",
    "            ], \n",
    "        1 : [0, 1, 3, 0, 2, 2],\n",
    "        2 : [0, 1, 3, 1, 2, 2],\n",
    "        3 : [1, 3, 0, 2, 0, 2],\n",
    "        4 : [1, 3, 0, 2, 1, 2],\n",
    "        5 : [1, 3, 1, 2, 2, 2],\n",
    "        6 : [0, 2, 3, 2, 2, 2],\n",
    "        7 : [1, 3, 2, 2, 2, 0],\n",
    "        8 : [1, 3, 2, 2, 2, 1],\n",
    "    }\n",
    "    # mapping hierarchical labels (integers) to hierarchical names (strings)\n",
    "    hierLabelToClassName = {\n",
    "        # diversity of colors: many or few (so classes: 0,1)\n",
    "        0 : ['manyColors', 'fewColors'], \n",
    "        # many colors: selfies, memes (fmm or emm), or eGreetingsAndMisc (+1 if none of them) (so classes: 0,1,2,3)\n",
    "        1 : ['00. selfies', 'memes', '70. eGreetingAndMisc', 'none'],\n",
    "        # few colors: socialMedia (fsm or esm), fTxtMssgs, academic (photos or digital) (+1 if none of them) (so classes: 0,1,2,3)\n",
    "        2 : ['socialMedia', '50. fTxtMssgs', 'academic', 'none'],\n",
    "        # meme type: fmm or emm (+1 if none of them) (so classes: 0,1,2)\n",
    "        3 : ['10. fmemes', '20. ememes', 'none'],\n",
    "        # social media type: fsm or esm (+1 if none of them) (so classes: 0,1,2)\n",
    "        4 : ['30. fSocialMedia', '40. fSocialMedia', 'none'],\n",
    "        # academic type: photos or digital (+1 if none of them) (so classes: 0,1,2)\n",
    "        5 : ['81. academicPhotos', '82. academicDigital', 'none'],\n",
    "    }\n",
    "\n",
    "    def create_hnn_model_arch(model_version:str, img_size:tuple, class_groups:dict, classes:dict, hierarchy:dict):\n",
    "        base_model = nn.Sequential(\n",
    "            # note that padding is 0 to re-produce TF's implementation of m01, where the default value for 'padding' argument is 'valid' which means no padding\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(512),\n",
    "            nn.ReLU(),\n",
    "            # unlike TF implementation of m01, we'll not include this layer here, rather we'll add it later\n",
    "            # nn.Softmax(dim=1) \n",
    "        )\n",
    "\n",
    "        # these are the indepdent layers of parent and children\n",
    "        model_layers = [\n",
    "            # the output of this layer is feed forward from parent to child\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Linear(128, 64),\n",
    "            # we do not include the layer below, as it is automatically created at the end of each parent/child. check this for details:\n",
    "            # https://github.com/rajivsarvepalli/SimpleHierarchy/blob/8e4c29f334928f43509b2d328b11b4a83f2d2af6/src/simple_hierarchy/hierarchal_model.py#L173\n",
    "            # nn.Linear(64, num_classes), \n",
    "        ]\n",
    "\n",
    "        # 512 is the output size of our base model\n",
    "        # 512 is the input size of our additional indepdent layers (called model_layers)\n",
    "        # 64 is the output size of our additional indepdent layers (called model_layers) (excluding the final automatically-created hidden layer)\n",
    "        # 128 is the output size of third to last additional indepdent layer to feed (note: we say 'second to last' if we neglect talking about the final automatically-created hidden layer)\n",
    "        # forward from parent to child (with concatenation)\n",
    "        size = (512,512,64,128)\n",
    "        # all 2 layers are distinct for each grouping of classes of model_layers (actually 5 layers if you add the automatically-created hidden layer)\n",
    "        k = 2\n",
    "        # we want to feed from the fourth to last layer (from parent to child (with concatenation)) (note: we say 'third to last' if we neglect talking about the final automatically-created hidden layer)\n",
    "        feed_from = 1\n",
    "        output_order = [*class_groups.values()]\n",
    "        print(f'output_order:\\n{output_order}')\n",
    "        idx_to_class_name = {k : v for k, v in enumerate([*class_groups.keys(), *classes.keys()])}\n",
    "        model = HierarchalModel(hierarchy=hierarchy, size=size, output_order=output_order, \n",
    "                                base_model=base_model, model=model_layers, k=k, feed_from=feed_from)\n",
    "        \n",
    "        a = torch.rand(3,*img_size).unsqueeze(0)\n",
    "        # running an arbitrary forward pass to initialze weights/params (since LazyLinear was used)\n",
    "        model(a) \n",
    "\n",
    "        writer = SummaryWriter(f\"../models/hnn_model_v{model_version}_tb_graphs\")\n",
    "        writer.add_graph(model, a)\n",
    "        writer.close()\n",
    "\n",
    "        return model, idx_to_class_name\n",
    "    \n",
    "    img_size = (306,306)\n",
    "    hier_model, idx_to_class_name = create_hnn_model_arch(\"03\", img_size, cgs, cs, hierarchy)\n",
    "    num_classes_per_layer = [num_outputs for layer_name, num_outputs in hier_model.output_order]\n",
    "\n",
    "\n",
    "    class HierarchalModelPL(pl.LightningModule):\n",
    "        def __init__(self, hierarchical_model, num_classes_per_layer:list, loss_weights=None, dummy_input_size=(32, 3, 306, 306), the_device='cpu'):\n",
    "            super(HierarchalModelPL, self).__init__()\n",
    "            self.hierarchical_model = hierarchical_model # .to(the_device)\n",
    "            self.num_output_layers = len(num_classes_per_layer)\n",
    "            self.loss_weights = [1]*self.num_output_layers if loss_weights is None else loss_weights\n",
    "            self.num_output_layers = self.num_output_layers\n",
    "            \n",
    "            # useful for writing computational graph in tensorboard, summary(), etc\n",
    "            self.example_input_array = torch.randn(*dummy_input_size) # , device=the_device\n",
    "\n",
    "            self.the_device = the_device if 'cpu' in the_device else 'cuda'\n",
    "            self.metrics = ['acc', 'f1_score']\n",
    "            def _create_metric_func(metric_type:str, num_classes):\n",
    "                if 'acc' in metric_type.lower():\n",
    "                    return torchmetrics.Accuracy(task='multiclass', \n",
    "                                                num_classes=num_classes, \n",
    "                                                average=\"micro\").to(the_device)\n",
    "                elif 'f1' in metric_type.lower():\n",
    "                    return torchmetrics.F1Score(task=\"multiclass\", \n",
    "                                                num_classes=num_classes, \n",
    "                                                average=None).to(the_device)\n",
    "            \n",
    "            #                                         step/epoch vvv\n",
    "            # max indexing possible: self.metric_funcs['f1_score'][1][num_output_layers-1][2]\n",
    "            #                                       softmax output layers ^^^            ^^^  train/val/test \n",
    "            # output example of metric_funcs['acc']\n",
    "            # (note: MCA == MulticlassAccuracy()):\n",
    "            # {'acc': [ \n",
    "            #   [ #step\n",
    "            #    [MCA, MCA, MCA], # softmax_output_layer_0 ; train/val/test\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA], # ...\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA] # # softmax_output_layer_5 ; train/val/test\n",
    "            #   ],\n",
    "            #   [ #epoch\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA],\n",
    "            #    [MCA, MCA, MCA]\n",
    "            #   ]\n",
    "            # ]\n",
    "            \n",
    "            self.metric_funcs = {}\n",
    "            for metric in self.metrics:\n",
    "                # i --> step/epoch, j --> num_output_layers-1, k --> train/val/test\n",
    "                self.metric_funcs[metric] = [[[_create_metric_func(metric, num_classes_per_layer[j]) for k in range(3)] \n",
    "                                            for j in range(self.num_output_layers)] \n",
    "                                            for i in range(2)]\n",
    "            \n",
    "            # history_per_output_layer = {\n",
    "            #    # per epoch\n",
    "            #    'loss':[],\n",
    "            #    'acc':[],\n",
    "            #    'f1_score':[],\n",
    "            #    'val_loss':[],\n",
    "            #    'val_acc':[],\n",
    "            #    'val_f1_score':[],\n",
    "            #    'test_loss':[],\n",
    "            #    'test_acc':[],\n",
    "            #    'test_f1_score':[],\n",
    "            #    # per step\n",
    "            #    'step_loss':[],\n",
    "            #    'step_acc':[],\n",
    "            #    'step_f1_score':[],\n",
    "            #    'val_step_loss':[],\n",
    "            #    'val_step_acc':[],\n",
    "            #    'val_step_f1_score':[],\n",
    "            #    'test_step_loss':[],\n",
    "            #    'test_step_acc':[],\n",
    "            #    'test_step_f1_score':[],\n",
    "            #    # for tracking pred/true values for train/val/test sets\n",
    "            #    'step_y_pred': [],\n",
    "            #    'step_y_true': [],\n",
    "            #    'val_step_y_pred': [],\n",
    "            #    'val_step_y_true': [],\n",
    "            #    'test_step_y_pred': [],\n",
    "            #    'test_step_y_true': [],\n",
    "            # }\n",
    "\n",
    "            history_per_output_layer = {\n",
    "                # per epoch\n",
    "                'loss':[],\n",
    "                'acc':[],\n",
    "                'f1_score':[],\n",
    "                # these 2 will be replaced with step_y_pred and step_y_true respectively\n",
    "                'y_pred':[],\n",
    "                'y_true':[],\n",
    "            }\n",
    "\n",
    "            def pre_suf_fix_dict(dict, fix_val='output_layer', prefix=True):\n",
    "                return {f\"{fix_val}_{key}\" if prefix else f\"{key}_{fix_val}\" : value for key, value in dict.items()}\n",
    "\n",
    "            # to create val/test prefixed keys\n",
    "            history_per_output_layer.update(\n",
    "                **pre_suf_fix_dict(history_per_output_layer, fix_val='val', prefix=True),\n",
    "                **pre_suf_fix_dict(history_per_output_layer, fix_val='test', prefix=True),\n",
    "            )\n",
    "            # to create step_ prefixed keys\n",
    "            history_per_output_layer.update(\n",
    "                **pre_suf_fix_dict(history_per_output_layer, fix_val='step', prefix=True),\n",
    "            )\n",
    "            # removing y_pred, y_true and keeping step_y_pred and step_y_true\n",
    "            history_per_output_layer.pop('y_pred')\n",
    "            history_per_output_layer.pop('y_true')\n",
    "            \n",
    "            # fix_val == suffix without \"_\"; to provide flexibility in using fix_val as prefix as well (if we wanted to!)\n",
    "            self.history_layers_fix_val = 'output_layer' \n",
    "\n",
    "            # pre_suf_fix_dict function is used to create unique key names to merge the `num_output_layers` histories into one history dictionary\n",
    "            history_ol_lists = [pre_suf_fix_dict(history_per_output_layer, \n",
    "                                                fix_val=f'{self.history_layers_fix_val}_{i}', \n",
    "                                                prefix=False) \n",
    "                                for i in range(self.num_output_layers)] # ol == output layers\n",
    "            \n",
    "            # 'loss' here is for the final loss calculated by the loss aggregator function which uses all `num_output_layers` losses\n",
    "            self.history = {'loss':[], 'val_loss':[], 'test_loss':[]}\n",
    "            self.history.update(**pre_suf_fix_dict(self.history, 'step', prefix=True))\n",
    "            # merging the `num_output_layers` histories into one history dictionary\n",
    "            for hist_dict in history_ol_lists:\n",
    "                self.history.update(**hist_dict)\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            # len(hier_y_probs) == num_hierarchy_output_layers, \n",
    "            # while each element is 2D tensor of shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "            hier_y_pred = []\n",
    "            for tensor_output in self.hierarchical_model(x):\n",
    "                hier_y_pred.append(F.softmax(tensor_output, dim=1))\n",
    "            hier_y_pred\n",
    "            return hier_y_pred\n",
    "        \n",
    "        def training_step(self, batch, batch_idx):\n",
    "            ds_prefix = ''\n",
    "            metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "            # add other key:value pairs here or pass the entire metrics_dict if you want. just make sure 'loss' key is present\n",
    "            return metrics_dict\n",
    "            \n",
    "        def training_epoch_end(self, outputs) -> None:\n",
    "            # 'outputs' argument here contains values from what was returned from training_step()\n",
    "            ds_prefix = ''\n",
    "            self._epoch_end_logic(outputs, ds_prefix)\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            ds_prefix = 'val_'\n",
    "            metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "            return metrics_dict\n",
    "        \n",
    "        def validation_epoch_end(self, outputs) -> None:\n",
    "            ds_prefix = 'val_'\n",
    "            self._epoch_end_logic(outputs, ds_prefix)\n",
    "        \n",
    "        def test_step(self, batch, batch_idx):\n",
    "            ds_prefix = 'test_'\n",
    "            metrics_dict = self._step_logic(batch, ds_prefix=ds_prefix)\n",
    "            return metrics_dict\n",
    "        \n",
    "        def test_epoch_end(self, outputs) -> None:\n",
    "            ds_prefix = 'test_'\n",
    "            self._epoch_end_logic(outputs, ds_prefix=ds_prefix)\n",
    "        \n",
    "\n",
    "        def _store_metric(self, metric_name, metric_val):\n",
    "            try:\n",
    "                self.log(metric_name, metric_val)\n",
    "            except Exception:\n",
    "                # then metric_val is not scalar, then this means:\n",
    "                # metric_name is an f1 score metric \n",
    "                # so, we just take the mean value\n",
    "                self.log(metric_name, metric_val.mean())\n",
    "            # putting this here ensures we don't include the mean of f1 score, \n",
    "            # but rather the tensor of shape (1, num_classes)\n",
    "            self.history[metric_name].append(metric_val)\n",
    "\n",
    "        def _step_logic(self, batch, ds_prefix=''):\n",
    "            # ds_prefix == dataset_prefix\n",
    "            if 'val' in ds_prefix:\n",
    "                ds_type_idx = 1\n",
    "            elif 'test' in ds_prefix:\n",
    "                ds_type_idx = 2\n",
    "            else:\n",
    "                ds_type_idx = 0\n",
    "            ds_prefix = 'step_' + ds_prefix\n",
    "\n",
    "            x, y = batch\n",
    "            hier_y_pred = self(x)\n",
    "\n",
    "            # y now has shape (batch_size, len(hier_y_pred))\n",
    "            # in other words, each row now consists of `len(hier_y_pred)` output layers' labels of 1 sample\n",
    "            # side note: len(hier_y_pred) == self.num_output_layers\n",
    "            y = torch.tensor([labelToHierarchy[int(y[i])] for i in range(len(y))], dtype=int, device=self.the_device) # alternatively, range(y.size()[0])\n",
    "            # transpose y to take each row (batch of labels) with its corresponding hier_y_pred row (batch of predicted labels)\n",
    "            # in other words, each row now consists of `batch_size` labels of the i_th output layer\n",
    "            y = y.T\n",
    "            # softmax_output_layer elements each has shape (batch_size, num_classes of the i_th output layer in hierarchy)\n",
    "            # side note: the wording of \"i_th output layers\" refers to the strings mentioned in \n",
    "            # output_order argument of HierarchalModel() used in create_hnn_model_arch()\n",
    "\n",
    "            losses_dict = {}\n",
    "            other_metrics_dict = {}\n",
    "            for out_layer_idx, softmax_output_layer in enumerate(hier_y_pred): \n",
    "                # 'cur' refers to current output layer\n",
    "                y_cur = y[out_layer_idx]\n",
    "                y_pred_cur = softmax_output_layer.to(self.the_device)\n",
    "                ol_suffix = f'_{self.history_layers_fix_val}_{out_layer_idx}'\n",
    "\n",
    "                # log step metrics\n",
    "\n",
    "                loss = F.cross_entropy(y_pred_cur, y_cur)\n",
    "                loss_full_metric_name = f'{ds_prefix}loss{ol_suffix}'\n",
    "                self._store_metric(loss_full_metric_name, loss)\n",
    "                losses_dict[loss_full_metric_name] = loss\n",
    "\n",
    "                for metric_name in self.metrics:\n",
    "                    # recall the indexing of self.metric_funcs:\n",
    "                    # metric_name, step/epoch, num_output_layers-1, train/val/test\n",
    "                    metric_val = self.metric_funcs[metric_name][0][out_layer_idx][ds_type_idx](y_pred_cur, y_cur)\n",
    "                    full_metric_name = f'{ds_prefix}{metric_name}{ol_suffix}'\n",
    "                    self._store_metric(full_metric_name, metric_val)\n",
    "                    self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].update(y_pred_cur, y_cur)\n",
    "                    other_metrics_dict[full_metric_name] = metric_val\n",
    "            \n",
    "                f1_score = self.metric_funcs[metric_name][0][out_layer_idx][ds_type_idx](y_pred_cur, y_cur)\n",
    "                self._store_metric(f'{ds_prefix}{metric_name}{ol_suffix}', f1_score)\n",
    "                self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].update(y_pred_cur, y_cur)\n",
    "\n",
    "                # storing y_pred/y_true\n",
    "                self.history[f'{ds_prefix}y_pred{ol_suffix}'].extend(y_pred_cur)\n",
    "                self.history[f'{ds_prefix}y_true{ol_suffix}'].extend(y_cur)\n",
    "\n",
    "            final_loss = self.metric_reduce_fx(losses_dict.values(), 'weighted_sum')\n",
    "            self._store_metric(f'{ds_prefix}loss', final_loss)\n",
    "\n",
    "            # Important note: 'loss' key must be present, or else you'll get this error:\n",
    "            # MisconfigurationException: In automatic_optimization, \n",
    "            # when `training_step` returns a dict, the 'loss' key needs to be present\n",
    "            # side note: add `.update(losses_dict)` and `.update(other_metrics_dict)` \n",
    "            # if you want to directly use other metrics in \"..._epoch_end()\" methods\n",
    "            return {f'loss' : final_loss} \n",
    "\n",
    "        def _epoch_end_logic(self, outputs, ds_prefix=''):\n",
    "            if 'val' in ds_prefix:\n",
    "                ds_type_idx = 1\n",
    "            elif 'test' in ds_prefix:\n",
    "                ds_type_idx = 2\n",
    "            else:\n",
    "                ds_type_idx = 0\n",
    "\n",
    "            # log epoch metrics\n",
    "            final_loss_epoch = torch.tensor([x[f'loss'] for x in outputs], dtype=float).mean()\n",
    "            self._store_metric(f'{ds_prefix}loss', final_loss_epoch)\n",
    "\n",
    "            for out_layer_idx in range(self.num_output_layers):\n",
    "                ol_suffix = f'_{self.history_layers_fix_val}_{out_layer_idx}'\n",
    "\n",
    "                for metric_name in self.metrics:\n",
    "                    # recall: '1' for accessing epoch func (not step func)\n",
    "                    metric_val_epoch = self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].compute()\n",
    "                    self.metric_funcs[metric_name][1][out_layer_idx][ds_type_idx].reset()\n",
    "                    full_metric_name = f'{ds_prefix}{metric_name}{ol_suffix}'\n",
    "                    self._store_metric(full_metric_name, metric_val_epoch)\n",
    "        \n",
    "        def metric_reduce_fx(self, metric_list, agg_type='weighted_sum'):\n",
    "            '''\n",
    "            aggregates metric values from all `num_output_layers` into a single value\n",
    "            side note: called \"reduce_fx\" as a reference to PyTorch Lightning's reduce_fx parameter found in self.log()\n",
    "            '''\n",
    "            if 'weighted' in agg_type.lower() and 'sum' in agg_type.lower():\n",
    "                weighted_sum_val = 0\n",
    "                for i, layer_metric_val in enumerate(metric_list):\n",
    "                    weighted_sum_val += self.loss_weights[i] * layer_metric_val\n",
    "                final_val = weighted_sum_val\n",
    "\n",
    "            return final_val\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            # make it self.hierarchical_model.parameters() if you defined other parameters in __init()__ which you don't want to optimize\n",
    "            # side note: under the hood, pl automatically gets gradients \n",
    "            # from final_loss returned from training_step() and adjusts the models' branches accordingly\n",
    "            # source: https://github.com/Lightning-AI/lightning/issues/2645#issuecomment-660681760\n",
    "            return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    train_gen, val_gen, test_gen, \\\n",
    "    spe, vsteps, tsteps = get_dataloaders(classNameToNum=classNameToNum,\n",
    "                                        dataReDir=dataReDir,\n",
    "                                        img_size=(306, 306), \n",
    "                                        batch_size=32,\n",
    "                                        aug_val=False,\n",
    "                                        aug_test=False,\n",
    "                                        aug_values=False,\n",
    "                                        shuffle=True,\n",
    "                                        stratify=True,\n",
    "                                        drop_last=True, \n",
    "                                        num_workers=2)\n",
    "\n",
    "    model = HierarchalModelPL(hierarchical_model=hier_model, \n",
    "                            num_classes_per_layer=num_classes_per_layer, \n",
    "                            loss_weights=None, # same weight for all output layers' losses\n",
    "                            dummy_input_size=(32, 3, 306, 306), \n",
    "                            the_device='cuda')\n",
    "    trainer = pl.Trainer(log_every_n_steps=1, num_sanity_val_steps=0, max_epochs=2, max_steps=-1, accelerator='gpu', fast_dev_run=False)\n",
    "    trainer.fit(model=model, train_dataloaders=train_gen, val_dataloaders=val_gen) # , val_dataloaders=val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
